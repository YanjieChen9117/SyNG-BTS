{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://test.pypi.org/simple/, https://pypi.org/simple/\n",
      "Collecting syng-bts==2.4\n",
      "  Using cached https://test-files.pythonhosted.org/packages/cd/50/6090c8ff7014ecad0c1dc02c1554309e4ab67e85a28d15c5c111fa8c893b/syng_bts-2.4-py3-none-any.whl.metadata (542 bytes)\n",
      "Collecting sphinx==7.1.2 (from syng-bts==2.4)\n",
      "  Using cached sphinx-7.1.2-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting sphinx-rtd-theme==1.3.0rc1 (from syng-bts==2.4)\n",
      "  Using cached sphinx_rtd_theme-1.3.0rc1-py2.py3-none-any.whl.metadata (4.5 kB)\n",
      "Collecting torch>=1.3.1 (from syng-bts==2.4)\n",
      "  Using cached torch-2.8.0-cp311-none-macosx_11_0_arm64.whl.metadata (30 kB)\n",
      "Collecting pandas>=1.0.5 (from syng-bts==2.4)\n",
      "  Downloading pandas-2.3.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (91 kB)\n",
      "Collecting seaborn>=0.9.0 (from syng-bts==2.4)\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting numpy>=1.19.1 (from syng-bts==2.4)\n",
      "  Using cached numpy-2.3.3-cp311-cp311-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting pathlib2>=2.3.2 (from syng-bts==2.4)\n",
      "  Using cached pathlib2-2.3.7.post1-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting scipy>=1.4.1 (from syng-bts==2.4)\n",
      "  Using cached scipy-1.16.2-cp311-cp311-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting matplotlib>=2.2.3 (from syng-bts==2.4)\n",
      "  Using cached matplotlib-3.10.6-cp311-cp311-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Collecting tqdm>=4.26.0 (from syng-bts==2.4)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting tensorboardX>=2.5.0 (from syng-bts==2.4)\n",
      "  Using cached tensorboardx-2.6.4-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting umap-learn>=0.5.6 (from syng-bts==2.4)\n",
      "  Using cached umap_learn-0.5.9.post2-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting nbsphinx (from syng-bts==2.4)\n",
      "  Using cached nbsphinx-0.9.7-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting sphinxcontrib-applehelp (from sphinx==7.1.2->syng-bts==2.4)\n",
      "  Using cached sphinxcontrib_applehelp-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting sphinxcontrib-devhelp (from sphinx==7.1.2->syng-bts==2.4)\n",
      "  Using cached sphinxcontrib_devhelp-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting sphinxcontrib-jsmath (from sphinx==7.1.2->syng-bts==2.4)\n",
      "  Using cached sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting sphinxcontrib-htmlhelp>=2.0.0 (from sphinx==7.1.2->syng-bts==2.4)\n",
      "  Using cached sphinxcontrib_htmlhelp-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting sphinxcontrib-serializinghtml>=1.1.5 (from sphinx==7.1.2->syng-bts==2.4)\n",
      "  Using cached sphinxcontrib_serializinghtml-2.0.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting sphinxcontrib-qthelp (from sphinx==7.1.2->syng-bts==2.4)\n",
      "  Using cached sphinxcontrib_qthelp-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting Jinja2>=3.0 (from sphinx==7.1.2->syng-bts==2.4)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: Pygments>=2.13 in /Users/yanjiechen/Documents/Github/SyNG-BTS/.conda/lib/python3.11/site-packages (from sphinx==7.1.2->syng-bts==2.4) (2.19.2)\n",
      "Collecting docutils<0.21,>=0.18.1 (from sphinx==7.1.2->syng-bts==2.4)\n",
      "  Using cached docutils-0.20.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting snowballstemmer>=2.0 (from sphinx==7.1.2->syng-bts==2.4)\n",
      "  Using cached snowballstemmer-3.0.1-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting babel>=2.9 (from sphinx==7.1.2->syng-bts==2.4)\n",
      "  Using cached babel-2.17.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting alabaster<0.8,>=0.7 (from sphinx==7.1.2->syng-bts==2.4)\n",
      "  Using cached alabaster-0.7.16-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting imagesize>=1.3 (from sphinx==7.1.2->syng-bts==2.4)\n",
      "  Using cached imagesize-1.4.1-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting requests>=2.25.0 (from sphinx==7.1.2->syng-bts==2.4)\n",
      "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: packaging>=21.0 in /Users/yanjiechen/Documents/Github/SyNG-BTS/.conda/lib/python3.11/site-packages (from sphinx==7.1.2->syng-bts==2.4) (25.0)\n",
      "Collecting docutils<0.21,>=0.18.1 (from sphinx==7.1.2->syng-bts==2.4)\n",
      "  Using cached docutils-0.18.1-py2.py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting sphinxcontrib-jquery<5,>=4 (from sphinx-rtd-theme==1.3.0rc1->syng-bts==2.4)\n",
      "  Using cached sphinxcontrib_jquery-4.1-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from Jinja2>=3.0->sphinx==7.1.2->syng-bts==2.4)\n",
      "  Downloading markupsafe-3.0.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.7 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib>=2.2.3->syng-bts==2.4)\n",
      "  Using cached contourpy-1.3.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib>=2.2.3->syng-bts==2.4)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib>=2.2.3->syng-bts==2.4)\n",
      "  Downloading fonttools-4.60.1-cp311-cp311-macosx_10_9_universal2.whl.metadata (112 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib>=2.2.3->syng-bts==2.4)\n",
      "  Using cached kiwisolver-1.4.9-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.3 kB)\n",
      "Collecting pillow>=8 (from matplotlib>=2.2.3->syng-bts==2.4)\n",
      "  Using cached pillow-11.3.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.0 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib>=2.2.3->syng-bts==2.4)\n",
      "  Using cached pyparsing-3.2.5-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/yanjiechen/Documents/Github/SyNG-BTS/.conda/lib/python3.11/site-packages (from matplotlib>=2.2.3->syng-bts==2.4) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas>=1.0.5->syng-bts==2.4)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas>=1.0.5->syng-bts==2.4)\n",
      "  Downloading https://test-files.pythonhosted.org/packages/b3/d0/ad597ad8e6636b0e673a4866d0d7d415d45c27f0d1ae81e055b919a0df08/tzdata-2025.2.post0-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six in /Users/yanjiechen/Documents/Github/SyNG-BTS/.conda/lib/python3.11/site-packages (from pathlib2>=2.3.2->syng-bts==2.4) (1.17.0)\n",
      "Collecting charset_normalizer<4,>=2 (from requests>=2.25.0->sphinx==7.1.2->syng-bts==2.4)\n",
      "  Using cached charset_normalizer-3.4.3-cp311-cp311-macosx_10_9_universal2.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests>=2.25.0->sphinx==7.1.2->syng-bts==2.4)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.25.0->sphinx==7.1.2->syng-bts==2.4)\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.25.0->sphinx==7.1.2->syng-bts==2.4)\n",
      "  Using cached certifi-2025.8.3-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting protobuf>=3.20 (from tensorboardX>=2.5.0->syng-bts==2.4)\n",
      "  Using cached protobuf-6.32.1-cp39-abi3-macosx_10_9_universal2.whl.metadata (593 bytes)\n",
      "Collecting filelock (from torch>=1.3.1->syng-bts==2.4)\n",
      "  Using cached filelock-3.19.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/yanjiechen/Documents/Github/SyNG-BTS/.conda/lib/python3.11/site-packages (from torch>=1.3.1->syng-bts==2.4) (4.15.0)\n",
      "Collecting sympy>=1.13.3 (from torch>=1.3.1->syng-bts==2.4)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch>=1.3.1->syng-bts==2.4)\n",
      "  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting fsspec (from torch>=1.3.1->syng-bts==2.4)\n",
      "  Using cached fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=1.3.1->syng-bts==2.4)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting scikit-learn>=1.6 (from umap-learn>=0.5.6->syng-bts==2.4)\n",
      "  Using cached scikit_learn-1.7.2-cp311-cp311-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Collecting numba>=0.51.2 (from umap-learn>=0.5.6->syng-bts==2.4)\n",
      "  Downloading numba-0.62.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.8 kB)\n",
      "Collecting pynndescent>=0.5 (from umap-learn>=0.5.6->syng-bts==2.4)\n",
      "  Using cached pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting llvmlite<0.46,>=0.45.0dev0 (from numba>=0.51.2->umap-learn>=0.5.6->syng-bts==2.4)\n",
      "  Downloading llvmlite-0.45.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (4.8 kB)\n",
      "Collecting joblib>=0.11 (from pynndescent>=0.5->umap-learn>=0.5.6->syng-bts==2.4)\n",
      "  Using cached joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn>=1.6->umap-learn>=0.5.6->syng-bts==2.4)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting nbconvert!=5.4,>=5.3 (from nbsphinx->syng-bts==2.4)\n",
      "  Using cached nbconvert-7.16.6-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: traitlets>=5 in /Users/yanjiechen/Documents/Github/SyNG-BTS/.conda/lib/python3.11/site-packages (from nbsphinx->syng-bts==2.4) (5.14.3)\n",
      "Collecting nbformat (from nbsphinx->syng-bts==2.4)\n",
      "  Using cached nbformat-5.10.4-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting beautifulsoup4 (from nbconvert!=5.4,>=5.3->nbsphinx->syng-bts==2.4)\n",
      "  Downloading beautifulsoup4-4.14.2-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting bleach!=5.0.0 (from bleach[css]!=5.0.0->nbconvert!=5.4,>=5.3->nbsphinx->syng-bts==2.4)\n",
      "  Using cached bleach-6.2.0-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting defusedxml (from nbconvert!=5.4,>=5.3->nbsphinx->syng-bts==2.4)\n",
      "  Using cached defusedxml-0.7.1-py2.py3-none-any.whl.metadata (32 kB)\n",
      "Requirement already satisfied: jupyter-core>=4.7 in /Users/yanjiechen/Documents/Github/SyNG-BTS/.conda/lib/python3.11/site-packages (from nbconvert!=5.4,>=5.3->nbsphinx->syng-bts==2.4) (5.8.1)\n",
      "Collecting jupyterlab-pygments (from nbconvert!=5.4,>=5.3->nbsphinx->syng-bts==2.4)\n",
      "  Using cached jupyterlab_pygments-0.3.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting mistune<4,>=2.0.3 (from nbconvert!=5.4,>=5.3->nbsphinx->syng-bts==2.4)\n",
      "  Using cached mistune-3.1.4-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting nbclient>=0.5.0 (from nbconvert!=5.4,>=5.3->nbsphinx->syng-bts==2.4)\n",
      "  Using cached nbclient-0.10.2-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting pandocfilters>=1.4.1 (from nbconvert!=5.4,>=5.3->nbsphinx->syng-bts==2.4)\n",
      "  Using cached pandocfilters-1.5.1-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting webencodings (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert!=5.4,>=5.3->nbsphinx->syng-bts==2.4)\n",
      "  Using cached webencodings-0.5.1-py2.py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting tinycss2<1.5,>=1.1.0 (from bleach[css]!=5.0.0->nbconvert!=5.4,>=5.3->nbsphinx->syng-bts==2.4)\n",
      "  Using cached tinycss2-1.4.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /Users/yanjiechen/Documents/Github/SyNG-BTS/.conda/lib/python3.11/site-packages (from jupyter-core>=4.7->nbconvert!=5.4,>=5.3->nbsphinx->syng-bts==2.4) (4.4.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /Users/yanjiechen/Documents/Github/SyNG-BTS/.conda/lib/python3.11/site-packages (from nbclient>=0.5.0->nbconvert!=5.4,>=5.3->nbsphinx->syng-bts==2.4) (8.6.3)\n",
      "Requirement already satisfied: pyzmq>=23.0 in /Users/yanjiechen/Documents/Github/SyNG-BTS/.conda/lib/python3.11/site-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert!=5.4,>=5.3->nbsphinx->syng-bts==2.4) (27.1.0)\n",
      "Requirement already satisfied: tornado>=6.2 in /Users/yanjiechen/Documents/Github/SyNG-BTS/.conda/lib/python3.11/site-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert!=5.4,>=5.3->nbsphinx->syng-bts==2.4) (6.5.2)\n",
      "Collecting fastjsonschema>=2.15 (from nbformat->nbsphinx->syng-bts==2.4)\n",
      "  Using cached fastjsonschema-2.21.2-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting jsonschema>=2.6 (from nbformat->nbsphinx->syng-bts==2.4)\n",
      "  Using cached jsonschema-4.25.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting attrs>=22.2.0 (from jsonschema>=2.6->nbformat->nbsphinx->syng-bts==2.4)\n",
      "  Using cached attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=2.6->nbformat->nbsphinx->syng-bts==2.4)\n",
      "  Using cached jsonschema_specifications-2025.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=2.6->nbformat->nbsphinx->syng-bts==2.4)\n",
      "  Using cached referencing-0.36.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema>=2.6->nbformat->nbsphinx->syng-bts==2.4)\n",
      "  Using cached rpds_py-0.27.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (4.2 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->nbconvert!=5.4,>=5.3->nbsphinx->syng-bts==2.4)\n",
      "  Using cached soupsieve-2.8-py3-none-any.whl.metadata (4.6 kB)\n",
      "Using cached https://test-files.pythonhosted.org/packages/cd/50/6090c8ff7014ecad0c1dc02c1554309e4ab67e85a28d15c5c111fa8c893b/syng_bts-2.4-py3-none-any.whl (2.4 MB)\n",
      "Using cached sphinx-7.1.2-py3-none-any.whl (3.2 MB)\n",
      "Using cached sphinx_rtd_theme-1.3.0rc1-py2.py3-none-any.whl (2.8 MB)\n",
      "Using cached alabaster-0.7.16-py3-none-any.whl (13 kB)\n",
      "Using cached docutils-0.18.1-py2.py3-none-any.whl (570 kB)\n",
      "Using cached sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
      "Using cached babel-2.17.0-py3-none-any.whl (10.2 MB)\n",
      "Using cached imagesize-1.4.1-py2.py3-none-any.whl (8.8 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Downloading markupsafe-3.0.3-cp311-cp311-macosx_11_0_arm64.whl (12 kB)\n",
      "Using cached matplotlib-3.10.6-cp311-cp311-macosx_11_0_arm64.whl (8.1 MB)\n",
      "Using cached contourpy-1.3.3-cp311-cp311-macosx_11_0_arm64.whl (270 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.60.1-cp311-cp311-macosx_10_9_universal2.whl (2.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached kiwisolver-1.4.9-cp311-cp311-macosx_11_0_arm64.whl (65 kB)\n",
      "Downloading numpy-2.3.3-cp311-cp311-macosx_14_0_arm64.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.3.3-cp311-cp311-macosx_11_0_arm64.whl (10.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pathlib2-2.3.7.post1-py2.py3-none-any.whl (18 kB)\n",
      "Using cached pillow-11.3.0-cp311-cp311-macosx_11_0_arm64.whl (4.7 MB)\n",
      "Using cached pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Using cached charset_normalizer-3.4.3-cp311-cp311-macosx_10_9_universal2.whl (204 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Using cached certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
      "Using cached scipy-1.16.2-cp311-cp311-macosx_14_0_arm64.whl (20.9 MB)\n",
      "Using cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Using cached snowballstemmer-3.0.1-py3-none-any.whl (103 kB)\n",
      "Using cached sphinxcontrib_htmlhelp-2.1.0-py3-none-any.whl (98 kB)\n",
      "Using cached sphinxcontrib_serializinghtml-2.0.0-py3-none-any.whl (92 kB)\n",
      "Using cached tensorboardx-2.6.4-py3-none-any.whl (87 kB)\n",
      "Using cached protobuf-6.32.1-cp39-abi3-macosx_10_9_universal2.whl (426 kB)\n",
      "Using cached torch-2.8.0-cp311-none-macosx_11_0_arm64.whl (73.6 MB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading https://test-files.pythonhosted.org/packages/b3/d0/ad597ad8e6636b0e673a4866d0d7d415d45c27f0d1ae81e055b919a0df08/tzdata-2025.2.post0-py2.py3-none-any.whl (347 kB)\n",
      "Using cached umap_learn-0.5.9.post2-py3-none-any.whl (90 kB)\n",
      "Downloading numba-0.62.1-cp311-cp311-macosx_11_0_arm64.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading llvmlite-0.45.0-cp311-cp311-macosx_11_0_arm64.whl (37.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.3/37.3 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m  \u001b[33m0:00:04\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
      "Using cached joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Using cached scikit_learn-1.7.2-cp311-cp311-macosx_12_0_arm64.whl (8.6 MB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Using cached filelock-3.19.1-py3-none-any.whl (15 kB)\n",
      "Using cached fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
      "Using cached nbsphinx-0.9.7-py3-none-any.whl (31 kB)\n",
      "Using cached nbconvert-7.16.6-py3-none-any.whl (258 kB)\n",
      "Using cached mistune-3.1.4-py3-none-any.whl (53 kB)\n",
      "Using cached bleach-6.2.0-py3-none-any.whl (163 kB)\n",
      "Using cached tinycss2-1.4.0-py3-none-any.whl (26 kB)\n",
      "Using cached nbclient-0.10.2-py3-none-any.whl (25 kB)\n",
      "Using cached nbformat-5.10.4-py3-none-any.whl (78 kB)\n",
      "Using cached fastjsonschema-2.21.2-py3-none-any.whl (24 kB)\n",
      "Using cached jsonschema-4.25.1-py3-none-any.whl (90 kB)\n",
      "Using cached attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Using cached jsonschema_specifications-2025.9.1-py3-none-any.whl (18 kB)\n",
      "Using cached pandocfilters-1.5.1-py2.py3-none-any.whl (8.7 kB)\n",
      "Using cached referencing-0.36.2-py3-none-any.whl (26 kB)\n",
      "Using cached rpds_py-0.27.1-cp311-cp311-macosx_11_0_arm64.whl (353 kB)\n",
      "Using cached webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Downloading beautifulsoup4-4.14.2-py3-none-any.whl (106 kB)\n",
      "Using cached soupsieve-2.8-py3-none-any.whl (36 kB)\n",
      "Using cached defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
      "Using cached jupyterlab_pygments-0.3.0-py3-none-any.whl (15 kB)\n",
      "Using cached networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "Using cached sphinxcontrib_applehelp-2.0.0-py3-none-any.whl (119 kB)\n",
      "Using cached sphinxcontrib_devhelp-2.0.0-py3-none-any.whl (82 kB)\n",
      "Using cached sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl (5.1 kB)\n",
      "Using cached sphinxcontrib_qthelp-2.0.0-py3-none-any.whl (88 kB)\n",
      "Installing collected packages: webencodings, pytz, mpmath, fastjsonschema, urllib3, tzdata, tqdm, tinycss2, threadpoolctl, sympy, sphinxcontrib-serializinghtml, sphinxcontrib-qthelp, sphinxcontrib-jsmath, sphinxcontrib-htmlhelp, sphinxcontrib-devhelp, sphinxcontrib-applehelp, soupsieve, snowballstemmer, rpds-py, pyparsing, protobuf, pillow, pathlib2, pandocfilters, numpy, networkx, mistune, MarkupSafe, llvmlite, kiwisolver, jupyterlab-pygments, joblib, imagesize, idna, fsspec, fonttools, filelock, docutils, defusedxml, cycler, charset_normalizer, certifi, bleach, babel, attrs, alabaster, tensorboardX, scipy, requests, referencing, pandas, numba, Jinja2, contourpy, beautifulsoup4, torch, sphinx, scikit-learn, matplotlib, jsonschema-specifications, sphinxcontrib-jquery, seaborn, pynndescent, jsonschema, umap-learn, sphinx-rtd-theme, nbformat, nbclient, nbconvert, nbsphinx, syng-bts\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71/71\u001b[0m [syng-bts]syng-bts]umap-learn]n]lizer]\n",
      "\u001b[1A\u001b[2KSuccessfully installed Jinja2-3.1.6 MarkupSafe-3.0.3 alabaster-0.7.16 attrs-25.3.0 babel-2.17.0 beautifulsoup4-4.14.2 bleach-6.2.0 certifi-2025.8.3 charset_normalizer-3.4.3 contourpy-1.3.3 cycler-0.12.1 defusedxml-0.7.1 docutils-0.18.1 fastjsonschema-2.21.2 filelock-3.19.1 fonttools-4.60.1 fsspec-2025.9.0 idna-3.10 imagesize-1.4.1 joblib-1.5.2 jsonschema-4.25.1 jsonschema-specifications-2025.9.1 jupyterlab-pygments-0.3.0 kiwisolver-1.4.9 llvmlite-0.45.0 matplotlib-3.10.6 mistune-3.1.4 mpmath-1.3.0 nbclient-0.10.2 nbconvert-7.16.6 nbformat-5.10.4 nbsphinx-0.9.7 networkx-3.5 numba-0.62.1 numpy-2.3.3 pandas-2.3.3 pandocfilters-1.5.1 pathlib2-2.3.7.post1 pillow-11.3.0 protobuf-6.32.1 pynndescent-0.5.13 pyparsing-3.2.5 pytz-2025.2 referencing-0.36.2 requests-2.32.5 rpds-py-0.27.1 scikit-learn-1.7.2 scipy-1.16.2 seaborn-0.13.2 snowballstemmer-3.0.1 soupsieve-2.8 sphinx-7.1.2 sphinx-rtd-theme-1.3.0rc1 sphinxcontrib-applehelp-2.0.0 sphinxcontrib-devhelp-2.0.0 sphinxcontrib-htmlhelp-2.1.0 sphinxcontrib-jquery-4.1 sphinxcontrib-jsmath-1.0.1 sphinxcontrib-qthelp-2.0.0 sphinxcontrib-serializinghtml-2.0.0 sympy-1.14.0 syng-bts-2.4 tensorboardX-2.6.4 threadpoolctl-3.6.0 tinycss2-1.4.0 torch-2.8.0 tqdm-4.67.1 tzdata-2025.2.post0 umap-learn-0.5.9.post2 urllib3-2.5.0 webencodings-0.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# restart kernel before reinstall updated package\n",
    "%pip install --index-url https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple/ syng-bts==2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from syng_bts.python.Experiments_new import *\n",
    "# from Experiments_new import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Read data, path is ../RealData/SKCMPositive_4.csv\n",
      "2. Determine the model is VAE1-10 with kl-weight = 10\n",
      "3. Determine the training parameters are epoch = early_stop off_aug = No learing rate = 0.0005 batch_frac = 0.1\n",
      "4. Pilot experiments start ... \n",
      "Training for data=SKCMPositive_4, model=epochES_batch01_VAE1-10, pilot size=100, for 1-th draw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yanjiechen/Documents/Github/SyNG-BTS/.conda/lib/python3.11/site-packages/syng_bts/python/helper_train_new.py:192: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
      "Consider using tensor.detach() first. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:836.)\n",
      "  print('Epoch: %03d/%03d | Batch %04d/%04d | Loss: %.4f'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/1000 | Batch 0000/0010 | Loss: 26375.0156\n",
      "Epoch: 001/1000 | Batch 0001/0010 | Loss: 27025.9316\n",
      "Epoch: 001/1000 | Batch 0002/0010 | Loss: 27630.9570\n",
      "Epoch: 001/1000 | Batch 0003/0010 | Loss: 27412.4551\n",
      "Epoch: 001/1000 | Batch 0004/0010 | Loss: 27923.8008\n",
      "Epoch: 001/1000 | Batch 0005/0010 | Loss: 26026.7344\n",
      "Epoch: 001/1000 | Batch 0006/0010 | Loss: 26995.7656\n",
      "Epoch: 001/1000 | Batch 0007/0010 | Loss: 27327.1602\n",
      "Epoch: 001/1000 | Batch 0008/0010 | Loss: 25349.2012\n",
      "Epoch: 001/1000 | Batch 0009/0010 | Loss: 27356.4805\n",
      "Time elapsed: 0.00 min\n",
      "Epoch: 002/1000 | Batch 0000/0010 | Loss: 27077.6035\n",
      "Epoch: 002/1000 | Batch 0001/0010 | Loss: 27939.8770\n",
      "Epoch: 002/1000 | Batch 0002/0010 | Loss: 24336.8008\n",
      "Epoch: 002/1000 | Batch 0003/0010 | Loss: 23893.9453\n",
      "Epoch: 002/1000 | Batch 0004/0010 | Loss: 24075.0039\n",
      "Epoch: 002/1000 | Batch 0005/0010 | Loss: 27311.3145\n",
      "Epoch: 002/1000 | Batch 0006/0010 | Loss: 24135.5312\n",
      "Epoch: 002/1000 | Batch 0007/0010 | Loss: 22999.2930\n",
      "Epoch: 002/1000 | Batch 0008/0010 | Loss: 21879.9121\n",
      "Epoch: 002/1000 | Batch 0009/0010 | Loss: 21328.3730\n",
      "Time elapsed: 0.00 min\n",
      "Epoch: 003/1000 | Batch 0000/0010 | Loss: 20622.6797\n",
      "Epoch: 003/1000 | Batch 0001/0010 | Loss: 17679.3379\n",
      "Epoch: 003/1000 | Batch 0002/0010 | Loss: 16553.8633\n",
      "Epoch: 003/1000 | Batch 0003/0010 | Loss: 14783.7627\n",
      "Epoch: 003/1000 | Batch 0004/0010 | Loss: 14753.3164\n",
      "Epoch: 003/1000 | Batch 0005/0010 | Loss: 11946.6836\n",
      "Epoch: 003/1000 | Batch 0006/0010 | Loss: 11275.3223\n",
      "Epoch: 003/1000 | Batch 0007/0010 | Loss: 10452.3486\n",
      "Epoch: 003/1000 | Batch 0008/0010 | Loss: 10258.1826\n",
      "Epoch: 003/1000 | Batch 0009/0010 | Loss: 9057.9219\n",
      "Time elapsed: 0.00 min\n",
      "Epoch: 004/1000 | Batch 0000/0010 | Loss: 8114.0215\n",
      "Epoch: 004/1000 | Batch 0001/0010 | Loss: 6918.6401\n",
      "Epoch: 004/1000 | Batch 0002/0010 | Loss: 6909.6196\n",
      "Epoch: 004/1000 | Batch 0003/0010 | Loss: 6827.4736\n",
      "Epoch: 004/1000 | Batch 0004/0010 | Loss: 6496.4385\n",
      "Epoch: 004/1000 | Batch 0005/0010 | Loss: 6001.9424\n",
      "Epoch: 004/1000 | Batch 0006/0010 | Loss: 5813.4092\n",
      "Epoch: 004/1000 | Batch 0007/0010 | Loss: 5530.9170\n",
      "Epoch: 004/1000 | Batch 0008/0010 | Loss: 5485.4844\n",
      "Epoch: 004/1000 | Batch 0009/0010 | Loss: 5322.8857\n",
      "Time elapsed: 0.00 min\n",
      "Epoch: 005/1000 | Batch 0000/0010 | Loss: 5424.8271\n",
      "Epoch: 005/1000 | Batch 0001/0010 | Loss: 5218.1436\n",
      "Epoch: 005/1000 | Batch 0002/0010 | Loss: 5404.1650\n",
      "Epoch: 005/1000 | Batch 0003/0010 | Loss: 5025.2505\n",
      "Epoch: 005/1000 | Batch 0004/0010 | Loss: 4755.6030\n",
      "Epoch: 005/1000 | Batch 0005/0010 | Loss: 5117.3340\n",
      "Epoch: 005/1000 | Batch 0006/0010 | Loss: 5129.6392\n",
      "Epoch: 005/1000 | Batch 0007/0010 | Loss: 4631.8975\n",
      "Epoch: 005/1000 | Batch 0008/0010 | Loss: 4740.7090\n",
      "Epoch: 005/1000 | Batch 0009/0010 | Loss: 4471.5576\n",
      "Time elapsed: 0.00 min\n",
      "Epoch: 006/1000 | Batch 0000/0010 | Loss: 4549.3779\n",
      "Epoch: 006/1000 | Batch 0001/0010 | Loss: 4716.6406\n",
      "Epoch: 006/1000 | Batch 0002/0010 | Loss: 4158.5693\n",
      "Epoch: 006/1000 | Batch 0003/0010 | Loss: 4711.0869\n",
      "Epoch: 006/1000 | Batch 0004/0010 | Loss: 4831.3198\n",
      "Epoch: 006/1000 | Batch 0005/0010 | Loss: 4395.6445\n",
      "Epoch: 006/1000 | Batch 0006/0010 | Loss: 4943.7651\n",
      "Epoch: 006/1000 | Batch 0007/0010 | Loss: 4634.8135\n",
      "Epoch: 006/1000 | Batch 0008/0010 | Loss: 4382.7212\n",
      "Epoch: 006/1000 | Batch 0009/0010 | Loss: 4443.2383\n",
      "Time elapsed: 0.00 min\n",
      "Epoch: 007/1000 | Batch 0000/0010 | Loss: 4185.7485\n",
      "Epoch: 007/1000 | Batch 0001/0010 | Loss: 4219.8857\n",
      "Epoch: 007/1000 | Batch 0002/0010 | Loss: 4424.9082\n",
      "Epoch: 007/1000 | Batch 0003/0010 | Loss: 4599.9155\n",
      "Epoch: 007/1000 | Batch 0004/0010 | Loss: 4299.6919\n",
      "Epoch: 007/1000 | Batch 0005/0010 | Loss: 4348.4619\n",
      "Epoch: 007/1000 | Batch 0006/0010 | Loss: 4339.2241\n",
      "Epoch: 007/1000 | Batch 0007/0010 | Loss: 4368.7783\n",
      "Epoch: 007/1000 | Batch 0008/0010 | Loss: 4078.4001\n",
      "Epoch: 007/1000 | Batch 0009/0010 | Loss: 3881.6074\n",
      "Time elapsed: 0.00 min\n",
      "Epoch: 008/1000 | Batch 0000/0010 | Loss: 4116.8125\n",
      "Epoch: 008/1000 | Batch 0001/0010 | Loss: 4505.1655\n",
      "Epoch: 008/1000 | Batch 0002/0010 | Loss: 4469.7144\n",
      "Epoch: 008/1000 | Batch 0003/0010 | Loss: 4568.7217\n",
      "Epoch: 008/1000 | Batch 0004/0010 | Loss: 4464.4404\n",
      "Epoch: 008/1000 | Batch 0005/0010 | Loss: 4226.2983\n",
      "Epoch: 008/1000 | Batch 0006/0010 | Loss: 4069.7190\n",
      "Epoch: 008/1000 | Batch 0007/0010 | Loss: 4295.1714\n",
      "Epoch: 008/1000 | Batch 0008/0010 | Loss: 4283.0728\n",
      "Epoch: 008/1000 | Batch 0009/0010 | Loss: 3965.1418\n",
      "Time elapsed: 0.00 min\n",
      "Epoch: 009/1000 | Batch 0000/0010 | Loss: 4038.0359\n",
      "Epoch: 009/1000 | Batch 0001/0010 | Loss: 4731.2573\n",
      "Epoch: 009/1000 | Batch 0002/0010 | Loss: 4118.4023\n",
      "Epoch: 009/1000 | Batch 0003/0010 | Loss: 4201.1709\n",
      "Epoch: 009/1000 | Batch 0004/0010 | Loss: 3895.3118\n",
      "Epoch: 009/1000 | Batch 0005/0010 | Loss: 4177.6206\n",
      "Epoch: 009/1000 | Batch 0006/0010 | Loss: 4100.1592\n",
      "Epoch: 009/1000 | Batch 0007/0010 | Loss: 4053.9734\n",
      "Epoch: 009/1000 | Batch 0008/0010 | Loss: 4599.4038\n",
      "Epoch: 009/1000 | Batch 0009/0010 | Loss: 4155.6348\n",
      "Time elapsed: 0.00 min\n",
      "Epoch: 010/1000 | Batch 0000/0010 | Loss: 4124.9355\n",
      "Epoch: 010/1000 | Batch 0001/0010 | Loss: 4512.9526\n",
      "Epoch: 010/1000 | Batch 0002/0010 | Loss: 4267.3662\n",
      "Epoch: 010/1000 | Batch 0003/0010 | Loss: 4383.4121\n",
      "Epoch: 010/1000 | Batch 0004/0010 | Loss: 3902.2874\n",
      "Epoch: 010/1000 | Batch 0005/0010 | Loss: 4725.1914\n",
      "Epoch: 010/1000 | Batch 0006/0010 | Loss: 3942.5186\n",
      "Epoch: 010/1000 | Batch 0007/0010 | Loss: 3936.2317\n",
      "Epoch: 010/1000 | Batch 0008/0010 | Loss: 4410.9868\n",
      "Epoch: 010/1000 | Batch 0009/0010 | Loss: 4256.8818\n",
      "Time elapsed: 0.00 min\n",
      "Epoch: 011/1000 | Batch 0000/0010 | Loss: 4242.4717\n",
      "Epoch: 011/1000 | Batch 0001/0010 | Loss: 4191.7231\n",
      "Epoch: 011/1000 | Batch 0002/0010 | Loss: 4088.6919\n",
      "Epoch: 011/1000 | Batch 0003/0010 | Loss: 4362.6802\n",
      "Epoch: 011/1000 | Batch 0004/0010 | Loss: 4078.8247\n",
      "Epoch: 011/1000 | Batch 0005/0010 | Loss: 3824.0286\n",
      "Epoch: 011/1000 | Batch 0006/0010 | Loss: 4076.3960\n",
      "Epoch: 011/1000 | Batch 0007/0010 | Loss: 4377.0435\n",
      "Epoch: 011/1000 | Batch 0008/0010 | Loss: 4180.0796\n",
      "Epoch: 011/1000 | Batch 0009/0010 | Loss: 4255.8271\n",
      "Time elapsed: 0.00 min\n",
      "Epoch: 012/1000 | Batch 0000/0010 | Loss: 4033.9529\n",
      "Epoch: 012/1000 | Batch 0001/0010 | Loss: 4034.6870\n",
      "Epoch: 012/1000 | Batch 0002/0010 | Loss: 4019.1086\n",
      "Epoch: 012/1000 | Batch 0003/0010 | Loss: 4196.7339\n",
      "Epoch: 012/1000 | Batch 0004/0010 | Loss: 4422.7334\n",
      "Epoch: 012/1000 | Batch 0005/0010 | Loss: 3954.8660\n",
      "Epoch: 012/1000 | Batch 0006/0010 | Loss: 3957.5127\n",
      "Epoch: 012/1000 | Batch 0007/0010 | Loss: 4399.1626\n",
      "Epoch: 012/1000 | Batch 0008/0010 | Loss: 3695.3347\n",
      "Epoch: 012/1000 | Batch 0009/0010 | Loss: 4348.6187\n",
      "Time elapsed: 0.00 min\n",
      "Epoch: 013/1000 | Batch 0000/0010 | Loss: 4122.6011\n",
      "Epoch: 013/1000 | Batch 0001/0010 | Loss: 4182.8560\n",
      "Epoch: 013/1000 | Batch 0002/0010 | Loss: 4079.7651\n",
      "Epoch: 013/1000 | Batch 0003/0010 | Loss: 4265.0894\n",
      "Epoch: 013/1000 | Batch 0004/0010 | Loss: 4424.5210\n",
      "Epoch: 013/1000 | Batch 0005/0010 | Loss: 3767.6650\n",
      "Epoch: 013/1000 | Batch 0006/0010 | Loss: 4113.0283\n",
      "Epoch: 013/1000 | Batch 0007/0010 | Loss: 4402.6172\n",
      "Epoch: 013/1000 | Batch 0008/0010 | Loss: 3921.4392\n",
      "Epoch: 013/1000 | Batch 0009/0010 | Loss: 3982.7456\n",
      "Time elapsed: 0.00 min\n",
      "Epoch: 014/1000 | Batch 0000/0010 | Loss: 4162.8896\n",
      "Epoch: 014/1000 | Batch 0001/0010 | Loss: 4045.5559\n",
      "Epoch: 014/1000 | Batch 0002/0010 | Loss: 4169.5591\n",
      "Epoch: 014/1000 | Batch 0003/0010 | Loss: 4488.6689\n",
      "Epoch: 014/1000 | Batch 0004/0010 | Loss: 4139.8740\n",
      "Epoch: 014/1000 | Batch 0005/0010 | Loss: 3951.1755\n",
      "Epoch: 014/1000 | Batch 0006/0010 | Loss: 4081.5107\n",
      "Epoch: 014/1000 | Batch 0007/0010 | Loss: 4045.5095\n",
      "Epoch: 014/1000 | Batch 0008/0010 | Loss: 4063.2534\n",
      "Epoch: 014/1000 | Batch 0009/0010 | Loss: 4077.1179\n",
      "Time elapsed: 0.00 min\n",
      "Epoch: 015/1000 | Batch 0000/0010 | Loss: 3907.8210\n",
      "Epoch: 015/1000 | Batch 0001/0010 | Loss: 4204.4663\n",
      "Epoch: 015/1000 | Batch 0002/0010 | Loss: 4029.7305\n",
      "Epoch: 015/1000 | Batch 0003/0010 | Loss: 3917.9421\n",
      "Epoch: 015/1000 | Batch 0004/0010 | Loss: 4641.8770\n",
      "Epoch: 015/1000 | Batch 0005/0010 | Loss: 4075.5857\n",
      "Epoch: 015/1000 | Batch 0006/0010 | Loss: 4069.9324\n",
      "Epoch: 015/1000 | Batch 0007/0010 | Loss: 4397.8247\n",
      "Epoch: 015/1000 | Batch 0008/0010 | Loss: 4422.3066\n",
      "Epoch: 015/1000 | Batch 0009/0010 | Loss: 3888.5378\n",
      "Time elapsed: 0.00 min\n",
      "Epoch: 016/1000 | Batch 0000/0010 | Loss: 4037.4338\n",
      "Epoch: 016/1000 | Batch 0001/0010 | Loss: 3816.1958\n",
      "Epoch: 016/1000 | Batch 0002/0010 | Loss: 4027.1489\n",
      "Epoch: 016/1000 | Batch 0003/0010 | Loss: 4226.5376\n",
      "Epoch: 016/1000 | Batch 0004/0010 | Loss: 4134.7759\n",
      "Epoch: 016/1000 | Batch 0005/0010 | Loss: 4057.1301\n",
      "Epoch: 016/1000 | Batch 0006/0010 | Loss: 4315.5439\n",
      "Epoch: 016/1000 | Batch 0007/0010 | Loss: 4133.2939\n",
      "Epoch: 016/1000 | Batch 0008/0010 | Loss: 4308.1489\n",
      "Epoch: 016/1000 | Batch 0009/0010 | Loss: 3939.1697\n",
      "Time elapsed: 0.00 min\n",
      "Epoch: 017/1000 | Batch 0000/0010 | Loss: 4010.5371\n",
      "Epoch: 017/1000 | Batch 0001/0010 | Loss: 4067.1436\n",
      "Epoch: 017/1000 | Batch 0002/0010 | Loss: 3888.7329\n",
      "Epoch: 017/1000 | Batch 0003/0010 | Loss: 4173.4590\n",
      "Epoch: 017/1000 | Batch 0004/0010 | Loss: 3902.7654\n",
      "Epoch: 017/1000 | Batch 0005/0010 | Loss: 4110.5142\n",
      "Epoch: 017/1000 | Batch 0006/0010 | Loss: 3868.5530\n",
      "Epoch: 017/1000 | Batch 0007/0010 | Loss: 4412.6235\n",
      "Epoch: 017/1000 | Batch 0008/0010 | Loss: 4293.7349\n",
      "Epoch: 017/1000 | Batch 0009/0010 | Loss: 4216.4429\n",
      "Time elapsed: 0.00 min\n",
      "Epoch: 018/1000 | Batch 0000/0010 | Loss: 4092.0747\n",
      "Epoch: 018/1000 | Batch 0001/0010 | Loss: 4353.0396\n",
      "Epoch: 018/1000 | Batch 0002/0010 | Loss: 4142.3193\n",
      "Epoch: 018/1000 | Batch 0003/0010 | Loss: 4087.5422\n",
      "Epoch: 018/1000 | Batch 0004/0010 | Loss: 4378.5322\n",
      "Epoch: 018/1000 | Batch 0005/0010 | Loss: 4066.5879\n",
      "Epoch: 018/1000 | Batch 0006/0010 | Loss: 4198.0571\n",
      "Epoch: 018/1000 | Batch 0007/0010 | Loss: 4109.8789\n",
      "Epoch: 018/1000 | Batch 0008/0010 | Loss: 4086.6729\n",
      "Epoch: 018/1000 | Batch 0009/0010 | Loss: 4051.7222\n",
      "Time elapsed: 0.00 min\n",
      "Epoch: 019/1000 | Batch 0000/0010 | Loss: 3919.1038\n",
      "Epoch: 019/1000 | Batch 0001/0010 | Loss: 4197.5957\n",
      "Epoch: 019/1000 | Batch 0002/0010 | Loss: 4270.3887\n",
      "Epoch: 019/1000 | Batch 0003/0010 | Loss: 3772.4775\n",
      "Epoch: 019/1000 | Batch 0004/0010 | Loss: 4198.0269\n",
      "Epoch: 019/1000 | Batch 0005/0010 | Loss: 4118.9639\n",
      "Epoch: 019/1000 | Batch 0006/0010 | Loss: 3980.5898\n",
      "Epoch: 019/1000 | Batch 0007/0010 | Loss: 4294.1030\n",
      "Epoch: 019/1000 | Batch 0008/0010 | Loss: 3992.0647\n",
      "Epoch: 019/1000 | Batch 0009/0010 | Loss: 3960.7012\n",
      "Time elapsed: 0.00 min\n",
      "Epoch: 020/1000 | Batch 0000/0010 | Loss: 4196.7158\n",
      "Epoch: 020/1000 | Batch 0001/0010 | Loss: 4063.9219\n",
      "Epoch: 020/1000 | Batch 0002/0010 | Loss: 3804.7058\n",
      "Epoch: 020/1000 | Batch 0003/0010 | Loss: 3906.7708\n",
      "Epoch: 020/1000 | Batch 0004/0010 | Loss: 3733.5510\n",
      "Epoch: 020/1000 | Batch 0005/0010 | Loss: 4314.1104\n",
      "Epoch: 020/1000 | Batch 0006/0010 | Loss: 4056.8503\n",
      "Epoch: 020/1000 | Batch 0007/0010 | Loss: 4293.6987\n",
      "Epoch: 020/1000 | Batch 0008/0010 | Loss: 3891.5161\n",
      "Epoch: 020/1000 | Batch 0009/0010 | Loss: 4356.7998\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 021/1000 | Batch 0000/0010 | Loss: 4217.8604\n",
      "Epoch: 021/1000 | Batch 0001/0010 | Loss: 3857.2705\n",
      "Epoch: 021/1000 | Batch 0002/0010 | Loss: 3882.6162\n",
      "Epoch: 021/1000 | Batch 0003/0010 | Loss: 3839.3567\n",
      "Epoch: 021/1000 | Batch 0004/0010 | Loss: 4107.9058\n",
      "Epoch: 021/1000 | Batch 0005/0010 | Loss: 4238.3682\n",
      "Epoch: 021/1000 | Batch 0006/0010 | Loss: 4015.2825\n",
      "Epoch: 021/1000 | Batch 0007/0010 | Loss: 4255.6509\n",
      "Epoch: 021/1000 | Batch 0008/0010 | Loss: 4194.6294\n",
      "Epoch: 021/1000 | Batch 0009/0010 | Loss: 4057.4390\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 022/1000 | Batch 0000/0010 | Loss: 3804.0183\n",
      "Epoch: 022/1000 | Batch 0001/0010 | Loss: 3981.0515\n",
      "Epoch: 022/1000 | Batch 0002/0010 | Loss: 3941.3066\n",
      "Epoch: 022/1000 | Batch 0003/0010 | Loss: 4147.9688\n",
      "Epoch: 022/1000 | Batch 0004/0010 | Loss: 4278.5801\n",
      "Epoch: 022/1000 | Batch 0005/0010 | Loss: 4191.7192\n",
      "Epoch: 022/1000 | Batch 0006/0010 | Loss: 4200.0020\n",
      "Epoch: 022/1000 | Batch 0007/0010 | Loss: 3960.8169\n",
      "Epoch: 022/1000 | Batch 0008/0010 | Loss: 3960.6064\n",
      "Epoch: 022/1000 | Batch 0009/0010 | Loss: 3957.7534\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 023/1000 | Batch 0000/0010 | Loss: 4444.3633\n",
      "Epoch: 023/1000 | Batch 0001/0010 | Loss: 3839.6826\n",
      "Epoch: 023/1000 | Batch 0002/0010 | Loss: 4063.9246\n",
      "Epoch: 023/1000 | Batch 0003/0010 | Loss: 3832.9048\n",
      "Epoch: 023/1000 | Batch 0004/0010 | Loss: 4076.3298\n",
      "Epoch: 023/1000 | Batch 0005/0010 | Loss: 4209.7905\n",
      "Epoch: 023/1000 | Batch 0006/0010 | Loss: 4027.2500\n",
      "Epoch: 023/1000 | Batch 0007/0010 | Loss: 3974.0447\n",
      "Epoch: 023/1000 | Batch 0008/0010 | Loss: 3756.1389\n",
      "Epoch: 023/1000 | Batch 0009/0010 | Loss: 4362.8833\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 024/1000 | Batch 0000/0010 | Loss: 4087.5166\n",
      "Epoch: 024/1000 | Batch 0001/0010 | Loss: 4018.5239\n",
      "Epoch: 024/1000 | Batch 0002/0010 | Loss: 3939.2603\n",
      "Epoch: 024/1000 | Batch 0003/0010 | Loss: 4137.9238\n",
      "Epoch: 024/1000 | Batch 0004/0010 | Loss: 4016.5762\n",
      "Epoch: 024/1000 | Batch 0005/0010 | Loss: 4077.0234\n",
      "Epoch: 024/1000 | Batch 0006/0010 | Loss: 3827.7581\n",
      "Epoch: 024/1000 | Batch 0007/0010 | Loss: 4291.0664\n",
      "Epoch: 024/1000 | Batch 0008/0010 | Loss: 3999.8315\n",
      "Epoch: 024/1000 | Batch 0009/0010 | Loss: 4073.1235\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 025/1000 | Batch 0000/0010 | Loss: 3974.8623\n",
      "Epoch: 025/1000 | Batch 0001/0010 | Loss: 4070.7378\n",
      "Epoch: 025/1000 | Batch 0002/0010 | Loss: 3492.0850\n",
      "Epoch: 025/1000 | Batch 0003/0010 | Loss: 4411.4370\n",
      "Epoch: 025/1000 | Batch 0004/0010 | Loss: 4416.5518\n",
      "Epoch: 025/1000 | Batch 0005/0010 | Loss: 4271.6924\n",
      "Epoch: 025/1000 | Batch 0006/0010 | Loss: 4096.8335\n",
      "Epoch: 025/1000 | Batch 0007/0010 | Loss: 4263.9277\n",
      "Epoch: 025/1000 | Batch 0008/0010 | Loss: 3956.2239\n",
      "Epoch: 025/1000 | Batch 0009/0010 | Loss: 4068.3069\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 026/1000 | Batch 0000/0010 | Loss: 4054.9470\n",
      "Epoch: 026/1000 | Batch 0001/0010 | Loss: 4134.5518\n",
      "Epoch: 026/1000 | Batch 0002/0010 | Loss: 3807.3313\n",
      "Epoch: 026/1000 | Batch 0003/0010 | Loss: 4064.3521\n",
      "Epoch: 026/1000 | Batch 0004/0010 | Loss: 3680.1313\n",
      "Epoch: 026/1000 | Batch 0005/0010 | Loss: 3746.6455\n",
      "Epoch: 026/1000 | Batch 0006/0010 | Loss: 4278.2056\n",
      "Epoch: 026/1000 | Batch 0007/0010 | Loss: 4032.3999\n",
      "Epoch: 026/1000 | Batch 0008/0010 | Loss: 4155.7280\n",
      "Epoch: 026/1000 | Batch 0009/0010 | Loss: 4304.1899\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 027/1000 | Batch 0000/0010 | Loss: 4298.2964\n",
      "Epoch: 027/1000 | Batch 0001/0010 | Loss: 4435.2412\n",
      "Epoch: 027/1000 | Batch 0002/0010 | Loss: 3976.4077\n",
      "Epoch: 027/1000 | Batch 0003/0010 | Loss: 3937.1575\n",
      "Epoch: 027/1000 | Batch 0004/0010 | Loss: 3708.9043\n",
      "Epoch: 027/1000 | Batch 0005/0010 | Loss: 4115.3931\n",
      "Epoch: 027/1000 | Batch 0006/0010 | Loss: 3927.1868\n",
      "Epoch: 027/1000 | Batch 0007/0010 | Loss: 3933.0137\n",
      "Epoch: 027/1000 | Batch 0008/0010 | Loss: 4290.9512\n",
      "Epoch: 027/1000 | Batch 0009/0010 | Loss: 3952.4866\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 028/1000 | Batch 0000/0010 | Loss: 4031.7957\n",
      "Epoch: 028/1000 | Batch 0001/0010 | Loss: 4060.4883\n",
      "Epoch: 028/1000 | Batch 0002/0010 | Loss: 3879.7341\n",
      "Epoch: 028/1000 | Batch 0003/0010 | Loss: 4172.0791\n",
      "Epoch: 028/1000 | Batch 0004/0010 | Loss: 3717.9419\n",
      "Epoch: 028/1000 | Batch 0005/0010 | Loss: 4120.2266\n",
      "Epoch: 028/1000 | Batch 0006/0010 | Loss: 4206.3296\n",
      "Epoch: 028/1000 | Batch 0007/0010 | Loss: 4222.2290\n",
      "Epoch: 028/1000 | Batch 0008/0010 | Loss: 4129.8960\n",
      "Epoch: 028/1000 | Batch 0009/0010 | Loss: 4127.8267\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 029/1000 | Batch 0000/0010 | Loss: 4057.9253\n",
      "Epoch: 029/1000 | Batch 0001/0010 | Loss: 3995.7754\n",
      "Epoch: 029/1000 | Batch 0002/0010 | Loss: 4281.1133\n",
      "Epoch: 029/1000 | Batch 0003/0010 | Loss: 3994.2078\n",
      "Epoch: 029/1000 | Batch 0004/0010 | Loss: 3961.2690\n",
      "Epoch: 029/1000 | Batch 0005/0010 | Loss: 3896.1738\n",
      "Epoch: 029/1000 | Batch 0006/0010 | Loss: 3975.1721\n",
      "Epoch: 029/1000 | Batch 0007/0010 | Loss: 4071.5952\n",
      "Epoch: 029/1000 | Batch 0008/0010 | Loss: 4148.0210\n",
      "Epoch: 029/1000 | Batch 0009/0010 | Loss: 3970.1787\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 030/1000 | Batch 0000/0010 | Loss: 3873.0137\n",
      "Epoch: 030/1000 | Batch 0001/0010 | Loss: 4056.6956\n",
      "Epoch: 030/1000 | Batch 0002/0010 | Loss: 3900.7322\n",
      "Epoch: 030/1000 | Batch 0003/0010 | Loss: 4111.7998\n",
      "Epoch: 030/1000 | Batch 0004/0010 | Loss: 4035.7236\n",
      "Epoch: 030/1000 | Batch 0005/0010 | Loss: 4031.7537\n",
      "Epoch: 030/1000 | Batch 0006/0010 | Loss: 4166.7397\n",
      "Epoch: 030/1000 | Batch 0007/0010 | Loss: 3977.6094\n",
      "Epoch: 030/1000 | Batch 0008/0010 | Loss: 4047.5925\n",
      "Epoch: 030/1000 | Batch 0009/0010 | Loss: 3982.9023\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 031/1000 | Batch 0000/0010 | Loss: 4029.5854\n",
      "Epoch: 031/1000 | Batch 0001/0010 | Loss: 4274.4375\n",
      "Epoch: 031/1000 | Batch 0002/0010 | Loss: 4001.4465\n",
      "Epoch: 031/1000 | Batch 0003/0010 | Loss: 3924.8120\n",
      "Epoch: 031/1000 | Batch 0004/0010 | Loss: 3968.7537\n",
      "Epoch: 031/1000 | Batch 0005/0010 | Loss: 4233.6479\n",
      "Epoch: 031/1000 | Batch 0006/0010 | Loss: 4131.9971\n",
      "Epoch: 031/1000 | Batch 0007/0010 | Loss: 4089.4482\n",
      "Epoch: 031/1000 | Batch 0008/0010 | Loss: 3797.3777\n",
      "Epoch: 031/1000 | Batch 0009/0010 | Loss: 3896.6487\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 032/1000 | Batch 0000/0010 | Loss: 4132.9590\n",
      "Epoch: 032/1000 | Batch 0001/0010 | Loss: 4001.6868\n",
      "Epoch: 032/1000 | Batch 0002/0010 | Loss: 4025.7749\n",
      "Epoch: 032/1000 | Batch 0003/0010 | Loss: 4221.4775\n",
      "Epoch: 032/1000 | Batch 0004/0010 | Loss: 3779.2722\n",
      "Epoch: 032/1000 | Batch 0005/0010 | Loss: 4190.4507\n",
      "Epoch: 032/1000 | Batch 0006/0010 | Loss: 3888.5750\n",
      "Epoch: 032/1000 | Batch 0007/0010 | Loss: 3961.5757\n",
      "Epoch: 032/1000 | Batch 0008/0010 | Loss: 3802.8704\n",
      "Epoch: 032/1000 | Batch 0009/0010 | Loss: 3845.0037\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 033/1000 | Batch 0000/0010 | Loss: 3891.6526\n",
      "Epoch: 033/1000 | Batch 0001/0010 | Loss: 3929.9392\n",
      "Epoch: 033/1000 | Batch 0002/0010 | Loss: 4036.3320\n",
      "Epoch: 033/1000 | Batch 0003/0010 | Loss: 4249.3838\n",
      "Epoch: 033/1000 | Batch 0004/0010 | Loss: 3908.6667\n",
      "Epoch: 033/1000 | Batch 0005/0010 | Loss: 4289.8423\n",
      "Epoch: 033/1000 | Batch 0006/0010 | Loss: 3981.0488\n",
      "Epoch: 033/1000 | Batch 0007/0010 | Loss: 3819.2317\n",
      "Epoch: 033/1000 | Batch 0008/0010 | Loss: 3726.0835\n",
      "Epoch: 033/1000 | Batch 0009/0010 | Loss: 3940.0181\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 034/1000 | Batch 0000/0010 | Loss: 4099.5957\n",
      "Epoch: 034/1000 | Batch 0001/0010 | Loss: 4184.9038\n",
      "Epoch: 034/1000 | Batch 0002/0010 | Loss: 3825.4399\n",
      "Epoch: 034/1000 | Batch 0003/0010 | Loss: 3900.7302\n",
      "Epoch: 034/1000 | Batch 0004/0010 | Loss: 3697.5288\n",
      "Epoch: 034/1000 | Batch 0005/0010 | Loss: 3807.4014\n",
      "Epoch: 034/1000 | Batch 0006/0010 | Loss: 4021.7024\n",
      "Epoch: 034/1000 | Batch 0007/0010 | Loss: 4026.0857\n",
      "Epoch: 034/1000 | Batch 0008/0010 | Loss: 4184.3140\n",
      "Epoch: 034/1000 | Batch 0009/0010 | Loss: 3856.3484\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 035/1000 | Batch 0000/0010 | Loss: 3707.7024\n",
      "Epoch: 035/1000 | Batch 0001/0010 | Loss: 4252.0918\n",
      "Epoch: 035/1000 | Batch 0002/0010 | Loss: 4215.4717\n",
      "Epoch: 035/1000 | Batch 0003/0010 | Loss: 3779.4490\n",
      "Epoch: 035/1000 | Batch 0004/0010 | Loss: 4042.5042\n",
      "Epoch: 035/1000 | Batch 0005/0010 | Loss: 3950.0996\n",
      "Epoch: 035/1000 | Batch 0006/0010 | Loss: 4075.4514\n",
      "Epoch: 035/1000 | Batch 0007/0010 | Loss: 3899.1987\n",
      "Epoch: 035/1000 | Batch 0008/0010 | Loss: 3689.0132\n",
      "Epoch: 035/1000 | Batch 0009/0010 | Loss: 4021.5876\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 036/1000 | Batch 0000/0010 | Loss: 3798.9131\n",
      "Epoch: 036/1000 | Batch 0001/0010 | Loss: 3960.1409\n",
      "Epoch: 036/1000 | Batch 0002/0010 | Loss: 3853.6602\n",
      "Epoch: 036/1000 | Batch 0003/0010 | Loss: 3938.0251\n",
      "Epoch: 036/1000 | Batch 0004/0010 | Loss: 4359.0361\n",
      "Epoch: 036/1000 | Batch 0005/0010 | Loss: 3777.5623\n",
      "Epoch: 036/1000 | Batch 0006/0010 | Loss: 4354.7212\n",
      "Epoch: 036/1000 | Batch 0007/0010 | Loss: 3807.4182\n",
      "Epoch: 036/1000 | Batch 0008/0010 | Loss: 4052.9163\n",
      "Epoch: 036/1000 | Batch 0009/0010 | Loss: 3854.9434\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 037/1000 | Batch 0000/0010 | Loss: 4365.7163\n",
      "Epoch: 037/1000 | Batch 0001/0010 | Loss: 4085.1401\n",
      "Epoch: 037/1000 | Batch 0002/0010 | Loss: 3829.7034\n",
      "Epoch: 037/1000 | Batch 0003/0010 | Loss: 3774.7639\n",
      "Epoch: 037/1000 | Batch 0004/0010 | Loss: 4015.4060\n",
      "Epoch: 037/1000 | Batch 0005/0010 | Loss: 4011.2461\n",
      "Epoch: 037/1000 | Batch 0006/0010 | Loss: 3719.0757\n",
      "Epoch: 037/1000 | Batch 0007/0010 | Loss: 4131.0742\n",
      "Epoch: 037/1000 | Batch 0008/0010 | Loss: 3756.8479\n",
      "Epoch: 037/1000 | Batch 0009/0010 | Loss: 4085.5703\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 038/1000 | Batch 0000/0010 | Loss: 4174.0576\n",
      "Epoch: 038/1000 | Batch 0001/0010 | Loss: 4074.5608\n",
      "Epoch: 038/1000 | Batch 0002/0010 | Loss: 3996.5806\n",
      "Epoch: 038/1000 | Batch 0003/0010 | Loss: 3908.3538\n",
      "Epoch: 038/1000 | Batch 0004/0010 | Loss: 3865.8606\n",
      "Epoch: 038/1000 | Batch 0005/0010 | Loss: 4015.7051\n",
      "Epoch: 038/1000 | Batch 0006/0010 | Loss: 3948.1064\n",
      "Epoch: 038/1000 | Batch 0007/0010 | Loss: 4051.6152\n",
      "Epoch: 038/1000 | Batch 0008/0010 | Loss: 3751.4373\n",
      "Epoch: 038/1000 | Batch 0009/0010 | Loss: 3680.4119\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 039/1000 | Batch 0000/0010 | Loss: 3858.7087\n",
      "Epoch: 039/1000 | Batch 0001/0010 | Loss: 3993.1487\n",
      "Epoch: 039/1000 | Batch 0002/0010 | Loss: 3953.2769\n",
      "Epoch: 039/1000 | Batch 0003/0010 | Loss: 4181.1338\n",
      "Epoch: 039/1000 | Batch 0004/0010 | Loss: 4188.8657\n",
      "Epoch: 039/1000 | Batch 0005/0010 | Loss: 4056.1077\n",
      "Epoch: 039/1000 | Batch 0006/0010 | Loss: 4116.4536\n",
      "Epoch: 039/1000 | Batch 0007/0010 | Loss: 3837.6438\n",
      "Epoch: 039/1000 | Batch 0008/0010 | Loss: 3677.6177\n",
      "Epoch: 039/1000 | Batch 0009/0010 | Loss: 3872.2183\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 040/1000 | Batch 0000/0010 | Loss: 4107.3574\n",
      "Epoch: 040/1000 | Batch 0001/0010 | Loss: 4042.7996\n",
      "Epoch: 040/1000 | Batch 0002/0010 | Loss: 4184.8291\n",
      "Epoch: 040/1000 | Batch 0003/0010 | Loss: 3694.4546\n",
      "Epoch: 040/1000 | Batch 0004/0010 | Loss: 3889.8811\n",
      "Epoch: 040/1000 | Batch 0005/0010 | Loss: 3949.2083\n",
      "Epoch: 040/1000 | Batch 0006/0010 | Loss: 3785.0522\n",
      "Epoch: 040/1000 | Batch 0007/0010 | Loss: 3908.8887\n",
      "Epoch: 040/1000 | Batch 0008/0010 | Loss: 3982.4277\n",
      "Epoch: 040/1000 | Batch 0009/0010 | Loss: 3935.3889\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 041/1000 | Batch 0000/0010 | Loss: 4058.5391\n",
      "Epoch: 041/1000 | Batch 0001/0010 | Loss: 3736.6575\n",
      "Epoch: 041/1000 | Batch 0002/0010 | Loss: 3847.9463\n",
      "Epoch: 041/1000 | Batch 0003/0010 | Loss: 3870.3704\n",
      "Epoch: 041/1000 | Batch 0004/0010 | Loss: 4172.5171\n",
      "Epoch: 041/1000 | Batch 0005/0010 | Loss: 4196.7500\n",
      "Epoch: 041/1000 | Batch 0006/0010 | Loss: 3923.5762\n",
      "Epoch: 041/1000 | Batch 0007/0010 | Loss: 3944.5823\n",
      "Epoch: 041/1000 | Batch 0008/0010 | Loss: 3851.7615\n",
      "Epoch: 041/1000 | Batch 0009/0010 | Loss: 3959.2949\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 042/1000 | Batch 0000/0010 | Loss: 4281.0601\n",
      "Epoch: 042/1000 | Batch 0001/0010 | Loss: 3834.1257\n",
      "Epoch: 042/1000 | Batch 0002/0010 | Loss: 3811.1006\n",
      "Epoch: 042/1000 | Batch 0003/0010 | Loss: 3879.6841\n",
      "Epoch: 042/1000 | Batch 0004/0010 | Loss: 4193.8643\n",
      "Epoch: 042/1000 | Batch 0005/0010 | Loss: 3716.1638\n",
      "Epoch: 042/1000 | Batch 0006/0010 | Loss: 4123.4209\n",
      "Epoch: 042/1000 | Batch 0007/0010 | Loss: 3958.9836\n",
      "Epoch: 042/1000 | Batch 0008/0010 | Loss: 4272.8350\n",
      "Epoch: 042/1000 | Batch 0009/0010 | Loss: 3906.3389\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 043/1000 | Batch 0000/0010 | Loss: 3966.6426\n",
      "Epoch: 043/1000 | Batch 0001/0010 | Loss: 4009.8118\n",
      "Epoch: 043/1000 | Batch 0002/0010 | Loss: 4020.0535\n",
      "Epoch: 043/1000 | Batch 0003/0010 | Loss: 3917.7341\n",
      "Epoch: 043/1000 | Batch 0004/0010 | Loss: 4239.9375\n",
      "Epoch: 043/1000 | Batch 0005/0010 | Loss: 4073.9565\n",
      "Epoch: 043/1000 | Batch 0006/0010 | Loss: 3873.7576\n",
      "Epoch: 043/1000 | Batch 0007/0010 | Loss: 3893.4978\n",
      "Epoch: 043/1000 | Batch 0008/0010 | Loss: 3821.6946\n",
      "Epoch: 043/1000 | Batch 0009/0010 | Loss: 3785.5234\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 044/1000 | Batch 0000/0010 | Loss: 3904.1619\n",
      "Epoch: 044/1000 | Batch 0001/0010 | Loss: 4027.0405\n",
      "Epoch: 044/1000 | Batch 0002/0010 | Loss: 3983.4504\n",
      "Epoch: 044/1000 | Batch 0003/0010 | Loss: 3990.4309\n",
      "Epoch: 044/1000 | Batch 0004/0010 | Loss: 4026.1162\n",
      "Epoch: 044/1000 | Batch 0005/0010 | Loss: 3890.9263\n",
      "Epoch: 044/1000 | Batch 0006/0010 | Loss: 4242.6455\n",
      "Epoch: 044/1000 | Batch 0007/0010 | Loss: 3987.9304\n",
      "Epoch: 044/1000 | Batch 0008/0010 | Loss: 3691.2351\n",
      "Epoch: 044/1000 | Batch 0009/0010 | Loss: 3883.8552\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 045/1000 | Batch 0000/0010 | Loss: 3968.0947\n",
      "Epoch: 045/1000 | Batch 0001/0010 | Loss: 4014.2063\n",
      "Epoch: 045/1000 | Batch 0002/0010 | Loss: 3851.2651\n",
      "Epoch: 045/1000 | Batch 0003/0010 | Loss: 3941.4233\n",
      "Epoch: 045/1000 | Batch 0004/0010 | Loss: 3753.6487\n",
      "Epoch: 045/1000 | Batch 0005/0010 | Loss: 3814.0554\n",
      "Epoch: 045/1000 | Batch 0006/0010 | Loss: 4063.4722\n",
      "Epoch: 045/1000 | Batch 0007/0010 | Loss: 3960.0366\n",
      "Epoch: 045/1000 | Batch 0008/0010 | Loss: 3856.0034\n",
      "Epoch: 045/1000 | Batch 0009/0010 | Loss: 3841.6045\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 046/1000 | Batch 0000/0010 | Loss: 3943.8174\n",
      "Epoch: 046/1000 | Batch 0001/0010 | Loss: 4053.2463\n",
      "Epoch: 046/1000 | Batch 0002/0010 | Loss: 3922.5547\n",
      "Epoch: 046/1000 | Batch 0003/0010 | Loss: 4044.8743\n",
      "Epoch: 046/1000 | Batch 0004/0010 | Loss: 4023.7056\n",
      "Epoch: 046/1000 | Batch 0005/0010 | Loss: 4359.5645\n",
      "Epoch: 046/1000 | Batch 0006/0010 | Loss: 3640.9441\n",
      "Epoch: 046/1000 | Batch 0007/0010 | Loss: 3668.1240\n",
      "Epoch: 046/1000 | Batch 0008/0010 | Loss: 3929.2202\n",
      "Epoch: 046/1000 | Batch 0009/0010 | Loss: 3769.0859\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 047/1000 | Batch 0000/0010 | Loss: 3905.1313\n",
      "Epoch: 047/1000 | Batch 0001/0010 | Loss: 4159.8838\n",
      "Epoch: 047/1000 | Batch 0002/0010 | Loss: 3729.5386\n",
      "Epoch: 047/1000 | Batch 0003/0010 | Loss: 3924.9348\n",
      "Epoch: 047/1000 | Batch 0004/0010 | Loss: 3749.4663\n",
      "Epoch: 047/1000 | Batch 0005/0010 | Loss: 3973.9866\n",
      "Epoch: 047/1000 | Batch 0006/0010 | Loss: 3929.6865\n",
      "Epoch: 047/1000 | Batch 0007/0010 | Loss: 3611.1228\n",
      "Epoch: 047/1000 | Batch 0008/0010 | Loss: 4212.9336\n",
      "Epoch: 047/1000 | Batch 0009/0010 | Loss: 4086.6602\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 048/1000 | Batch 0000/0010 | Loss: 4242.0781\n",
      "Epoch: 048/1000 | Batch 0001/0010 | Loss: 3812.2737\n",
      "Epoch: 048/1000 | Batch 0002/0010 | Loss: 4255.7476\n",
      "Epoch: 048/1000 | Batch 0003/0010 | Loss: 3775.4097\n",
      "Epoch: 048/1000 | Batch 0004/0010 | Loss: 3738.2117\n",
      "Epoch: 048/1000 | Batch 0005/0010 | Loss: 3841.9768\n",
      "Epoch: 048/1000 | Batch 0006/0010 | Loss: 3864.0510\n",
      "Epoch: 048/1000 | Batch 0007/0010 | Loss: 4069.9475\n",
      "Epoch: 048/1000 | Batch 0008/0010 | Loss: 3579.1379\n",
      "Epoch: 048/1000 | Batch 0009/0010 | Loss: 3919.1006\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 049/1000 | Batch 0000/0010 | Loss: 3824.9526\n",
      "Epoch: 049/1000 | Batch 0001/0010 | Loss: 3717.1685\n",
      "Epoch: 049/1000 | Batch 0002/0010 | Loss: 3909.6921\n",
      "Epoch: 049/1000 | Batch 0003/0010 | Loss: 3875.5410\n",
      "Epoch: 049/1000 | Batch 0004/0010 | Loss: 3956.7656\n",
      "Epoch: 049/1000 | Batch 0005/0010 | Loss: 3823.1096\n",
      "Epoch: 049/1000 | Batch 0006/0010 | Loss: 4129.0669\n",
      "Epoch: 049/1000 | Batch 0007/0010 | Loss: 3877.8994\n",
      "Epoch: 049/1000 | Batch 0008/0010 | Loss: 4084.4578\n",
      "Epoch: 049/1000 | Batch 0009/0010 | Loss: 3744.6252\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 050/1000 | Batch 0000/0010 | Loss: 3976.7983\n",
      "Epoch: 050/1000 | Batch 0001/0010 | Loss: 3866.1357\n",
      "Epoch: 050/1000 | Batch 0002/0010 | Loss: 3937.2725\n",
      "Epoch: 050/1000 | Batch 0003/0010 | Loss: 3894.5583\n",
      "Epoch: 050/1000 | Batch 0004/0010 | Loss: 3834.1860\n",
      "Epoch: 050/1000 | Batch 0005/0010 | Loss: 3922.2349\n",
      "Epoch: 050/1000 | Batch 0006/0010 | Loss: 3866.4839\n",
      "Epoch: 050/1000 | Batch 0007/0010 | Loss: 3752.8772\n",
      "Epoch: 050/1000 | Batch 0008/0010 | Loss: 3975.8301\n",
      "Epoch: 050/1000 | Batch 0009/0010 | Loss: 3914.6487\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 051/1000 | Batch 0000/0010 | Loss: 4118.4805\n",
      "Epoch: 051/1000 | Batch 0001/0010 | Loss: 3945.1748\n",
      "Epoch: 051/1000 | Batch 0002/0010 | Loss: 3944.8818\n",
      "Epoch: 051/1000 | Batch 0003/0010 | Loss: 3782.6296\n",
      "Epoch: 051/1000 | Batch 0004/0010 | Loss: 3992.9246\n",
      "Epoch: 051/1000 | Batch 0005/0010 | Loss: 4034.2397\n",
      "Epoch: 051/1000 | Batch 0006/0010 | Loss: 3859.1367\n",
      "Epoch: 051/1000 | Batch 0007/0010 | Loss: 3916.3789\n",
      "Epoch: 051/1000 | Batch 0008/0010 | Loss: 3632.3279\n",
      "Epoch: 051/1000 | Batch 0009/0010 | Loss: 3893.1448\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 052/1000 | Batch 0000/0010 | Loss: 3890.0413\n",
      "Epoch: 052/1000 | Batch 0001/0010 | Loss: 3927.0767\n",
      "Epoch: 052/1000 | Batch 0002/0010 | Loss: 4106.9751\n",
      "Epoch: 052/1000 | Batch 0003/0010 | Loss: 4031.4756\n",
      "Epoch: 052/1000 | Batch 0004/0010 | Loss: 3937.1333\n",
      "Epoch: 052/1000 | Batch 0005/0010 | Loss: 3456.4629\n",
      "Epoch: 052/1000 | Batch 0006/0010 | Loss: 4007.1235\n",
      "Epoch: 052/1000 | Batch 0007/0010 | Loss: 3871.9546\n",
      "Epoch: 052/1000 | Batch 0008/0010 | Loss: 3972.8289\n",
      "Epoch: 052/1000 | Batch 0009/0010 | Loss: 4091.8149\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 053/1000 | Batch 0000/0010 | Loss: 3772.1816\n",
      "Epoch: 053/1000 | Batch 0001/0010 | Loss: 3908.0190\n",
      "Epoch: 053/1000 | Batch 0002/0010 | Loss: 4023.0864\n",
      "Epoch: 053/1000 | Batch 0003/0010 | Loss: 3931.6357\n",
      "Epoch: 053/1000 | Batch 0004/0010 | Loss: 3760.7556\n",
      "Epoch: 053/1000 | Batch 0005/0010 | Loss: 4119.8364\n",
      "Epoch: 053/1000 | Batch 0006/0010 | Loss: 3859.5061\n",
      "Epoch: 053/1000 | Batch 0007/0010 | Loss: 3898.3909\n",
      "Epoch: 053/1000 | Batch 0008/0010 | Loss: 4162.5278\n",
      "Epoch: 053/1000 | Batch 0009/0010 | Loss: 4018.5310\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 054/1000 | Batch 0000/0010 | Loss: 4045.9966\n",
      "Epoch: 054/1000 | Batch 0001/0010 | Loss: 3834.1484\n",
      "Epoch: 054/1000 | Batch 0002/0010 | Loss: 3743.6067\n",
      "Epoch: 054/1000 | Batch 0003/0010 | Loss: 3872.0859\n",
      "Epoch: 054/1000 | Batch 0004/0010 | Loss: 4109.4756\n",
      "Epoch: 054/1000 | Batch 0005/0010 | Loss: 3783.6204\n",
      "Epoch: 054/1000 | Batch 0006/0010 | Loss: 3537.5896\n",
      "Epoch: 054/1000 | Batch 0007/0010 | Loss: 4145.9536\n",
      "Epoch: 054/1000 | Batch 0008/0010 | Loss: 3757.5125\n",
      "Epoch: 054/1000 | Batch 0009/0010 | Loss: 4222.0288\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 055/1000 | Batch 0000/0010 | Loss: 3934.6721\n",
      "Epoch: 055/1000 | Batch 0001/0010 | Loss: 3630.1694\n",
      "Epoch: 055/1000 | Batch 0002/0010 | Loss: 3857.0391\n",
      "Epoch: 055/1000 | Batch 0003/0010 | Loss: 3834.2539\n",
      "Epoch: 055/1000 | Batch 0004/0010 | Loss: 4207.5464\n",
      "Epoch: 055/1000 | Batch 0005/0010 | Loss: 3928.2612\n",
      "Epoch: 055/1000 | Batch 0006/0010 | Loss: 3978.3979\n",
      "Epoch: 055/1000 | Batch 0007/0010 | Loss: 3871.8081\n",
      "Epoch: 055/1000 | Batch 0008/0010 | Loss: 3866.4055\n",
      "Epoch: 055/1000 | Batch 0009/0010 | Loss: 3873.6489\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 056/1000 | Batch 0000/0010 | Loss: 3915.2168\n",
      "Epoch: 056/1000 | Batch 0001/0010 | Loss: 3940.6646\n",
      "Epoch: 056/1000 | Batch 0002/0010 | Loss: 3826.3882\n",
      "Epoch: 056/1000 | Batch 0003/0010 | Loss: 3937.2021\n",
      "Epoch: 056/1000 | Batch 0004/0010 | Loss: 3855.8464\n",
      "Epoch: 056/1000 | Batch 0005/0010 | Loss: 3893.8665\n",
      "Epoch: 056/1000 | Batch 0006/0010 | Loss: 3704.3479\n",
      "Epoch: 056/1000 | Batch 0007/0010 | Loss: 4013.1121\n",
      "Epoch: 056/1000 | Batch 0008/0010 | Loss: 3796.7366\n",
      "Epoch: 056/1000 | Batch 0009/0010 | Loss: 4149.5049\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 057/1000 | Batch 0000/0010 | Loss: 3713.0325\n",
      "Epoch: 057/1000 | Batch 0001/0010 | Loss: 3899.4312\n",
      "Epoch: 057/1000 | Batch 0002/0010 | Loss: 3861.2485\n",
      "Epoch: 057/1000 | Batch 0003/0010 | Loss: 3890.3186\n",
      "Epoch: 057/1000 | Batch 0004/0010 | Loss: 4036.6326\n",
      "Epoch: 057/1000 | Batch 0005/0010 | Loss: 3821.7820\n",
      "Epoch: 057/1000 | Batch 0006/0010 | Loss: 3808.9685\n",
      "Epoch: 057/1000 | Batch 0007/0010 | Loss: 3906.5090\n",
      "Epoch: 057/1000 | Batch 0008/0010 | Loss: 3968.8445\n",
      "Epoch: 057/1000 | Batch 0009/0010 | Loss: 4056.0178\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 058/1000 | Batch 0000/0010 | Loss: 4191.8853\n",
      "Epoch: 058/1000 | Batch 0001/0010 | Loss: 4018.0649\n",
      "Epoch: 058/1000 | Batch 0002/0010 | Loss: 3946.1824\n",
      "Epoch: 058/1000 | Batch 0003/0010 | Loss: 3821.2722\n",
      "Epoch: 058/1000 | Batch 0004/0010 | Loss: 4103.3901\n",
      "Epoch: 058/1000 | Batch 0005/0010 | Loss: 3882.8188\n",
      "Epoch: 058/1000 | Batch 0006/0010 | Loss: 3686.3584\n",
      "Epoch: 058/1000 | Batch 0007/0010 | Loss: 3771.3152\n",
      "Epoch: 058/1000 | Batch 0008/0010 | Loss: 3905.4260\n",
      "Epoch: 058/1000 | Batch 0009/0010 | Loss: 3748.5237\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 059/1000 | Batch 0000/0010 | Loss: 3711.4434\n",
      "Epoch: 059/1000 | Batch 0001/0010 | Loss: 3820.9343\n",
      "Epoch: 059/1000 | Batch 0002/0010 | Loss: 3740.4856\n",
      "Epoch: 059/1000 | Batch 0003/0010 | Loss: 3776.9722\n",
      "Epoch: 059/1000 | Batch 0004/0010 | Loss: 3970.7629\n",
      "Epoch: 059/1000 | Batch 0005/0010 | Loss: 3988.6697\n",
      "Epoch: 059/1000 | Batch 0006/0010 | Loss: 3698.9595\n",
      "Epoch: 059/1000 | Batch 0007/0010 | Loss: 4174.5835\n",
      "Epoch: 059/1000 | Batch 0008/0010 | Loss: 4104.9707\n",
      "Epoch: 059/1000 | Batch 0009/0010 | Loss: 3971.8879\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 060/1000 | Batch 0000/0010 | Loss: 3843.6631\n",
      "Epoch: 060/1000 | Batch 0001/0010 | Loss: 3888.1748\n",
      "Epoch: 060/1000 | Batch 0002/0010 | Loss: 3669.9041\n",
      "Epoch: 060/1000 | Batch 0003/0010 | Loss: 3847.0542\n",
      "Epoch: 060/1000 | Batch 0004/0010 | Loss: 3780.0117\n",
      "Epoch: 060/1000 | Batch 0005/0010 | Loss: 3624.2051\n",
      "Epoch: 060/1000 | Batch 0006/0010 | Loss: 4093.1804\n",
      "Epoch: 060/1000 | Batch 0007/0010 | Loss: 4107.3477\n",
      "Epoch: 060/1000 | Batch 0008/0010 | Loss: 4072.0701\n",
      "Epoch: 060/1000 | Batch 0009/0010 | Loss: 3785.0264\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 061/1000 | Batch 0000/0010 | Loss: 4100.5811\n",
      "Epoch: 061/1000 | Batch 0001/0010 | Loss: 3872.5247\n",
      "Epoch: 061/1000 | Batch 0002/0010 | Loss: 3934.5942\n",
      "Epoch: 061/1000 | Batch 0003/0010 | Loss: 3957.1724\n",
      "Epoch: 061/1000 | Batch 0004/0010 | Loss: 3694.4482\n",
      "Epoch: 061/1000 | Batch 0005/0010 | Loss: 3929.9023\n",
      "Epoch: 061/1000 | Batch 0006/0010 | Loss: 4003.1699\n",
      "Epoch: 061/1000 | Batch 0007/0010 | Loss: 3547.3486\n",
      "Epoch: 061/1000 | Batch 0008/0010 | Loss: 3956.8152\n",
      "Epoch: 061/1000 | Batch 0009/0010 | Loss: 3826.9968\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 062/1000 | Batch 0000/0010 | Loss: 4000.6914\n",
      "Epoch: 062/1000 | Batch 0001/0010 | Loss: 3880.8323\n",
      "Epoch: 062/1000 | Batch 0002/0010 | Loss: 3587.6526\n",
      "Epoch: 062/1000 | Batch 0003/0010 | Loss: 4106.6206\n",
      "Epoch: 062/1000 | Batch 0004/0010 | Loss: 3982.7839\n",
      "Epoch: 062/1000 | Batch 0005/0010 | Loss: 3730.4612\n",
      "Epoch: 062/1000 | Batch 0006/0010 | Loss: 3467.0408\n",
      "Epoch: 062/1000 | Batch 0007/0010 | Loss: 3960.4946\n",
      "Epoch: 062/1000 | Batch 0008/0010 | Loss: 3986.4082\n",
      "Epoch: 062/1000 | Batch 0009/0010 | Loss: 4065.8157\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 063/1000 | Batch 0000/0010 | Loss: 3919.3557\n",
      "Epoch: 063/1000 | Batch 0001/0010 | Loss: 4121.2788\n",
      "Epoch: 063/1000 | Batch 0002/0010 | Loss: 3755.0117\n",
      "Epoch: 063/1000 | Batch 0003/0010 | Loss: 3926.0281\n",
      "Epoch: 063/1000 | Batch 0004/0010 | Loss: 3996.8823\n",
      "Epoch: 063/1000 | Batch 0005/0010 | Loss: 3893.3943\n",
      "Epoch: 063/1000 | Batch 0006/0010 | Loss: 3526.3826\n",
      "Epoch: 063/1000 | Batch 0007/0010 | Loss: 4105.0273\n",
      "Epoch: 063/1000 | Batch 0008/0010 | Loss: 4205.2871\n",
      "Epoch: 063/1000 | Batch 0009/0010 | Loss: 3684.8821\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 064/1000 | Batch 0000/0010 | Loss: 3764.3723\n",
      "Epoch: 064/1000 | Batch 0001/0010 | Loss: 4060.7507\n",
      "Epoch: 064/1000 | Batch 0002/0010 | Loss: 3831.0417\n",
      "Epoch: 064/1000 | Batch 0003/0010 | Loss: 3612.8142\n",
      "Epoch: 064/1000 | Batch 0004/0010 | Loss: 4032.6970\n",
      "Epoch: 064/1000 | Batch 0005/0010 | Loss: 4205.3794\n",
      "Epoch: 064/1000 | Batch 0006/0010 | Loss: 3813.4033\n",
      "Epoch: 064/1000 | Batch 0007/0010 | Loss: 3846.4277\n",
      "Epoch: 064/1000 | Batch 0008/0010 | Loss: 3916.5640\n",
      "Epoch: 064/1000 | Batch 0009/0010 | Loss: 3800.6936\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 065/1000 | Batch 0000/0010 | Loss: 3817.1016\n",
      "Epoch: 065/1000 | Batch 0001/0010 | Loss: 3977.5203\n",
      "Epoch: 065/1000 | Batch 0002/0010 | Loss: 3713.4031\n",
      "Epoch: 065/1000 | Batch 0003/0010 | Loss: 3731.2039\n",
      "Epoch: 065/1000 | Batch 0004/0010 | Loss: 3957.9883\n",
      "Epoch: 065/1000 | Batch 0005/0010 | Loss: 3652.6562\n",
      "Epoch: 065/1000 | Batch 0006/0010 | Loss: 3933.5745\n",
      "Epoch: 065/1000 | Batch 0007/0010 | Loss: 3828.1086\n",
      "Epoch: 065/1000 | Batch 0008/0010 | Loss: 3785.2114\n",
      "Epoch: 065/1000 | Batch 0009/0010 | Loss: 4223.3037\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 066/1000 | Batch 0000/0010 | Loss: 4162.4287\n",
      "Epoch: 066/1000 | Batch 0001/0010 | Loss: 3909.0024\n",
      "Epoch: 066/1000 | Batch 0002/0010 | Loss: 3973.9246\n",
      "Epoch: 066/1000 | Batch 0003/0010 | Loss: 3866.7793\n",
      "Epoch: 066/1000 | Batch 0004/0010 | Loss: 3759.5457\n",
      "Epoch: 066/1000 | Batch 0005/0010 | Loss: 3862.0420\n",
      "Epoch: 066/1000 | Batch 0006/0010 | Loss: 3855.2881\n",
      "Epoch: 066/1000 | Batch 0007/0010 | Loss: 3647.4912\n",
      "Epoch: 066/1000 | Batch 0008/0010 | Loss: 3851.8574\n",
      "Epoch: 066/1000 | Batch 0009/0010 | Loss: 3730.6953\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 067/1000 | Batch 0000/0010 | Loss: 3889.8860\n",
      "Epoch: 067/1000 | Batch 0001/0010 | Loss: 3875.8987\n",
      "Epoch: 067/1000 | Batch 0002/0010 | Loss: 4038.9436\n",
      "Epoch: 067/1000 | Batch 0003/0010 | Loss: 3807.0198\n",
      "Epoch: 067/1000 | Batch 0004/0010 | Loss: 3591.0615\n",
      "Epoch: 067/1000 | Batch 0005/0010 | Loss: 3994.3799\n",
      "Epoch: 067/1000 | Batch 0006/0010 | Loss: 3965.2144\n",
      "Epoch: 067/1000 | Batch 0007/0010 | Loss: 3747.5281\n",
      "Epoch: 067/1000 | Batch 0008/0010 | Loss: 4010.0444\n",
      "Epoch: 067/1000 | Batch 0009/0010 | Loss: 3731.0205\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 068/1000 | Batch 0000/0010 | Loss: 3846.3713\n",
      "Epoch: 068/1000 | Batch 0001/0010 | Loss: 3960.7473\n",
      "Epoch: 068/1000 | Batch 0002/0010 | Loss: 3890.9985\n",
      "Epoch: 068/1000 | Batch 0003/0010 | Loss: 3741.0686\n",
      "Epoch: 068/1000 | Batch 0004/0010 | Loss: 3775.9253\n",
      "Epoch: 068/1000 | Batch 0005/0010 | Loss: 3752.6792\n",
      "Epoch: 068/1000 | Batch 0006/0010 | Loss: 3748.9155\n",
      "Epoch: 068/1000 | Batch 0007/0010 | Loss: 3830.4534\n",
      "Epoch: 068/1000 | Batch 0008/0010 | Loss: 4169.0532\n",
      "Epoch: 068/1000 | Batch 0009/0010 | Loss: 4028.7275\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 069/1000 | Batch 0000/0010 | Loss: 4043.6331\n",
      "Epoch: 069/1000 | Batch 0001/0010 | Loss: 4069.1890\n",
      "Epoch: 069/1000 | Batch 0002/0010 | Loss: 3837.9792\n",
      "Epoch: 069/1000 | Batch 0003/0010 | Loss: 3544.6836\n",
      "Epoch: 069/1000 | Batch 0004/0010 | Loss: 3644.9329\n",
      "Epoch: 069/1000 | Batch 0005/0010 | Loss: 3917.0066\n",
      "Epoch: 069/1000 | Batch 0006/0010 | Loss: 3987.8105\n",
      "Epoch: 069/1000 | Batch 0007/0010 | Loss: 4199.3408\n",
      "Epoch: 069/1000 | Batch 0008/0010 | Loss: 3889.6907\n",
      "Epoch: 069/1000 | Batch 0009/0010 | Loss: 3781.7734\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 070/1000 | Batch 0000/0010 | Loss: 3911.1184\n",
      "Epoch: 070/1000 | Batch 0001/0010 | Loss: 4022.6169\n",
      "Epoch: 070/1000 | Batch 0002/0010 | Loss: 4018.7092\n",
      "Epoch: 070/1000 | Batch 0003/0010 | Loss: 3707.4802\n",
      "Epoch: 070/1000 | Batch 0004/0010 | Loss: 3551.6643\n",
      "Epoch: 070/1000 | Batch 0005/0010 | Loss: 3943.0959\n",
      "Epoch: 070/1000 | Batch 0006/0010 | Loss: 3805.9558\n",
      "Epoch: 070/1000 | Batch 0007/0010 | Loss: 3932.0896\n",
      "Epoch: 070/1000 | Batch 0008/0010 | Loss: 4147.9541\n",
      "Epoch: 070/1000 | Batch 0009/0010 | Loss: 3936.0479\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 071/1000 | Batch 0000/0010 | Loss: 3963.7915\n",
      "Epoch: 071/1000 | Batch 0001/0010 | Loss: 3875.8936\n",
      "Epoch: 071/1000 | Batch 0002/0010 | Loss: 3916.5940\n",
      "Epoch: 071/1000 | Batch 0003/0010 | Loss: 3850.0061\n",
      "Epoch: 071/1000 | Batch 0004/0010 | Loss: 3668.6143\n",
      "Epoch: 071/1000 | Batch 0005/0010 | Loss: 3798.3274\n",
      "Epoch: 071/1000 | Batch 0006/0010 | Loss: 3523.3098\n",
      "Epoch: 071/1000 | Batch 0007/0010 | Loss: 3780.1660\n",
      "Epoch: 071/1000 | Batch 0008/0010 | Loss: 4089.1936\n",
      "Epoch: 071/1000 | Batch 0009/0010 | Loss: 4065.5518\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 072/1000 | Batch 0000/0010 | Loss: 4160.9927\n",
      "Epoch: 072/1000 | Batch 0001/0010 | Loss: 3946.7952\n",
      "Epoch: 072/1000 | Batch 0002/0010 | Loss: 3712.3193\n",
      "Epoch: 072/1000 | Batch 0003/0010 | Loss: 3956.5386\n",
      "Epoch: 072/1000 | Batch 0004/0010 | Loss: 3718.2937\n",
      "Epoch: 072/1000 | Batch 0005/0010 | Loss: 3785.2456\n",
      "Epoch: 072/1000 | Batch 0006/0010 | Loss: 3834.0417\n",
      "Epoch: 072/1000 | Batch 0007/0010 | Loss: 3821.7327\n",
      "Epoch: 072/1000 | Batch 0008/0010 | Loss: 3893.7024\n",
      "Epoch: 072/1000 | Batch 0009/0010 | Loss: 3832.2012\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 073/1000 | Batch 0000/0010 | Loss: 4017.7361\n",
      "Epoch: 073/1000 | Batch 0001/0010 | Loss: 3680.4861\n",
      "Epoch: 073/1000 | Batch 0002/0010 | Loss: 3928.4639\n",
      "Epoch: 073/1000 | Batch 0003/0010 | Loss: 3684.1023\n",
      "Epoch: 073/1000 | Batch 0004/0010 | Loss: 3814.7905\n",
      "Epoch: 073/1000 | Batch 0005/0010 | Loss: 3925.2456\n",
      "Epoch: 073/1000 | Batch 0006/0010 | Loss: 3921.3577\n",
      "Epoch: 073/1000 | Batch 0007/0010 | Loss: 3908.1638\n",
      "Epoch: 073/1000 | Batch 0008/0010 | Loss: 3772.3762\n",
      "Epoch: 073/1000 | Batch 0009/0010 | Loss: 3816.7168\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 074/1000 | Batch 0000/0010 | Loss: 3918.2505\n",
      "Epoch: 074/1000 | Batch 0001/0010 | Loss: 3475.9822\n",
      "Epoch: 074/1000 | Batch 0002/0010 | Loss: 3942.4973\n",
      "Epoch: 074/1000 | Batch 0003/0010 | Loss: 3892.6045\n",
      "Epoch: 074/1000 | Batch 0004/0010 | Loss: 3986.9321\n",
      "Epoch: 074/1000 | Batch 0005/0010 | Loss: 4027.0154\n",
      "Epoch: 074/1000 | Batch 0006/0010 | Loss: 3904.3645\n",
      "Epoch: 074/1000 | Batch 0007/0010 | Loss: 3641.6750\n",
      "Epoch: 074/1000 | Batch 0008/0010 | Loss: 4028.1711\n",
      "Epoch: 074/1000 | Batch 0009/0010 | Loss: 3604.0129\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 075/1000 | Batch 0000/0010 | Loss: 3927.5444\n",
      "Epoch: 075/1000 | Batch 0001/0010 | Loss: 3894.2239\n",
      "Epoch: 075/1000 | Batch 0002/0010 | Loss: 3755.2324\n",
      "Epoch: 075/1000 | Batch 0003/0010 | Loss: 3961.3135\n",
      "Epoch: 075/1000 | Batch 0004/0010 | Loss: 3635.2158\n",
      "Epoch: 075/1000 | Batch 0005/0010 | Loss: 3973.5491\n",
      "Epoch: 075/1000 | Batch 0006/0010 | Loss: 3747.9893\n",
      "Epoch: 075/1000 | Batch 0007/0010 | Loss: 3632.6096\n",
      "Epoch: 075/1000 | Batch 0008/0010 | Loss: 3803.8877\n",
      "Epoch: 075/1000 | Batch 0009/0010 | Loss: 4163.9282\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 076/1000 | Batch 0000/0010 | Loss: 3763.4810\n",
      "Epoch: 076/1000 | Batch 0001/0010 | Loss: 3848.3257\n",
      "Epoch: 076/1000 | Batch 0002/0010 | Loss: 3771.2424\n",
      "Epoch: 076/1000 | Batch 0003/0010 | Loss: 3904.9685\n",
      "Epoch: 076/1000 | Batch 0004/0010 | Loss: 3814.9883\n",
      "Epoch: 076/1000 | Batch 0005/0010 | Loss: 3928.2769\n",
      "Epoch: 076/1000 | Batch 0006/0010 | Loss: 3629.3750\n",
      "Epoch: 076/1000 | Batch 0007/0010 | Loss: 3945.1299\n",
      "Epoch: 076/1000 | Batch 0008/0010 | Loss: 3877.1738\n",
      "Epoch: 076/1000 | Batch 0009/0010 | Loss: 4075.3452\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 077/1000 | Batch 0000/0010 | Loss: 3914.2061\n",
      "Epoch: 077/1000 | Batch 0001/0010 | Loss: 3830.0676\n",
      "Epoch: 077/1000 | Batch 0002/0010 | Loss: 3652.7783\n",
      "Epoch: 077/1000 | Batch 0003/0010 | Loss: 3964.8594\n",
      "Epoch: 077/1000 | Batch 0004/0010 | Loss: 4031.6646\n",
      "Epoch: 077/1000 | Batch 0005/0010 | Loss: 3761.4033\n",
      "Epoch: 077/1000 | Batch 0006/0010 | Loss: 3770.8550\n",
      "Epoch: 077/1000 | Batch 0007/0010 | Loss: 4103.7241\n",
      "Epoch: 077/1000 | Batch 0008/0010 | Loss: 3720.3018\n",
      "Epoch: 077/1000 | Batch 0009/0010 | Loss: 3819.0027\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 078/1000 | Batch 0000/0010 | Loss: 3870.2292\n",
      "Epoch: 078/1000 | Batch 0001/0010 | Loss: 3930.7134\n",
      "Epoch: 078/1000 | Batch 0002/0010 | Loss: 3897.3855\n",
      "Epoch: 078/1000 | Batch 0003/0010 | Loss: 3736.3438\n",
      "Epoch: 078/1000 | Batch 0004/0010 | Loss: 3797.4214\n",
      "Epoch: 078/1000 | Batch 0005/0010 | Loss: 3627.5049\n",
      "Epoch: 078/1000 | Batch 0006/0010 | Loss: 3837.9490\n",
      "Epoch: 078/1000 | Batch 0007/0010 | Loss: 4014.0696\n",
      "Epoch: 078/1000 | Batch 0008/0010 | Loss: 4028.5425\n",
      "Epoch: 078/1000 | Batch 0009/0010 | Loss: 3748.2583\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 079/1000 | Batch 0000/0010 | Loss: 3691.7122\n",
      "Epoch: 079/1000 | Batch 0001/0010 | Loss: 3842.7786\n",
      "Epoch: 079/1000 | Batch 0002/0010 | Loss: 4008.1489\n",
      "Epoch: 079/1000 | Batch 0003/0010 | Loss: 3852.3713\n",
      "Epoch: 079/1000 | Batch 0004/0010 | Loss: 4143.7661\n",
      "Epoch: 079/1000 | Batch 0005/0010 | Loss: 3834.6323\n",
      "Epoch: 079/1000 | Batch 0006/0010 | Loss: 3816.3757\n",
      "Epoch: 079/1000 | Batch 0007/0010 | Loss: 3422.5872\n",
      "Epoch: 079/1000 | Batch 0008/0010 | Loss: 3927.1174\n",
      "Epoch: 079/1000 | Batch 0009/0010 | Loss: 3975.7212\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 080/1000 | Batch 0000/0010 | Loss: 3832.0349\n",
      "Epoch: 080/1000 | Batch 0001/0010 | Loss: 3947.6265\n",
      "Epoch: 080/1000 | Batch 0002/0010 | Loss: 3664.0549\n",
      "Epoch: 080/1000 | Batch 0003/0010 | Loss: 3879.1934\n",
      "Epoch: 080/1000 | Batch 0004/0010 | Loss: 3941.6868\n",
      "Epoch: 080/1000 | Batch 0005/0010 | Loss: 3990.8271\n",
      "Epoch: 080/1000 | Batch 0006/0010 | Loss: 4131.1807\n",
      "Epoch: 080/1000 | Batch 0007/0010 | Loss: 3839.4053\n",
      "Epoch: 080/1000 | Batch 0008/0010 | Loss: 3741.2483\n",
      "Epoch: 080/1000 | Batch 0009/0010 | Loss: 3595.7310\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 081/1000 | Batch 0000/0010 | Loss: 3869.0232\n",
      "Epoch: 081/1000 | Batch 0001/0010 | Loss: 4117.6235\n",
      "Epoch: 081/1000 | Batch 0002/0010 | Loss: 3941.1162\n",
      "Epoch: 081/1000 | Batch 0003/0010 | Loss: 3662.8105\n",
      "Epoch: 081/1000 | Batch 0004/0010 | Loss: 3988.2625\n",
      "Epoch: 081/1000 | Batch 0005/0010 | Loss: 3830.1887\n",
      "Epoch: 081/1000 | Batch 0006/0010 | Loss: 3864.5630\n",
      "Epoch: 081/1000 | Batch 0007/0010 | Loss: 3755.9739\n",
      "Epoch: 081/1000 | Batch 0008/0010 | Loss: 3735.4150\n",
      "Epoch: 081/1000 | Batch 0009/0010 | Loss: 3913.8635\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 082/1000 | Batch 0000/0010 | Loss: 3806.9702\n",
      "Epoch: 082/1000 | Batch 0001/0010 | Loss: 3752.1809\n",
      "Epoch: 082/1000 | Batch 0002/0010 | Loss: 3826.0022\n",
      "Epoch: 082/1000 | Batch 0003/0010 | Loss: 4041.6604\n",
      "Epoch: 082/1000 | Batch 0004/0010 | Loss: 3892.7327\n",
      "Epoch: 082/1000 | Batch 0005/0010 | Loss: 3852.4214\n",
      "Epoch: 082/1000 | Batch 0006/0010 | Loss: 3775.9290\n",
      "Epoch: 082/1000 | Batch 0007/0010 | Loss: 3832.2566\n",
      "Epoch: 082/1000 | Batch 0008/0010 | Loss: 3750.7104\n",
      "Epoch: 082/1000 | Batch 0009/0010 | Loss: 4293.8130\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 083/1000 | Batch 0000/0010 | Loss: 3810.3459\n",
      "Epoch: 083/1000 | Batch 0001/0010 | Loss: 3862.9399\n",
      "Epoch: 083/1000 | Batch 0002/0010 | Loss: 3960.3645\n",
      "Epoch: 083/1000 | Batch 0003/0010 | Loss: 3708.2798\n",
      "Epoch: 083/1000 | Batch 0004/0010 | Loss: 3729.1685\n",
      "Epoch: 083/1000 | Batch 0005/0010 | Loss: 3963.7922\n",
      "Epoch: 083/1000 | Batch 0006/0010 | Loss: 3718.6416\n",
      "Epoch: 083/1000 | Batch 0007/0010 | Loss: 3942.0188\n",
      "Epoch: 083/1000 | Batch 0008/0010 | Loss: 4060.6711\n",
      "Epoch: 083/1000 | Batch 0009/0010 | Loss: 3774.3518\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 084/1000 | Batch 0000/0010 | Loss: 3928.4585\n",
      "Epoch: 084/1000 | Batch 0001/0010 | Loss: 3975.6406\n",
      "Epoch: 084/1000 | Batch 0002/0010 | Loss: 4004.9802\n",
      "Epoch: 084/1000 | Batch 0003/0010 | Loss: 3797.3765\n",
      "Epoch: 084/1000 | Batch 0004/0010 | Loss: 3697.3110\n",
      "Epoch: 084/1000 | Batch 0005/0010 | Loss: 3717.5049\n",
      "Epoch: 084/1000 | Batch 0006/0010 | Loss: 3707.3569\n",
      "Epoch: 084/1000 | Batch 0007/0010 | Loss: 4065.5308\n",
      "Epoch: 084/1000 | Batch 0008/0010 | Loss: 3805.8594\n",
      "Epoch: 084/1000 | Batch 0009/0010 | Loss: 3793.0022\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 085/1000 | Batch 0000/0010 | Loss: 3871.3613\n",
      "Epoch: 085/1000 | Batch 0001/0010 | Loss: 4012.6399\n",
      "Epoch: 085/1000 | Batch 0002/0010 | Loss: 3850.6243\n",
      "Epoch: 085/1000 | Batch 0003/0010 | Loss: 4006.0481\n",
      "Epoch: 085/1000 | Batch 0004/0010 | Loss: 3757.9111\n",
      "Epoch: 085/1000 | Batch 0005/0010 | Loss: 3829.4651\n",
      "Epoch: 085/1000 | Batch 0006/0010 | Loss: 3826.6624\n",
      "Epoch: 085/1000 | Batch 0007/0010 | Loss: 3857.5039\n",
      "Epoch: 085/1000 | Batch 0008/0010 | Loss: 3773.3999\n",
      "Epoch: 085/1000 | Batch 0009/0010 | Loss: 3614.7544\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 086/1000 | Batch 0000/0010 | Loss: 4008.1523\n",
      "Epoch: 086/1000 | Batch 0001/0010 | Loss: 3956.5093\n",
      "Epoch: 086/1000 | Batch 0002/0010 | Loss: 3963.8303\n",
      "Epoch: 086/1000 | Batch 0003/0010 | Loss: 3777.9805\n",
      "Epoch: 086/1000 | Batch 0004/0010 | Loss: 3744.8762\n",
      "Epoch: 086/1000 | Batch 0005/0010 | Loss: 3768.3916\n",
      "Epoch: 086/1000 | Batch 0006/0010 | Loss: 3762.2214\n",
      "Epoch: 086/1000 | Batch 0007/0010 | Loss: 3758.7886\n",
      "Epoch: 086/1000 | Batch 0008/0010 | Loss: 3992.1458\n",
      "Epoch: 086/1000 | Batch 0009/0010 | Loss: 3729.6250\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 087/1000 | Batch 0000/0010 | Loss: 3901.5210\n",
      "Epoch: 087/1000 | Batch 0001/0010 | Loss: 3629.0483\n",
      "Epoch: 087/1000 | Batch 0002/0010 | Loss: 3759.6711\n",
      "Epoch: 087/1000 | Batch 0003/0010 | Loss: 4179.1074\n",
      "Epoch: 087/1000 | Batch 0004/0010 | Loss: 3623.2900\n",
      "Epoch: 087/1000 | Batch 0005/0010 | Loss: 3795.8411\n",
      "Epoch: 087/1000 | Batch 0006/0010 | Loss: 3993.5747\n",
      "Epoch: 087/1000 | Batch 0007/0010 | Loss: 3720.3489\n",
      "Epoch: 087/1000 | Batch 0008/0010 | Loss: 3930.0159\n",
      "Epoch: 087/1000 | Batch 0009/0010 | Loss: 3769.7844\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 088/1000 | Batch 0000/0010 | Loss: 3582.4456\n",
      "Epoch: 088/1000 | Batch 0001/0010 | Loss: 3747.8721\n",
      "Epoch: 088/1000 | Batch 0002/0010 | Loss: 3569.8362\n",
      "Epoch: 088/1000 | Batch 0003/0010 | Loss: 3882.9753\n",
      "Epoch: 088/1000 | Batch 0004/0010 | Loss: 4060.5896\n",
      "Epoch: 088/1000 | Batch 0005/0010 | Loss: 3876.8269\n",
      "Epoch: 088/1000 | Batch 0006/0010 | Loss: 3669.3801\n",
      "Epoch: 088/1000 | Batch 0007/0010 | Loss: 4220.3799\n",
      "Epoch: 088/1000 | Batch 0008/0010 | Loss: 3934.7371\n",
      "Epoch: 088/1000 | Batch 0009/0010 | Loss: 3893.7732\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 089/1000 | Batch 0000/0010 | Loss: 3882.5295\n",
      "Epoch: 089/1000 | Batch 0001/0010 | Loss: 4086.2334\n",
      "Epoch: 089/1000 | Batch 0002/0010 | Loss: 3704.8450\n",
      "Epoch: 089/1000 | Batch 0003/0010 | Loss: 3986.4692\n",
      "Epoch: 089/1000 | Batch 0004/0010 | Loss: 3959.7302\n",
      "Epoch: 089/1000 | Batch 0005/0010 | Loss: 3662.9116\n",
      "Epoch: 089/1000 | Batch 0006/0010 | Loss: 3720.3044\n",
      "Epoch: 089/1000 | Batch 0007/0010 | Loss: 4041.7861\n",
      "Epoch: 089/1000 | Batch 0008/0010 | Loss: 3778.1548\n",
      "Epoch: 089/1000 | Batch 0009/0010 | Loss: 3833.1687\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 090/1000 | Batch 0000/0010 | Loss: 3727.4089\n",
      "Epoch: 090/1000 | Batch 0001/0010 | Loss: 3629.1895\n",
      "Epoch: 090/1000 | Batch 0002/0010 | Loss: 3749.1074\n",
      "Epoch: 090/1000 | Batch 0003/0010 | Loss: 4223.2461\n",
      "Epoch: 090/1000 | Batch 0004/0010 | Loss: 3962.2734\n",
      "Epoch: 090/1000 | Batch 0005/0010 | Loss: 3956.2480\n",
      "Epoch: 090/1000 | Batch 0006/0010 | Loss: 3909.6997\n",
      "Epoch: 090/1000 | Batch 0007/0010 | Loss: 3773.4590\n",
      "Epoch: 090/1000 | Batch 0008/0010 | Loss: 3844.0903\n",
      "Epoch: 090/1000 | Batch 0009/0010 | Loss: 3757.7227\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 091/1000 | Batch 0000/0010 | Loss: 3689.4016\n",
      "Epoch: 091/1000 | Batch 0001/0010 | Loss: 4031.3811\n",
      "Epoch: 091/1000 | Batch 0002/0010 | Loss: 3810.4858\n",
      "Epoch: 091/1000 | Batch 0003/0010 | Loss: 3781.7854\n",
      "Epoch: 091/1000 | Batch 0004/0010 | Loss: 3690.7974\n",
      "Epoch: 091/1000 | Batch 0005/0010 | Loss: 4017.6760\n",
      "Epoch: 091/1000 | Batch 0006/0010 | Loss: 3731.4360\n",
      "Epoch: 091/1000 | Batch 0007/0010 | Loss: 3984.2290\n",
      "Epoch: 091/1000 | Batch 0008/0010 | Loss: 3812.6472\n",
      "Epoch: 091/1000 | Batch 0009/0010 | Loss: 3863.0369\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 092/1000 | Batch 0000/0010 | Loss: 3802.6274\n",
      "Epoch: 092/1000 | Batch 0001/0010 | Loss: 3764.3569\n",
      "Epoch: 092/1000 | Batch 0002/0010 | Loss: 3706.0767\n",
      "Epoch: 092/1000 | Batch 0003/0010 | Loss: 3890.5649\n",
      "Epoch: 092/1000 | Batch 0004/0010 | Loss: 3788.1389\n",
      "Epoch: 092/1000 | Batch 0005/0010 | Loss: 3735.3091\n",
      "Epoch: 092/1000 | Batch 0006/0010 | Loss: 4092.9292\n",
      "Epoch: 092/1000 | Batch 0007/0010 | Loss: 3735.8794\n",
      "Epoch: 092/1000 | Batch 0008/0010 | Loss: 3759.0020\n",
      "Epoch: 092/1000 | Batch 0009/0010 | Loss: 4059.4153\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 093/1000 | Batch 0000/0010 | Loss: 3617.6008\n",
      "Epoch: 093/1000 | Batch 0001/0010 | Loss: 3849.0715\n",
      "Epoch: 093/1000 | Batch 0002/0010 | Loss: 3775.1543\n",
      "Epoch: 093/1000 | Batch 0003/0010 | Loss: 3885.0574\n",
      "Epoch: 093/1000 | Batch 0004/0010 | Loss: 3667.5479\n",
      "Epoch: 093/1000 | Batch 0005/0010 | Loss: 3762.9438\n",
      "Epoch: 093/1000 | Batch 0006/0010 | Loss: 3857.1733\n",
      "Epoch: 093/1000 | Batch 0007/0010 | Loss: 3988.4783\n",
      "Epoch: 093/1000 | Batch 0008/0010 | Loss: 3963.2988\n",
      "Epoch: 093/1000 | Batch 0009/0010 | Loss: 3861.3047\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 094/1000 | Batch 0000/0010 | Loss: 3835.3042\n",
      "Epoch: 094/1000 | Batch 0001/0010 | Loss: 3736.3330\n",
      "Epoch: 094/1000 | Batch 0002/0010 | Loss: 3843.6196\n",
      "Epoch: 094/1000 | Batch 0003/0010 | Loss: 3790.5156\n",
      "Epoch: 094/1000 | Batch 0004/0010 | Loss: 3939.6621\n",
      "Epoch: 094/1000 | Batch 0005/0010 | Loss: 3949.3652\n",
      "Epoch: 094/1000 | Batch 0006/0010 | Loss: 3855.5583\n",
      "Epoch: 094/1000 | Batch 0007/0010 | Loss: 3579.2124\n",
      "Epoch: 094/1000 | Batch 0008/0010 | Loss: 3864.3804\n",
      "Epoch: 094/1000 | Batch 0009/0010 | Loss: 4018.4092\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 095/1000 | Batch 0000/0010 | Loss: 3841.6929\n",
      "Epoch: 095/1000 | Batch 0001/0010 | Loss: 3618.0146\n",
      "Epoch: 095/1000 | Batch 0002/0010 | Loss: 3981.1462\n",
      "Epoch: 095/1000 | Batch 0003/0010 | Loss: 3743.3064\n",
      "Epoch: 095/1000 | Batch 0004/0010 | Loss: 3939.6943\n",
      "Epoch: 095/1000 | Batch 0005/0010 | Loss: 3637.4641\n",
      "Epoch: 095/1000 | Batch 0006/0010 | Loss: 3846.8948\n",
      "Epoch: 095/1000 | Batch 0007/0010 | Loss: 4221.0449\n",
      "Epoch: 095/1000 | Batch 0008/0010 | Loss: 3762.6511\n",
      "Epoch: 095/1000 | Batch 0009/0010 | Loss: 3765.2009\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 096/1000 | Batch 0000/0010 | Loss: 3721.9883\n",
      "Epoch: 096/1000 | Batch 0001/0010 | Loss: 3788.6470\n",
      "Epoch: 096/1000 | Batch 0002/0010 | Loss: 3907.4487\n",
      "Epoch: 096/1000 | Batch 0003/0010 | Loss: 3692.3748\n",
      "Epoch: 096/1000 | Batch 0004/0010 | Loss: 3677.1851\n",
      "Epoch: 096/1000 | Batch 0005/0010 | Loss: 3806.1758\n",
      "Epoch: 096/1000 | Batch 0006/0010 | Loss: 3971.8032\n",
      "Epoch: 096/1000 | Batch 0007/0010 | Loss: 4093.3240\n",
      "Epoch: 096/1000 | Batch 0008/0010 | Loss: 3561.0513\n",
      "Epoch: 096/1000 | Batch 0009/0010 | Loss: 4117.6289\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 097/1000 | Batch 0000/0010 | Loss: 3637.4783\n",
      "Epoch: 097/1000 | Batch 0001/0010 | Loss: 3763.8516\n",
      "Epoch: 097/1000 | Batch 0002/0010 | Loss: 3707.9023\n",
      "Epoch: 097/1000 | Batch 0003/0010 | Loss: 3748.0730\n",
      "Epoch: 097/1000 | Batch 0004/0010 | Loss: 3790.4863\n",
      "Epoch: 097/1000 | Batch 0005/0010 | Loss: 3968.0759\n",
      "Epoch: 097/1000 | Batch 0006/0010 | Loss: 4180.1602\n",
      "Epoch: 097/1000 | Batch 0007/0010 | Loss: 3669.1711\n",
      "Epoch: 097/1000 | Batch 0008/0010 | Loss: 3997.1436\n",
      "Epoch: 097/1000 | Batch 0009/0010 | Loss: 3919.5642\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 098/1000 | Batch 0000/0010 | Loss: 3926.9172\n",
      "Epoch: 098/1000 | Batch 0001/0010 | Loss: 3784.5369\n",
      "Epoch: 098/1000 | Batch 0002/0010 | Loss: 3985.1941\n",
      "Epoch: 098/1000 | Batch 0003/0010 | Loss: 3946.3054\n",
      "Epoch: 098/1000 | Batch 0004/0010 | Loss: 3575.0427\n",
      "Epoch: 098/1000 | Batch 0005/0010 | Loss: 3956.0635\n",
      "Epoch: 098/1000 | Batch 0006/0010 | Loss: 3997.6614\n",
      "Epoch: 098/1000 | Batch 0007/0010 | Loss: 3818.8376\n",
      "Epoch: 098/1000 | Batch 0008/0010 | Loss: 3827.7798\n",
      "Epoch: 098/1000 | Batch 0009/0010 | Loss: 3659.2744\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 099/1000 | Batch 0000/0010 | Loss: 3833.8740\n",
      "Epoch: 099/1000 | Batch 0001/0010 | Loss: 3656.1973\n",
      "Epoch: 099/1000 | Batch 0002/0010 | Loss: 3867.1328\n",
      "Epoch: 099/1000 | Batch 0003/0010 | Loss: 3828.8406\n",
      "Epoch: 099/1000 | Batch 0004/0010 | Loss: 3834.8098\n",
      "Epoch: 099/1000 | Batch 0005/0010 | Loss: 4022.2805\n",
      "Epoch: 099/1000 | Batch 0006/0010 | Loss: 3637.8779\n",
      "Epoch: 099/1000 | Batch 0007/0010 | Loss: 3582.1350\n",
      "Epoch: 099/1000 | Batch 0008/0010 | Loss: 3958.0112\n",
      "Epoch: 099/1000 | Batch 0009/0010 | Loss: 4117.0449\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 100/1000 | Batch 0000/0010 | Loss: 3876.1736\n",
      "Epoch: 100/1000 | Batch 0001/0010 | Loss: 3912.8262\n",
      "Epoch: 100/1000 | Batch 0002/0010 | Loss: 3708.0374\n",
      "Epoch: 100/1000 | Batch 0003/0010 | Loss: 3980.3906\n",
      "Epoch: 100/1000 | Batch 0004/0010 | Loss: 3894.7148\n",
      "Epoch: 100/1000 | Batch 0005/0010 | Loss: 3936.4045\n",
      "Epoch: 100/1000 | Batch 0006/0010 | Loss: 3518.7849\n",
      "Epoch: 100/1000 | Batch 0007/0010 | Loss: 3902.5427\n",
      "Epoch: 100/1000 | Batch 0008/0010 | Loss: 3946.6414\n",
      "Epoch: 100/1000 | Batch 0009/0010 | Loss: 3740.6609\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 101/1000 | Batch 0000/0010 | Loss: 3861.6104\n",
      "Epoch: 101/1000 | Batch 0001/0010 | Loss: 3929.7068\n",
      "Epoch: 101/1000 | Batch 0002/0010 | Loss: 3840.6970\n",
      "Epoch: 101/1000 | Batch 0003/0010 | Loss: 3976.8860\n",
      "Epoch: 101/1000 | Batch 0004/0010 | Loss: 3852.1636\n",
      "Epoch: 101/1000 | Batch 0005/0010 | Loss: 3786.3792\n",
      "Epoch: 101/1000 | Batch 0006/0010 | Loss: 3625.9558\n",
      "Epoch: 101/1000 | Batch 0007/0010 | Loss: 3890.8506\n",
      "Epoch: 101/1000 | Batch 0008/0010 | Loss: 3709.3123\n",
      "Epoch: 101/1000 | Batch 0009/0010 | Loss: 3915.4883\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 102/1000 | Batch 0000/0010 | Loss: 3635.1838\n",
      "Epoch: 102/1000 | Batch 0001/0010 | Loss: 3921.5708\n",
      "Epoch: 102/1000 | Batch 0002/0010 | Loss: 4150.0972\n",
      "Epoch: 102/1000 | Batch 0003/0010 | Loss: 3858.2437\n",
      "Epoch: 102/1000 | Batch 0004/0010 | Loss: 3832.6128\n",
      "Epoch: 102/1000 | Batch 0005/0010 | Loss: 3648.6094\n",
      "Epoch: 102/1000 | Batch 0006/0010 | Loss: 3905.3884\n",
      "Epoch: 102/1000 | Batch 0007/0010 | Loss: 3729.0574\n",
      "Epoch: 102/1000 | Batch 0008/0010 | Loss: 3680.4417\n",
      "Epoch: 102/1000 | Batch 0009/0010 | Loss: 4056.3303\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 103/1000 | Batch 0000/0010 | Loss: 3916.1108\n",
      "Epoch: 103/1000 | Batch 0001/0010 | Loss: 3639.3757\n",
      "Epoch: 103/1000 | Batch 0002/0010 | Loss: 3767.4780\n",
      "Epoch: 103/1000 | Batch 0003/0010 | Loss: 3898.8662\n",
      "Epoch: 103/1000 | Batch 0004/0010 | Loss: 3749.8875\n",
      "Epoch: 103/1000 | Batch 0005/0010 | Loss: 3754.4773\n",
      "Epoch: 103/1000 | Batch 0006/0010 | Loss: 4115.4346\n",
      "Epoch: 103/1000 | Batch 0007/0010 | Loss: 3891.2231\n",
      "Epoch: 103/1000 | Batch 0008/0010 | Loss: 3787.1384\n",
      "Epoch: 103/1000 | Batch 0009/0010 | Loss: 3770.7847\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 104/1000 | Batch 0000/0010 | Loss: 3757.5178\n",
      "Epoch: 104/1000 | Batch 0001/0010 | Loss: 3568.1731\n",
      "Epoch: 104/1000 | Batch 0002/0010 | Loss: 3563.9023\n",
      "Epoch: 104/1000 | Batch 0003/0010 | Loss: 3548.3503\n",
      "Epoch: 104/1000 | Batch 0004/0010 | Loss: 3995.1082\n",
      "Epoch: 104/1000 | Batch 0005/0010 | Loss: 3936.6287\n",
      "Epoch: 104/1000 | Batch 0006/0010 | Loss: 4014.8667\n",
      "Epoch: 104/1000 | Batch 0007/0010 | Loss: 3889.3403\n",
      "Epoch: 104/1000 | Batch 0008/0010 | Loss: 3804.5869\n",
      "Epoch: 104/1000 | Batch 0009/0010 | Loss: 4115.0503\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 105/1000 | Batch 0000/0010 | Loss: 3803.4297\n",
      "Epoch: 105/1000 | Batch 0001/0010 | Loss: 3837.8132\n",
      "Epoch: 105/1000 | Batch 0002/0010 | Loss: 3887.3318\n",
      "Epoch: 105/1000 | Batch 0003/0010 | Loss: 3834.6018\n",
      "Epoch: 105/1000 | Batch 0004/0010 | Loss: 3754.5015\n",
      "Epoch: 105/1000 | Batch 0005/0010 | Loss: 3857.9041\n",
      "Epoch: 105/1000 | Batch 0006/0010 | Loss: 3722.9761\n",
      "Epoch: 105/1000 | Batch 0007/0010 | Loss: 3842.4004\n",
      "Epoch: 105/1000 | Batch 0008/0010 | Loss: 3721.2036\n",
      "Epoch: 105/1000 | Batch 0009/0010 | Loss: 3971.3247\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 106/1000 | Batch 0000/0010 | Loss: 3876.0962\n",
      "Epoch: 106/1000 | Batch 0001/0010 | Loss: 3812.0039\n",
      "Epoch: 106/1000 | Batch 0002/0010 | Loss: 3857.2297\n",
      "Epoch: 106/1000 | Batch 0003/0010 | Loss: 4016.3672\n",
      "Epoch: 106/1000 | Batch 0004/0010 | Loss: 4076.0479\n",
      "Epoch: 106/1000 | Batch 0005/0010 | Loss: 3560.3899\n",
      "Epoch: 106/1000 | Batch 0006/0010 | Loss: 3666.4663\n",
      "Epoch: 106/1000 | Batch 0007/0010 | Loss: 3726.6531\n",
      "Epoch: 106/1000 | Batch 0008/0010 | Loss: 3628.8291\n",
      "Epoch: 106/1000 | Batch 0009/0010 | Loss: 3987.3750\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 107/1000 | Batch 0000/0010 | Loss: 3666.7866\n",
      "Epoch: 107/1000 | Batch 0001/0010 | Loss: 3593.3325\n",
      "Epoch: 107/1000 | Batch 0002/0010 | Loss: 4036.0098\n",
      "Epoch: 107/1000 | Batch 0003/0010 | Loss: 3950.3350\n",
      "Epoch: 107/1000 | Batch 0004/0010 | Loss: 3805.3733\n",
      "Epoch: 107/1000 | Batch 0005/0010 | Loss: 3744.2710\n",
      "Epoch: 107/1000 | Batch 0006/0010 | Loss: 3588.9019\n",
      "Epoch: 107/1000 | Batch 0007/0010 | Loss: 3679.0593\n",
      "Epoch: 107/1000 | Batch 0008/0010 | Loss: 4237.0796\n",
      "Epoch: 107/1000 | Batch 0009/0010 | Loss: 3947.0532\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 108/1000 | Batch 0000/0010 | Loss: 3939.0239\n",
      "Epoch: 108/1000 | Batch 0001/0010 | Loss: 3704.8672\n",
      "Epoch: 108/1000 | Batch 0002/0010 | Loss: 3714.1829\n",
      "Epoch: 108/1000 | Batch 0003/0010 | Loss: 3613.5134\n",
      "Epoch: 108/1000 | Batch 0004/0010 | Loss: 3984.1021\n",
      "Epoch: 108/1000 | Batch 0005/0010 | Loss: 3869.1663\n",
      "Epoch: 108/1000 | Batch 0006/0010 | Loss: 3884.5488\n",
      "Epoch: 108/1000 | Batch 0007/0010 | Loss: 4192.2354\n",
      "Epoch: 108/1000 | Batch 0008/0010 | Loss: 3770.2976\n",
      "Epoch: 108/1000 | Batch 0009/0010 | Loss: 3552.1438\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 109/1000 | Batch 0000/0010 | Loss: 4085.7607\n",
      "Epoch: 109/1000 | Batch 0001/0010 | Loss: 3718.6958\n",
      "Epoch: 109/1000 | Batch 0002/0010 | Loss: 3857.0332\n",
      "Epoch: 109/1000 | Batch 0003/0010 | Loss: 3554.9019\n",
      "Epoch: 109/1000 | Batch 0004/0010 | Loss: 3844.3943\n",
      "Epoch: 109/1000 | Batch 0005/0010 | Loss: 3961.3933\n",
      "Epoch: 109/1000 | Batch 0006/0010 | Loss: 3757.4800\n",
      "Epoch: 109/1000 | Batch 0007/0010 | Loss: 3952.0574\n",
      "Epoch: 109/1000 | Batch 0008/0010 | Loss: 3780.5686\n",
      "Epoch: 109/1000 | Batch 0009/0010 | Loss: 3787.7300\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 110/1000 | Batch 0000/0010 | Loss: 3845.3926\n",
      "Epoch: 110/1000 | Batch 0001/0010 | Loss: 3839.0801\n",
      "Epoch: 110/1000 | Batch 0002/0010 | Loss: 3881.9473\n",
      "Epoch: 110/1000 | Batch 0003/0010 | Loss: 3947.6863\n",
      "Epoch: 110/1000 | Batch 0004/0010 | Loss: 3855.8943\n",
      "Epoch: 110/1000 | Batch 0005/0010 | Loss: 3656.1475\n",
      "Epoch: 110/1000 | Batch 0006/0010 | Loss: 3795.0859\n",
      "Epoch: 110/1000 | Batch 0007/0010 | Loss: 3915.9553\n",
      "Epoch: 110/1000 | Batch 0008/0010 | Loss: 3536.0664\n",
      "Epoch: 110/1000 | Batch 0009/0010 | Loss: 3963.0952\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 111/1000 | Batch 0000/0010 | Loss: 3782.2458\n",
      "Epoch: 111/1000 | Batch 0001/0010 | Loss: 3779.8501\n",
      "Epoch: 111/1000 | Batch 0002/0010 | Loss: 4006.2510\n",
      "Epoch: 111/1000 | Batch 0003/0010 | Loss: 3616.3547\n",
      "Epoch: 111/1000 | Batch 0004/0010 | Loss: 3869.0190\n",
      "Epoch: 111/1000 | Batch 0005/0010 | Loss: 3737.0166\n",
      "Epoch: 111/1000 | Batch 0006/0010 | Loss: 3679.9929\n",
      "Epoch: 111/1000 | Batch 0007/0010 | Loss: 3947.4319\n",
      "Epoch: 111/1000 | Batch 0008/0010 | Loss: 4036.3523\n",
      "Epoch: 111/1000 | Batch 0009/0010 | Loss: 3654.9302\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 112/1000 | Batch 0000/0010 | Loss: 3800.3162\n",
      "Epoch: 112/1000 | Batch 0001/0010 | Loss: 3729.4688\n",
      "Epoch: 112/1000 | Batch 0002/0010 | Loss: 3769.1367\n",
      "Epoch: 112/1000 | Batch 0003/0010 | Loss: 3851.4758\n",
      "Epoch: 112/1000 | Batch 0004/0010 | Loss: 3922.8940\n",
      "Epoch: 112/1000 | Batch 0005/0010 | Loss: 3912.6565\n",
      "Epoch: 112/1000 | Batch 0006/0010 | Loss: 3937.1152\n",
      "Epoch: 112/1000 | Batch 0007/0010 | Loss: 3740.4810\n",
      "Epoch: 112/1000 | Batch 0008/0010 | Loss: 3633.1841\n",
      "Epoch: 112/1000 | Batch 0009/0010 | Loss: 3897.5896\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 113/1000 | Batch 0000/0010 | Loss: 3862.5591\n",
      "Epoch: 113/1000 | Batch 0001/0010 | Loss: 3960.5625\n",
      "Epoch: 113/1000 | Batch 0002/0010 | Loss: 3651.9280\n",
      "Epoch: 113/1000 | Batch 0003/0010 | Loss: 3892.1663\n",
      "Epoch: 113/1000 | Batch 0004/0010 | Loss: 3632.7583\n",
      "Epoch: 113/1000 | Batch 0005/0010 | Loss: 3844.0742\n",
      "Epoch: 113/1000 | Batch 0006/0010 | Loss: 4103.3652\n",
      "Epoch: 113/1000 | Batch 0007/0010 | Loss: 3992.4529\n",
      "Epoch: 113/1000 | Batch 0008/0010 | Loss: 3651.8223\n",
      "Epoch: 113/1000 | Batch 0009/0010 | Loss: 3616.4041\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 114/1000 | Batch 0000/0010 | Loss: 4030.4299\n",
      "Epoch: 114/1000 | Batch 0001/0010 | Loss: 3541.6262\n",
      "Epoch: 114/1000 | Batch 0002/0010 | Loss: 4086.0508\n",
      "Epoch: 114/1000 | Batch 0003/0010 | Loss: 3748.3899\n",
      "Epoch: 114/1000 | Batch 0004/0010 | Loss: 3831.1875\n",
      "Epoch: 114/1000 | Batch 0005/0010 | Loss: 3699.8394\n",
      "Epoch: 114/1000 | Batch 0006/0010 | Loss: 3797.8809\n",
      "Epoch: 114/1000 | Batch 0007/0010 | Loss: 3544.9224\n",
      "Epoch: 114/1000 | Batch 0008/0010 | Loss: 3937.1545\n",
      "Epoch: 114/1000 | Batch 0009/0010 | Loss: 3961.1885\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 115/1000 | Batch 0000/0010 | Loss: 3764.8252\n",
      "Epoch: 115/1000 | Batch 0001/0010 | Loss: 3733.5869\n",
      "Epoch: 115/1000 | Batch 0002/0010 | Loss: 3615.4309\n",
      "Epoch: 115/1000 | Batch 0003/0010 | Loss: 3944.1562\n",
      "Epoch: 115/1000 | Batch 0004/0010 | Loss: 3948.8479\n",
      "Epoch: 115/1000 | Batch 0005/0010 | Loss: 3875.0898\n",
      "Epoch: 115/1000 | Batch 0006/0010 | Loss: 3808.0439\n",
      "Epoch: 115/1000 | Batch 0007/0010 | Loss: 3720.1543\n",
      "Epoch: 115/1000 | Batch 0008/0010 | Loss: 3812.7017\n",
      "Epoch: 115/1000 | Batch 0009/0010 | Loss: 4037.1250\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 116/1000 | Batch 0000/0010 | Loss: 3837.4673\n",
      "Epoch: 116/1000 | Batch 0001/0010 | Loss: 3826.6455\n",
      "Epoch: 116/1000 | Batch 0002/0010 | Loss: 3855.8235\n",
      "Epoch: 116/1000 | Batch 0003/0010 | Loss: 4032.4536\n",
      "Epoch: 116/1000 | Batch 0004/0010 | Loss: 3641.4802\n",
      "Epoch: 116/1000 | Batch 0005/0010 | Loss: 4147.5566\n",
      "Epoch: 116/1000 | Batch 0006/0010 | Loss: 3615.1396\n",
      "Epoch: 116/1000 | Batch 0007/0010 | Loss: 3642.5737\n",
      "Epoch: 116/1000 | Batch 0008/0010 | Loss: 3885.7822\n",
      "Epoch: 116/1000 | Batch 0009/0010 | Loss: 3700.1184\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 117/1000 | Batch 0000/0010 | Loss: 3878.3496\n",
      "Epoch: 117/1000 | Batch 0001/0010 | Loss: 3867.5977\n",
      "Epoch: 117/1000 | Batch 0002/0010 | Loss: 3983.3267\n",
      "Epoch: 117/1000 | Batch 0003/0010 | Loss: 3837.8694\n",
      "Epoch: 117/1000 | Batch 0004/0010 | Loss: 3958.6807\n",
      "Epoch: 117/1000 | Batch 0005/0010 | Loss: 3958.2920\n",
      "Epoch: 117/1000 | Batch 0006/0010 | Loss: 3691.3591\n",
      "Epoch: 117/1000 | Batch 0007/0010 | Loss: 3817.9717\n",
      "Epoch: 117/1000 | Batch 0008/0010 | Loss: 3695.1050\n",
      "Epoch: 117/1000 | Batch 0009/0010 | Loss: 3538.8286\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 118/1000 | Batch 0000/0010 | Loss: 4087.3159\n",
      "Epoch: 118/1000 | Batch 0001/0010 | Loss: 3679.1633\n",
      "Epoch: 118/1000 | Batch 0002/0010 | Loss: 4035.2507\n",
      "Epoch: 118/1000 | Batch 0003/0010 | Loss: 3695.0242\n",
      "Epoch: 118/1000 | Batch 0004/0010 | Loss: 3443.8796\n",
      "Epoch: 118/1000 | Batch 0005/0010 | Loss: 3884.4062\n",
      "Epoch: 118/1000 | Batch 0006/0010 | Loss: 3816.6707\n",
      "Epoch: 118/1000 | Batch 0007/0010 | Loss: 3895.4314\n",
      "Epoch: 118/1000 | Batch 0008/0010 | Loss: 4027.5349\n",
      "Epoch: 118/1000 | Batch 0009/0010 | Loss: 3724.9915\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 119/1000 | Batch 0000/0010 | Loss: 3909.1021\n",
      "Epoch: 119/1000 | Batch 0001/0010 | Loss: 3987.0881\n",
      "Epoch: 119/1000 | Batch 0002/0010 | Loss: 3649.5640\n",
      "Epoch: 119/1000 | Batch 0003/0010 | Loss: 3823.5837\n",
      "Epoch: 119/1000 | Batch 0004/0010 | Loss: 3885.0693\n",
      "Epoch: 119/1000 | Batch 0005/0010 | Loss: 3644.6443\n",
      "Epoch: 119/1000 | Batch 0006/0010 | Loss: 3927.0210\n",
      "Epoch: 119/1000 | Batch 0007/0010 | Loss: 3782.2480\n",
      "Epoch: 119/1000 | Batch 0008/0010 | Loss: 3889.0613\n",
      "Epoch: 119/1000 | Batch 0009/0010 | Loss: 3751.4702\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 120/1000 | Batch 0000/0010 | Loss: 3734.4761\n",
      "Epoch: 120/1000 | Batch 0001/0010 | Loss: 3625.7886\n",
      "Epoch: 120/1000 | Batch 0002/0010 | Loss: 4161.1689\n",
      "Epoch: 120/1000 | Batch 0003/0010 | Loss: 3706.8833\n",
      "Epoch: 120/1000 | Batch 0004/0010 | Loss: 3932.3823\n",
      "Epoch: 120/1000 | Batch 0005/0010 | Loss: 3968.1958\n",
      "Epoch: 120/1000 | Batch 0006/0010 | Loss: 3731.9939\n",
      "Epoch: 120/1000 | Batch 0007/0010 | Loss: 3627.8501\n",
      "Epoch: 120/1000 | Batch 0008/0010 | Loss: 3885.0632\n",
      "Epoch: 120/1000 | Batch 0009/0010 | Loss: 3895.8618\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 121/1000 | Batch 0000/0010 | Loss: 3735.5647\n",
      "Epoch: 121/1000 | Batch 0001/0010 | Loss: 3679.8645\n",
      "Epoch: 121/1000 | Batch 0002/0010 | Loss: 3712.6680\n",
      "Epoch: 121/1000 | Batch 0003/0010 | Loss: 3843.0776\n",
      "Epoch: 121/1000 | Batch 0004/0010 | Loss: 3760.2747\n",
      "Epoch: 121/1000 | Batch 0005/0010 | Loss: 3992.4873\n",
      "Epoch: 121/1000 | Batch 0006/0010 | Loss: 3883.1951\n",
      "Epoch: 121/1000 | Batch 0007/0010 | Loss: 4062.3933\n",
      "Epoch: 121/1000 | Batch 0008/0010 | Loss: 3946.9897\n",
      "Epoch: 121/1000 | Batch 0009/0010 | Loss: 3808.5190\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 122/1000 | Batch 0000/0010 | Loss: 3895.2710\n",
      "Epoch: 122/1000 | Batch 0001/0010 | Loss: 3758.6995\n",
      "Epoch: 122/1000 | Batch 0002/0010 | Loss: 3905.9731\n",
      "Epoch: 122/1000 | Batch 0003/0010 | Loss: 3762.0476\n",
      "Epoch: 122/1000 | Batch 0004/0010 | Loss: 3896.7566\n",
      "Epoch: 122/1000 | Batch 0005/0010 | Loss: 3671.7825\n",
      "Epoch: 122/1000 | Batch 0006/0010 | Loss: 3910.3860\n",
      "Epoch: 122/1000 | Batch 0007/0010 | Loss: 3782.6116\n",
      "Epoch: 122/1000 | Batch 0008/0010 | Loss: 3849.0256\n",
      "Epoch: 122/1000 | Batch 0009/0010 | Loss: 3798.7957\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 123/1000 | Batch 0000/0010 | Loss: 3819.0125\n",
      "Epoch: 123/1000 | Batch 0001/0010 | Loss: 3572.9602\n",
      "Epoch: 123/1000 | Batch 0002/0010 | Loss: 3879.7952\n",
      "Epoch: 123/1000 | Batch 0003/0010 | Loss: 3789.0552\n",
      "Epoch: 123/1000 | Batch 0004/0010 | Loss: 3992.2390\n",
      "Epoch: 123/1000 | Batch 0005/0010 | Loss: 3927.0034\n",
      "Epoch: 123/1000 | Batch 0006/0010 | Loss: 3725.3796\n",
      "Epoch: 123/1000 | Batch 0007/0010 | Loss: 4037.0034\n",
      "Epoch: 123/1000 | Batch 0008/0010 | Loss: 3698.6826\n",
      "Epoch: 123/1000 | Batch 0009/0010 | Loss: 3889.0054\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 124/1000 | Batch 0000/0010 | Loss: 3787.9504\n",
      "Epoch: 124/1000 | Batch 0001/0010 | Loss: 3798.7312\n",
      "Epoch: 124/1000 | Batch 0002/0010 | Loss: 3884.0017\n",
      "Epoch: 124/1000 | Batch 0003/0010 | Loss: 3975.5911\n",
      "Epoch: 124/1000 | Batch 0004/0010 | Loss: 3608.8418\n",
      "Epoch: 124/1000 | Batch 0005/0010 | Loss: 3726.4551\n",
      "Epoch: 124/1000 | Batch 0006/0010 | Loss: 4073.0081\n",
      "Epoch: 124/1000 | Batch 0007/0010 | Loss: 3852.3005\n",
      "Epoch: 124/1000 | Batch 0008/0010 | Loss: 3568.7668\n",
      "Epoch: 124/1000 | Batch 0009/0010 | Loss: 3976.0791\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 125/1000 | Batch 0000/0010 | Loss: 3736.0881\n",
      "Epoch: 125/1000 | Batch 0001/0010 | Loss: 3961.7512\n",
      "Epoch: 125/1000 | Batch 0002/0010 | Loss: 3678.8455\n",
      "Epoch: 125/1000 | Batch 0003/0010 | Loss: 3872.4292\n",
      "Epoch: 125/1000 | Batch 0004/0010 | Loss: 3994.0591\n",
      "Epoch: 125/1000 | Batch 0005/0010 | Loss: 3713.6873\n",
      "Epoch: 125/1000 | Batch 0006/0010 | Loss: 3729.1191\n",
      "Epoch: 125/1000 | Batch 0007/0010 | Loss: 3943.1077\n",
      "Epoch: 125/1000 | Batch 0008/0010 | Loss: 3761.0532\n",
      "Epoch: 125/1000 | Batch 0009/0010 | Loss: 3670.3394\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 126/1000 | Batch 0000/0010 | Loss: 3694.5361\n",
      "Epoch: 126/1000 | Batch 0001/0010 | Loss: 3861.2705\n",
      "Epoch: 126/1000 | Batch 0002/0010 | Loss: 3963.8291\n",
      "Epoch: 126/1000 | Batch 0003/0010 | Loss: 3706.2471\n",
      "Epoch: 126/1000 | Batch 0004/0010 | Loss: 3960.2834\n",
      "Epoch: 126/1000 | Batch 0005/0010 | Loss: 3876.9907\n",
      "Epoch: 126/1000 | Batch 0006/0010 | Loss: 3664.6860\n",
      "Epoch: 126/1000 | Batch 0007/0010 | Loss: 3701.0364\n",
      "Epoch: 126/1000 | Batch 0008/0010 | Loss: 3617.3296\n",
      "Epoch: 126/1000 | Batch 0009/0010 | Loss: 4021.1345\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 127/1000 | Batch 0000/0010 | Loss: 3653.0198\n",
      "Epoch: 127/1000 | Batch 0001/0010 | Loss: 3654.3032\n",
      "Epoch: 127/1000 | Batch 0002/0010 | Loss: 3916.1826\n",
      "Epoch: 127/1000 | Batch 0003/0010 | Loss: 3924.2991\n",
      "Epoch: 127/1000 | Batch 0004/0010 | Loss: 4340.8394\n",
      "Epoch: 127/1000 | Batch 0005/0010 | Loss: 3697.3179\n",
      "Epoch: 127/1000 | Batch 0006/0010 | Loss: 3566.8965\n",
      "Epoch: 127/1000 | Batch 0007/0010 | Loss: 3597.4595\n",
      "Epoch: 127/1000 | Batch 0008/0010 | Loss: 3691.6643\n",
      "Epoch: 127/1000 | Batch 0009/0010 | Loss: 4105.7368\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 128/1000 | Batch 0000/0010 | Loss: 3555.6289\n",
      "Epoch: 128/1000 | Batch 0001/0010 | Loss: 3581.0078\n",
      "Epoch: 128/1000 | Batch 0002/0010 | Loss: 3790.7759\n",
      "Epoch: 128/1000 | Batch 0003/0010 | Loss: 4004.3496\n",
      "Epoch: 128/1000 | Batch 0004/0010 | Loss: 3722.1628\n",
      "Epoch: 128/1000 | Batch 0005/0010 | Loss: 3840.2166\n",
      "Epoch: 128/1000 | Batch 0006/0010 | Loss: 4061.7698\n",
      "Epoch: 128/1000 | Batch 0007/0010 | Loss: 3839.9851\n",
      "Epoch: 128/1000 | Batch 0008/0010 | Loss: 3737.0271\n",
      "Epoch: 128/1000 | Batch 0009/0010 | Loss: 4016.2620\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 129/1000 | Batch 0000/0010 | Loss: 3864.0732\n",
      "Epoch: 129/1000 | Batch 0001/0010 | Loss: 3799.8335\n",
      "Epoch: 129/1000 | Batch 0002/0010 | Loss: 3626.4985\n",
      "Epoch: 129/1000 | Batch 0003/0010 | Loss: 3654.7139\n",
      "Epoch: 129/1000 | Batch 0004/0010 | Loss: 3830.4062\n",
      "Epoch: 129/1000 | Batch 0005/0010 | Loss: 3510.7839\n",
      "Epoch: 129/1000 | Batch 0006/0010 | Loss: 3968.4377\n",
      "Epoch: 129/1000 | Batch 0007/0010 | Loss: 3918.0505\n",
      "Epoch: 129/1000 | Batch 0008/0010 | Loss: 4069.4609\n",
      "Epoch: 129/1000 | Batch 0009/0010 | Loss: 3870.3577\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 130/1000 | Batch 0000/0010 | Loss: 4024.7507\n",
      "Epoch: 130/1000 | Batch 0001/0010 | Loss: 3489.0642\n",
      "Epoch: 130/1000 | Batch 0002/0010 | Loss: 3748.2290\n",
      "Epoch: 130/1000 | Batch 0003/0010 | Loss: 3747.2910\n",
      "Epoch: 130/1000 | Batch 0004/0010 | Loss: 3538.6545\n",
      "Epoch: 130/1000 | Batch 0005/0010 | Loss: 4013.2212\n",
      "Epoch: 130/1000 | Batch 0006/0010 | Loss: 3860.1533\n",
      "Epoch: 130/1000 | Batch 0007/0010 | Loss: 4038.6963\n",
      "Epoch: 130/1000 | Batch 0008/0010 | Loss: 3873.9683\n",
      "Epoch: 130/1000 | Batch 0009/0010 | Loss: 3796.8870\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 131/1000 | Batch 0000/0010 | Loss: 3864.4104\n",
      "Epoch: 131/1000 | Batch 0001/0010 | Loss: 3805.4822\n",
      "Epoch: 131/1000 | Batch 0002/0010 | Loss: 3885.7068\n",
      "Epoch: 131/1000 | Batch 0003/0010 | Loss: 3643.7622\n",
      "Epoch: 131/1000 | Batch 0004/0010 | Loss: 3742.5693\n",
      "Epoch: 131/1000 | Batch 0005/0010 | Loss: 3724.9924\n",
      "Epoch: 131/1000 | Batch 0006/0010 | Loss: 3765.4866\n",
      "Epoch: 131/1000 | Batch 0007/0010 | Loss: 3997.8079\n",
      "Epoch: 131/1000 | Batch 0008/0010 | Loss: 3694.5513\n",
      "Epoch: 131/1000 | Batch 0009/0010 | Loss: 3898.9185\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 132/1000 | Batch 0000/0010 | Loss: 3981.7422\n",
      "Epoch: 132/1000 | Batch 0001/0010 | Loss: 3588.3296\n",
      "Epoch: 132/1000 | Batch 0002/0010 | Loss: 3912.8767\n",
      "Epoch: 132/1000 | Batch 0003/0010 | Loss: 3756.8418\n",
      "Epoch: 132/1000 | Batch 0004/0010 | Loss: 3692.8215\n",
      "Epoch: 132/1000 | Batch 0005/0010 | Loss: 3805.3606\n",
      "Epoch: 132/1000 | Batch 0006/0010 | Loss: 3737.6785\n",
      "Epoch: 132/1000 | Batch 0007/0010 | Loss: 3680.4128\n",
      "Epoch: 132/1000 | Batch 0008/0010 | Loss: 3821.4023\n",
      "Epoch: 132/1000 | Batch 0009/0010 | Loss: 4051.1562\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 133/1000 | Batch 0000/0010 | Loss: 3613.1680\n",
      "Epoch: 133/1000 | Batch 0001/0010 | Loss: 3852.9160\n",
      "Epoch: 133/1000 | Batch 0002/0010 | Loss: 4007.1873\n",
      "Epoch: 133/1000 | Batch 0003/0010 | Loss: 3737.0769\n",
      "Epoch: 133/1000 | Batch 0004/0010 | Loss: 3824.4644\n",
      "Epoch: 133/1000 | Batch 0005/0010 | Loss: 3818.5708\n",
      "Epoch: 133/1000 | Batch 0006/0010 | Loss: 3853.9722\n",
      "Epoch: 133/1000 | Batch 0007/0010 | Loss: 4151.1118\n",
      "Epoch: 133/1000 | Batch 0008/0010 | Loss: 3669.8091\n",
      "Epoch: 133/1000 | Batch 0009/0010 | Loss: 3597.9463\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 134/1000 | Batch 0000/0010 | Loss: 3572.3867\n",
      "Epoch: 134/1000 | Batch 0001/0010 | Loss: 3878.8018\n",
      "Epoch: 134/1000 | Batch 0002/0010 | Loss: 3611.8921\n",
      "Epoch: 134/1000 | Batch 0003/0010 | Loss: 4048.4888\n",
      "Epoch: 134/1000 | Batch 0004/0010 | Loss: 3418.7698\n",
      "Epoch: 134/1000 | Batch 0005/0010 | Loss: 3933.4946\n",
      "Epoch: 134/1000 | Batch 0006/0010 | Loss: 3754.7295\n",
      "Epoch: 134/1000 | Batch 0007/0010 | Loss: 3934.9321\n",
      "Epoch: 134/1000 | Batch 0008/0010 | Loss: 3963.4575\n",
      "Epoch: 134/1000 | Batch 0009/0010 | Loss: 4009.6104\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 135/1000 | Batch 0000/0010 | Loss: 3816.2979\n",
      "Epoch: 135/1000 | Batch 0001/0010 | Loss: 3743.5559\n",
      "Epoch: 135/1000 | Batch 0002/0010 | Loss: 3724.6467\n",
      "Epoch: 135/1000 | Batch 0003/0010 | Loss: 3955.8130\n",
      "Epoch: 135/1000 | Batch 0004/0010 | Loss: 3883.7278\n",
      "Epoch: 135/1000 | Batch 0005/0010 | Loss: 3586.4771\n",
      "Epoch: 135/1000 | Batch 0006/0010 | Loss: 3870.7319\n",
      "Epoch: 135/1000 | Batch 0007/0010 | Loss: 3657.8384\n",
      "Epoch: 135/1000 | Batch 0008/0010 | Loss: 4043.9080\n",
      "Epoch: 135/1000 | Batch 0009/0010 | Loss: 3903.0747\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 136/1000 | Batch 0000/0010 | Loss: 4037.4067\n",
      "Epoch: 136/1000 | Batch 0001/0010 | Loss: 3439.1248\n",
      "Epoch: 136/1000 | Batch 0002/0010 | Loss: 3481.7788\n",
      "Epoch: 136/1000 | Batch 0003/0010 | Loss: 3952.2573\n",
      "Epoch: 136/1000 | Batch 0004/0010 | Loss: 3978.0374\n",
      "Epoch: 136/1000 | Batch 0005/0010 | Loss: 3916.0732\n",
      "Epoch: 136/1000 | Batch 0006/0010 | Loss: 3681.9397\n",
      "Epoch: 136/1000 | Batch 0007/0010 | Loss: 3929.1018\n",
      "Epoch: 136/1000 | Batch 0008/0010 | Loss: 4020.3945\n",
      "Epoch: 136/1000 | Batch 0009/0010 | Loss: 3743.9321\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 137/1000 | Batch 0000/0010 | Loss: 4001.8074\n",
      "Epoch: 137/1000 | Batch 0001/0010 | Loss: 3973.3264\n",
      "Epoch: 137/1000 | Batch 0002/0010 | Loss: 3856.7686\n",
      "Epoch: 137/1000 | Batch 0003/0010 | Loss: 3608.0999\n",
      "Epoch: 137/1000 | Batch 0004/0010 | Loss: 3680.6870\n",
      "Epoch: 137/1000 | Batch 0005/0010 | Loss: 3602.5635\n",
      "Epoch: 137/1000 | Batch 0006/0010 | Loss: 4021.9312\n",
      "Epoch: 137/1000 | Batch 0007/0010 | Loss: 3804.8054\n",
      "Epoch: 137/1000 | Batch 0008/0010 | Loss: 3627.2744\n",
      "Epoch: 137/1000 | Batch 0009/0010 | Loss: 3971.1746\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 138/1000 | Batch 0000/0010 | Loss: 3677.6643\n",
      "Epoch: 138/1000 | Batch 0001/0010 | Loss: 4075.5105\n",
      "Epoch: 138/1000 | Batch 0002/0010 | Loss: 3584.6436\n",
      "Epoch: 138/1000 | Batch 0003/0010 | Loss: 3815.0508\n",
      "Epoch: 138/1000 | Batch 0004/0010 | Loss: 3977.4324\n",
      "Epoch: 138/1000 | Batch 0005/0010 | Loss: 3547.4248\n",
      "Epoch: 138/1000 | Batch 0006/0010 | Loss: 3793.4592\n",
      "Epoch: 138/1000 | Batch 0007/0010 | Loss: 3849.6216\n",
      "Epoch: 138/1000 | Batch 0008/0010 | Loss: 3759.8635\n",
      "Epoch: 138/1000 | Batch 0009/0010 | Loss: 4038.5522\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 139/1000 | Batch 0000/0010 | Loss: 3813.7815\n",
      "Epoch: 139/1000 | Batch 0001/0010 | Loss: 3537.1169\n",
      "Epoch: 139/1000 | Batch 0002/0010 | Loss: 4017.1133\n",
      "Epoch: 139/1000 | Batch 0003/0010 | Loss: 3822.0737\n",
      "Epoch: 139/1000 | Batch 0004/0010 | Loss: 3816.1731\n",
      "Epoch: 139/1000 | Batch 0005/0010 | Loss: 3808.4700\n",
      "Epoch: 139/1000 | Batch 0006/0010 | Loss: 3720.7532\n",
      "Epoch: 139/1000 | Batch 0007/0010 | Loss: 3887.1831\n",
      "Epoch: 139/1000 | Batch 0008/0010 | Loss: 3873.1851\n",
      "Epoch: 139/1000 | Batch 0009/0010 | Loss: 3855.6174\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 140/1000 | Batch 0000/0010 | Loss: 3673.5071\n",
      "Epoch: 140/1000 | Batch 0001/0010 | Loss: 3676.9951\n",
      "Epoch: 140/1000 | Batch 0002/0010 | Loss: 3737.2461\n",
      "Epoch: 140/1000 | Batch 0003/0010 | Loss: 3932.2993\n",
      "Epoch: 140/1000 | Batch 0004/0010 | Loss: 3725.5557\n",
      "Epoch: 140/1000 | Batch 0005/0010 | Loss: 3690.9341\n",
      "Epoch: 140/1000 | Batch 0006/0010 | Loss: 3829.4604\n",
      "Epoch: 140/1000 | Batch 0007/0010 | Loss: 4063.3391\n",
      "Epoch: 140/1000 | Batch 0008/0010 | Loss: 3960.5881\n",
      "Epoch: 140/1000 | Batch 0009/0010 | Loss: 3777.1069\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 141/1000 | Batch 0000/0010 | Loss: 3842.8167\n",
      "Epoch: 141/1000 | Batch 0001/0010 | Loss: 3838.1951\n",
      "Epoch: 141/1000 | Batch 0002/0010 | Loss: 3621.9897\n",
      "Epoch: 141/1000 | Batch 0003/0010 | Loss: 4164.2520\n",
      "Epoch: 141/1000 | Batch 0004/0010 | Loss: 3631.2590\n",
      "Epoch: 141/1000 | Batch 0005/0010 | Loss: 3635.7793\n",
      "Epoch: 141/1000 | Batch 0006/0010 | Loss: 3709.3230\n",
      "Epoch: 141/1000 | Batch 0007/0010 | Loss: 3854.4363\n",
      "Epoch: 141/1000 | Batch 0008/0010 | Loss: 3953.6726\n",
      "Epoch: 141/1000 | Batch 0009/0010 | Loss: 3757.2271\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 142/1000 | Batch 0000/0010 | Loss: 3734.1477\n",
      "Epoch: 142/1000 | Batch 0001/0010 | Loss: 3931.2390\n",
      "Epoch: 142/1000 | Batch 0002/0010 | Loss: 3885.5830\n",
      "Epoch: 142/1000 | Batch 0003/0010 | Loss: 3632.1196\n",
      "Epoch: 142/1000 | Batch 0004/0010 | Loss: 3800.5308\n",
      "Epoch: 142/1000 | Batch 0005/0010 | Loss: 4065.1140\n",
      "Epoch: 142/1000 | Batch 0006/0010 | Loss: 3645.8735\n",
      "Epoch: 142/1000 | Batch 0007/0010 | Loss: 3969.5605\n",
      "Epoch: 142/1000 | Batch 0008/0010 | Loss: 3774.1809\n",
      "Epoch: 142/1000 | Batch 0009/0010 | Loss: 3626.1235\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 143/1000 | Batch 0000/0010 | Loss: 4078.5359\n",
      "Epoch: 143/1000 | Batch 0001/0010 | Loss: 3861.6721\n",
      "Epoch: 143/1000 | Batch 0002/0010 | Loss: 3982.9868\n",
      "Epoch: 143/1000 | Batch 0003/0010 | Loss: 3727.9226\n",
      "Epoch: 143/1000 | Batch 0004/0010 | Loss: 3842.9045\n",
      "Epoch: 143/1000 | Batch 0005/0010 | Loss: 3580.6167\n",
      "Epoch: 143/1000 | Batch 0006/0010 | Loss: 3712.1101\n",
      "Epoch: 143/1000 | Batch 0007/0010 | Loss: 3654.7578\n",
      "Epoch: 143/1000 | Batch 0008/0010 | Loss: 3873.5662\n",
      "Epoch: 143/1000 | Batch 0009/0010 | Loss: 3748.8320\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 144/1000 | Batch 0000/0010 | Loss: 3942.9426\n",
      "Epoch: 144/1000 | Batch 0001/0010 | Loss: 3604.8901\n",
      "Epoch: 144/1000 | Batch 0002/0010 | Loss: 3947.4551\n",
      "Epoch: 144/1000 | Batch 0003/0010 | Loss: 3904.2905\n",
      "Epoch: 144/1000 | Batch 0004/0010 | Loss: 3900.7256\n",
      "Epoch: 144/1000 | Batch 0005/0010 | Loss: 3960.8635\n",
      "Epoch: 144/1000 | Batch 0006/0010 | Loss: 3415.1694\n",
      "Epoch: 144/1000 | Batch 0007/0010 | Loss: 3736.9424\n",
      "Epoch: 144/1000 | Batch 0008/0010 | Loss: 3783.4761\n",
      "Epoch: 144/1000 | Batch 0009/0010 | Loss: 4001.6912\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 145/1000 | Batch 0000/0010 | Loss: 3724.8091\n",
      "Epoch: 145/1000 | Batch 0001/0010 | Loss: 3768.5540\n",
      "Epoch: 145/1000 | Batch 0002/0010 | Loss: 3852.9712\n",
      "Epoch: 145/1000 | Batch 0003/0010 | Loss: 3583.7117\n",
      "Epoch: 145/1000 | Batch 0004/0010 | Loss: 3957.8079\n",
      "Epoch: 145/1000 | Batch 0005/0010 | Loss: 3689.8831\n",
      "Epoch: 145/1000 | Batch 0006/0010 | Loss: 4166.5781\n",
      "Epoch: 145/1000 | Batch 0007/0010 | Loss: 3829.5366\n",
      "Epoch: 145/1000 | Batch 0008/0010 | Loss: 3888.2012\n",
      "Epoch: 145/1000 | Batch 0009/0010 | Loss: 3635.6909\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 146/1000 | Batch 0000/0010 | Loss: 3811.0278\n",
      "Epoch: 146/1000 | Batch 0001/0010 | Loss: 3661.1724\n",
      "Epoch: 146/1000 | Batch 0002/0010 | Loss: 3977.6169\n",
      "Epoch: 146/1000 | Batch 0003/0010 | Loss: 4001.0117\n",
      "Epoch: 146/1000 | Batch 0004/0010 | Loss: 3700.4431\n",
      "Epoch: 146/1000 | Batch 0005/0010 | Loss: 3787.7698\n",
      "Epoch: 146/1000 | Batch 0006/0010 | Loss: 3845.2581\n",
      "Epoch: 146/1000 | Batch 0007/0010 | Loss: 3808.0723\n",
      "Epoch: 146/1000 | Batch 0008/0010 | Loss: 3673.0742\n",
      "Epoch: 146/1000 | Batch 0009/0010 | Loss: 3842.2693\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 147/1000 | Batch 0000/0010 | Loss: 3350.5278\n",
      "Epoch: 147/1000 | Batch 0001/0010 | Loss: 3722.3037\n",
      "Epoch: 147/1000 | Batch 0002/0010 | Loss: 4029.8906\n",
      "Epoch: 147/1000 | Batch 0003/0010 | Loss: 3704.8179\n",
      "Epoch: 147/1000 | Batch 0004/0010 | Loss: 3716.1406\n",
      "Epoch: 147/1000 | Batch 0005/0010 | Loss: 3831.3572\n",
      "Epoch: 147/1000 | Batch 0006/0010 | Loss: 4142.8330\n",
      "Epoch: 147/1000 | Batch 0007/0010 | Loss: 3956.7385\n",
      "Epoch: 147/1000 | Batch 0008/0010 | Loss: 3828.5374\n",
      "Epoch: 147/1000 | Batch 0009/0010 | Loss: 3748.9375\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 148/1000 | Batch 0000/0010 | Loss: 3652.5964\n",
      "Epoch: 148/1000 | Batch 0001/0010 | Loss: 3984.8018\n",
      "Epoch: 148/1000 | Batch 0002/0010 | Loss: 3726.1284\n",
      "Epoch: 148/1000 | Batch 0003/0010 | Loss: 4042.5671\n",
      "Epoch: 148/1000 | Batch 0004/0010 | Loss: 3818.2869\n",
      "Epoch: 148/1000 | Batch 0005/0010 | Loss: 3541.3235\n",
      "Epoch: 148/1000 | Batch 0006/0010 | Loss: 3965.3352\n",
      "Epoch: 148/1000 | Batch 0007/0010 | Loss: 3548.0129\n",
      "Epoch: 148/1000 | Batch 0008/0010 | Loss: 4039.1504\n",
      "Epoch: 148/1000 | Batch 0009/0010 | Loss: 3771.3567\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 149/1000 | Batch 0000/0010 | Loss: 3696.8926\n",
      "Epoch: 149/1000 | Batch 0001/0010 | Loss: 3732.8938\n",
      "Epoch: 149/1000 | Batch 0002/0010 | Loss: 3589.4546\n",
      "Epoch: 149/1000 | Batch 0003/0010 | Loss: 3904.6555\n",
      "Epoch: 149/1000 | Batch 0004/0010 | Loss: 3693.3032\n",
      "Epoch: 149/1000 | Batch 0005/0010 | Loss: 3912.1729\n",
      "Epoch: 149/1000 | Batch 0006/0010 | Loss: 4161.1411\n",
      "Epoch: 149/1000 | Batch 0007/0010 | Loss: 3720.9094\n",
      "Epoch: 149/1000 | Batch 0008/0010 | Loss: 3928.7166\n",
      "Epoch: 149/1000 | Batch 0009/0010 | Loss: 3824.6587\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 150/1000 | Batch 0000/0010 | Loss: 3703.7161\n",
      "Epoch: 150/1000 | Batch 0001/0010 | Loss: 4040.8254\n",
      "Epoch: 150/1000 | Batch 0002/0010 | Loss: 3946.5747\n",
      "Epoch: 150/1000 | Batch 0003/0010 | Loss: 3874.7783\n",
      "Epoch: 150/1000 | Batch 0004/0010 | Loss: 3615.1128\n",
      "Epoch: 150/1000 | Batch 0005/0010 | Loss: 3941.8682\n",
      "Epoch: 150/1000 | Batch 0006/0010 | Loss: 3532.1436\n",
      "Epoch: 150/1000 | Batch 0007/0010 | Loss: 3677.4580\n",
      "Epoch: 150/1000 | Batch 0008/0010 | Loss: 4020.5569\n",
      "Epoch: 150/1000 | Batch 0009/0010 | Loss: 3939.5381\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 151/1000 | Batch 0000/0010 | Loss: 3867.7891\n",
      "Epoch: 151/1000 | Batch 0001/0010 | Loss: 4056.1323\n",
      "Epoch: 151/1000 | Batch 0002/0010 | Loss: 4034.4722\n",
      "Epoch: 151/1000 | Batch 0003/0010 | Loss: 3759.8274\n",
      "Epoch: 151/1000 | Batch 0004/0010 | Loss: 3827.2402\n",
      "Epoch: 151/1000 | Batch 0005/0010 | Loss: 3938.2644\n",
      "Epoch: 151/1000 | Batch 0006/0010 | Loss: 3503.8494\n",
      "Epoch: 151/1000 | Batch 0007/0010 | Loss: 3926.5835\n",
      "Epoch: 151/1000 | Batch 0008/0010 | Loss: 3703.1143\n",
      "Epoch: 151/1000 | Batch 0009/0010 | Loss: 3571.9180\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 152/1000 | Batch 0000/0010 | Loss: 3930.7522\n",
      "Epoch: 152/1000 | Batch 0001/0010 | Loss: 4126.5225\n",
      "Epoch: 152/1000 | Batch 0002/0010 | Loss: 3929.6392\n",
      "Epoch: 152/1000 | Batch 0003/0010 | Loss: 3683.1453\n",
      "Epoch: 152/1000 | Batch 0004/0010 | Loss: 3868.3584\n",
      "Epoch: 152/1000 | Batch 0005/0010 | Loss: 3671.0339\n",
      "Epoch: 152/1000 | Batch 0006/0010 | Loss: 3735.8870\n",
      "Epoch: 152/1000 | Batch 0007/0010 | Loss: 3980.4368\n",
      "Epoch: 152/1000 | Batch 0008/0010 | Loss: 3458.7312\n",
      "Epoch: 152/1000 | Batch 0009/0010 | Loss: 4029.8386\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 153/1000 | Batch 0000/0010 | Loss: 3688.6274\n",
      "Epoch: 153/1000 | Batch 0001/0010 | Loss: 3839.8579\n",
      "Epoch: 153/1000 | Batch 0002/0010 | Loss: 3547.6353\n",
      "Epoch: 153/1000 | Batch 0003/0010 | Loss: 3902.8499\n",
      "Epoch: 153/1000 | Batch 0004/0010 | Loss: 3890.5137\n",
      "Epoch: 153/1000 | Batch 0005/0010 | Loss: 3889.8975\n",
      "Epoch: 153/1000 | Batch 0006/0010 | Loss: 4005.2944\n",
      "Epoch: 153/1000 | Batch 0007/0010 | Loss: 3848.3469\n",
      "Epoch: 153/1000 | Batch 0008/0010 | Loss: 3930.9746\n",
      "Epoch: 153/1000 | Batch 0009/0010 | Loss: 3857.4844\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 154/1000 | Batch 0000/0010 | Loss: 3722.8059\n",
      "Epoch: 154/1000 | Batch 0001/0010 | Loss: 3656.7629\n",
      "Epoch: 154/1000 | Batch 0002/0010 | Loss: 3961.8818\n",
      "Epoch: 154/1000 | Batch 0003/0010 | Loss: 3918.1443\n",
      "Epoch: 154/1000 | Batch 0004/0010 | Loss: 3623.5852\n",
      "Epoch: 154/1000 | Batch 0005/0010 | Loss: 4011.8774\n",
      "Epoch: 154/1000 | Batch 0006/0010 | Loss: 4243.0225\n",
      "Epoch: 154/1000 | Batch 0007/0010 | Loss: 3810.7354\n",
      "Epoch: 154/1000 | Batch 0008/0010 | Loss: 3578.7397\n",
      "Epoch: 154/1000 | Batch 0009/0010 | Loss: 4073.9114\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 155/1000 | Batch 0000/0010 | Loss: 3898.7505\n",
      "Epoch: 155/1000 | Batch 0001/0010 | Loss: 3752.0464\n",
      "Epoch: 155/1000 | Batch 0002/0010 | Loss: 3701.1836\n",
      "Epoch: 155/1000 | Batch 0003/0010 | Loss: 3941.9128\n",
      "Epoch: 155/1000 | Batch 0004/0010 | Loss: 3881.9194\n",
      "Epoch: 155/1000 | Batch 0005/0010 | Loss: 3640.9277\n",
      "Epoch: 155/1000 | Batch 0006/0010 | Loss: 4018.1931\n",
      "Epoch: 155/1000 | Batch 0007/0010 | Loss: 4015.4133\n",
      "Epoch: 155/1000 | Batch 0008/0010 | Loss: 3847.1055\n",
      "Epoch: 155/1000 | Batch 0009/0010 | Loss: 3958.6033\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 156/1000 | Batch 0000/0010 | Loss: 3598.4907\n",
      "Epoch: 156/1000 | Batch 0001/0010 | Loss: 3631.8406\n",
      "Epoch: 156/1000 | Batch 0002/0010 | Loss: 3923.0481\n",
      "Epoch: 156/1000 | Batch 0003/0010 | Loss: 4047.4841\n",
      "Epoch: 156/1000 | Batch 0004/0010 | Loss: 3894.3279\n",
      "Epoch: 156/1000 | Batch 0005/0010 | Loss: 3827.2786\n",
      "Epoch: 156/1000 | Batch 0006/0010 | Loss: 3977.1287\n",
      "Epoch: 156/1000 | Batch 0007/0010 | Loss: 3752.2239\n",
      "Epoch: 156/1000 | Batch 0008/0010 | Loss: 3913.0234\n",
      "Epoch: 156/1000 | Batch 0009/0010 | Loss: 3871.1494\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 157/1000 | Batch 0000/0010 | Loss: 3550.3152\n",
      "Epoch: 157/1000 | Batch 0001/0010 | Loss: 4065.7654\n",
      "Epoch: 157/1000 | Batch 0002/0010 | Loss: 3505.3560\n",
      "Epoch: 157/1000 | Batch 0003/0010 | Loss: 3705.4856\n",
      "Epoch: 157/1000 | Batch 0004/0010 | Loss: 3696.6375\n",
      "Epoch: 157/1000 | Batch 0005/0010 | Loss: 3853.6714\n",
      "Epoch: 157/1000 | Batch 0006/0010 | Loss: 3892.1650\n",
      "Epoch: 157/1000 | Batch 0007/0010 | Loss: 4029.4612\n",
      "Epoch: 157/1000 | Batch 0008/0010 | Loss: 4040.5935\n",
      "Epoch: 157/1000 | Batch 0009/0010 | Loss: 3846.0840\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 158/1000 | Batch 0000/0010 | Loss: 4015.6699\n",
      "Epoch: 158/1000 | Batch 0001/0010 | Loss: 3646.3782\n",
      "Epoch: 158/1000 | Batch 0002/0010 | Loss: 3671.6836\n",
      "Epoch: 158/1000 | Batch 0003/0010 | Loss: 4109.8887\n",
      "Epoch: 158/1000 | Batch 0004/0010 | Loss: 3843.9917\n",
      "Epoch: 158/1000 | Batch 0005/0010 | Loss: 3914.9009\n",
      "Epoch: 158/1000 | Batch 0006/0010 | Loss: 3570.5845\n",
      "Epoch: 158/1000 | Batch 0007/0010 | Loss: 4011.7883\n",
      "Epoch: 158/1000 | Batch 0008/0010 | Loss: 3671.8005\n",
      "Epoch: 158/1000 | Batch 0009/0010 | Loss: 3647.2021\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 159/1000 | Batch 0000/0010 | Loss: 4027.7971\n",
      "Epoch: 159/1000 | Batch 0001/0010 | Loss: 3907.8516\n",
      "Epoch: 159/1000 | Batch 0002/0010 | Loss: 3776.5786\n",
      "Epoch: 159/1000 | Batch 0003/0010 | Loss: 3639.8491\n",
      "Epoch: 159/1000 | Batch 0004/0010 | Loss: 3626.2727\n",
      "Epoch: 159/1000 | Batch 0005/0010 | Loss: 4105.3286\n",
      "Epoch: 159/1000 | Batch 0006/0010 | Loss: 3808.2070\n",
      "Epoch: 159/1000 | Batch 0007/0010 | Loss: 3554.0845\n",
      "Epoch: 159/1000 | Batch 0008/0010 | Loss: 3985.4434\n",
      "Epoch: 159/1000 | Batch 0009/0010 | Loss: 3684.8381\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 160/1000 | Batch 0000/0010 | Loss: 3851.5718\n",
      "Epoch: 160/1000 | Batch 0001/0010 | Loss: 3664.0227\n",
      "Epoch: 160/1000 | Batch 0002/0010 | Loss: 3650.7500\n",
      "Epoch: 160/1000 | Batch 0003/0010 | Loss: 4186.0942\n",
      "Epoch: 160/1000 | Batch 0004/0010 | Loss: 3654.6621\n",
      "Epoch: 160/1000 | Batch 0005/0010 | Loss: 3756.2336\n",
      "Epoch: 160/1000 | Batch 0006/0010 | Loss: 3778.3674\n",
      "Epoch: 160/1000 | Batch 0007/0010 | Loss: 4241.3003\n",
      "Epoch: 160/1000 | Batch 0008/0010 | Loss: 3702.0813\n",
      "Epoch: 160/1000 | Batch 0009/0010 | Loss: 3634.4404\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 161/1000 | Batch 0000/0010 | Loss: 3760.2349\n",
      "Epoch: 161/1000 | Batch 0001/0010 | Loss: 4018.5520\n",
      "Epoch: 161/1000 | Batch 0002/0010 | Loss: 3829.5081\n",
      "Epoch: 161/1000 | Batch 0003/0010 | Loss: 3918.4441\n",
      "Epoch: 161/1000 | Batch 0004/0010 | Loss: 3856.4863\n",
      "Epoch: 161/1000 | Batch 0005/0010 | Loss: 3607.4824\n",
      "Epoch: 161/1000 | Batch 0006/0010 | Loss: 3471.2014\n",
      "Epoch: 161/1000 | Batch 0007/0010 | Loss: 3707.7332\n",
      "Epoch: 161/1000 | Batch 0008/0010 | Loss: 3852.1523\n",
      "Epoch: 161/1000 | Batch 0009/0010 | Loss: 4015.2805\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 162/1000 | Batch 0000/0010 | Loss: 3734.4546\n",
      "Epoch: 162/1000 | Batch 0001/0010 | Loss: 3575.7810\n",
      "Epoch: 162/1000 | Batch 0002/0010 | Loss: 3957.3057\n",
      "Epoch: 162/1000 | Batch 0003/0010 | Loss: 4012.5840\n",
      "Epoch: 162/1000 | Batch 0004/0010 | Loss: 3818.9575\n",
      "Epoch: 162/1000 | Batch 0005/0010 | Loss: 3657.2522\n",
      "Epoch: 162/1000 | Batch 0006/0010 | Loss: 3836.2747\n",
      "Epoch: 162/1000 | Batch 0007/0010 | Loss: 4033.4929\n",
      "Epoch: 162/1000 | Batch 0008/0010 | Loss: 3814.7495\n",
      "Epoch: 162/1000 | Batch 0009/0010 | Loss: 3677.1057\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 163/1000 | Batch 0000/0010 | Loss: 3938.9014\n",
      "Epoch: 163/1000 | Batch 0001/0010 | Loss: 3971.7212\n",
      "Epoch: 163/1000 | Batch 0002/0010 | Loss: 3717.1904\n",
      "Epoch: 163/1000 | Batch 0003/0010 | Loss: 3751.7556\n",
      "Epoch: 163/1000 | Batch 0004/0010 | Loss: 3900.1548\n",
      "Epoch: 163/1000 | Batch 0005/0010 | Loss: 3710.0293\n",
      "Epoch: 163/1000 | Batch 0006/0010 | Loss: 3681.9548\n",
      "Epoch: 163/1000 | Batch 0007/0010 | Loss: 3730.8440\n",
      "Epoch: 163/1000 | Batch 0008/0010 | Loss: 3775.4553\n",
      "Epoch: 163/1000 | Batch 0009/0010 | Loss: 3882.5640\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 164/1000 | Batch 0000/0010 | Loss: 4156.3560\n",
      "Epoch: 164/1000 | Batch 0001/0010 | Loss: 3614.4358\n",
      "Epoch: 164/1000 | Batch 0002/0010 | Loss: 3859.5444\n",
      "Epoch: 164/1000 | Batch 0003/0010 | Loss: 3626.7734\n",
      "Epoch: 164/1000 | Batch 0004/0010 | Loss: 3778.7014\n",
      "Epoch: 164/1000 | Batch 0005/0010 | Loss: 3919.7983\n",
      "Epoch: 164/1000 | Batch 0006/0010 | Loss: 3739.7061\n",
      "Epoch: 164/1000 | Batch 0007/0010 | Loss: 3726.7434\n",
      "Epoch: 164/1000 | Batch 0008/0010 | Loss: 3602.3147\n",
      "Epoch: 164/1000 | Batch 0009/0010 | Loss: 4087.6184\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 165/1000 | Batch 0000/0010 | Loss: 3916.9031\n",
      "Epoch: 165/1000 | Batch 0001/0010 | Loss: 3864.9004\n",
      "Epoch: 165/1000 | Batch 0002/0010 | Loss: 3899.5417\n",
      "Epoch: 165/1000 | Batch 0003/0010 | Loss: 3710.7583\n",
      "Epoch: 165/1000 | Batch 0004/0010 | Loss: 4123.5542\n",
      "Epoch: 165/1000 | Batch 0005/0010 | Loss: 3825.4443\n",
      "Epoch: 165/1000 | Batch 0006/0010 | Loss: 3942.8657\n",
      "Epoch: 165/1000 | Batch 0007/0010 | Loss: 3633.2495\n",
      "Epoch: 165/1000 | Batch 0008/0010 | Loss: 3546.8081\n",
      "Epoch: 165/1000 | Batch 0009/0010 | Loss: 3723.7957\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 166/1000 | Batch 0000/0010 | Loss: 3467.0513\n",
      "Epoch: 166/1000 | Batch 0001/0010 | Loss: 4014.3782\n",
      "Epoch: 166/1000 | Batch 0002/0010 | Loss: 3977.5078\n",
      "Epoch: 166/1000 | Batch 0003/0010 | Loss: 3800.4084\n",
      "Epoch: 166/1000 | Batch 0004/0010 | Loss: 3550.5989\n",
      "Epoch: 166/1000 | Batch 0005/0010 | Loss: 3529.4792\n",
      "Epoch: 166/1000 | Batch 0006/0010 | Loss: 3878.8689\n",
      "Epoch: 166/1000 | Batch 0007/0010 | Loss: 3922.7351\n",
      "Epoch: 166/1000 | Batch 0008/0010 | Loss: 3756.2585\n",
      "Epoch: 166/1000 | Batch 0009/0010 | Loss: 4186.0933\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 167/1000 | Batch 0000/0010 | Loss: 3934.3379\n",
      "Epoch: 167/1000 | Batch 0001/0010 | Loss: 3903.6228\n",
      "Epoch: 167/1000 | Batch 0002/0010 | Loss: 3829.7903\n",
      "Epoch: 167/1000 | Batch 0003/0010 | Loss: 3778.2959\n",
      "Epoch: 167/1000 | Batch 0004/0010 | Loss: 3554.0984\n",
      "Epoch: 167/1000 | Batch 0005/0010 | Loss: 3967.4929\n",
      "Epoch: 167/1000 | Batch 0006/0010 | Loss: 3732.6052\n",
      "Epoch: 167/1000 | Batch 0007/0010 | Loss: 3855.5647\n",
      "Epoch: 167/1000 | Batch 0008/0010 | Loss: 3687.9365\n",
      "Epoch: 167/1000 | Batch 0009/0010 | Loss: 4008.9917\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 168/1000 | Batch 0000/0010 | Loss: 3738.2771\n",
      "Epoch: 168/1000 | Batch 0001/0010 | Loss: 4138.0884\n",
      "Epoch: 168/1000 | Batch 0002/0010 | Loss: 3631.1790\n",
      "Epoch: 168/1000 | Batch 0003/0010 | Loss: 3691.8684\n",
      "Epoch: 168/1000 | Batch 0004/0010 | Loss: 3686.9199\n",
      "Epoch: 168/1000 | Batch 0005/0010 | Loss: 3730.5132\n",
      "Epoch: 168/1000 | Batch 0006/0010 | Loss: 3897.4229\n",
      "Epoch: 168/1000 | Batch 0007/0010 | Loss: 3870.3245\n",
      "Epoch: 168/1000 | Batch 0008/0010 | Loss: 3955.2690\n",
      "Epoch: 168/1000 | Batch 0009/0010 | Loss: 3854.6858\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 169/1000 | Batch 0000/0010 | Loss: 3485.8142\n",
      "Epoch: 169/1000 | Batch 0001/0010 | Loss: 4101.5518\n",
      "Epoch: 169/1000 | Batch 0002/0010 | Loss: 3685.5215\n",
      "Epoch: 169/1000 | Batch 0003/0010 | Loss: 3777.1025\n",
      "Epoch: 169/1000 | Batch 0004/0010 | Loss: 3543.4285\n",
      "Epoch: 169/1000 | Batch 0005/0010 | Loss: 3932.0625\n",
      "Epoch: 169/1000 | Batch 0006/0010 | Loss: 3865.8687\n",
      "Epoch: 169/1000 | Batch 0007/0010 | Loss: 3803.3518\n",
      "Epoch: 169/1000 | Batch 0008/0010 | Loss: 3929.2351\n",
      "Epoch: 169/1000 | Batch 0009/0010 | Loss: 3861.8860\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 170/1000 | Batch 0000/0010 | Loss: 4000.3049\n",
      "Epoch: 170/1000 | Batch 0001/0010 | Loss: 3638.0596\n",
      "Epoch: 170/1000 | Batch 0002/0010 | Loss: 3934.2375\n",
      "Epoch: 170/1000 | Batch 0003/0010 | Loss: 3591.4353\n",
      "Epoch: 170/1000 | Batch 0004/0010 | Loss: 3930.7444\n",
      "Epoch: 170/1000 | Batch 0005/0010 | Loss: 3732.9741\n",
      "Epoch: 170/1000 | Batch 0006/0010 | Loss: 3752.5642\n",
      "Epoch: 170/1000 | Batch 0007/0010 | Loss: 3854.1060\n",
      "Epoch: 170/1000 | Batch 0008/0010 | Loss: 3677.6799\n",
      "Epoch: 170/1000 | Batch 0009/0010 | Loss: 3978.5212\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 171/1000 | Batch 0000/0010 | Loss: 3953.2925\n",
      "Epoch: 171/1000 | Batch 0001/0010 | Loss: 3663.0588\n",
      "Epoch: 171/1000 | Batch 0002/0010 | Loss: 3735.9966\n",
      "Epoch: 171/1000 | Batch 0003/0010 | Loss: 3827.8433\n",
      "Epoch: 171/1000 | Batch 0004/0010 | Loss: 3465.5728\n",
      "Epoch: 171/1000 | Batch 0005/0010 | Loss: 3747.9255\n",
      "Epoch: 171/1000 | Batch 0006/0010 | Loss: 3878.4243\n",
      "Epoch: 171/1000 | Batch 0007/0010 | Loss: 4005.2705\n",
      "Epoch: 171/1000 | Batch 0008/0010 | Loss: 4255.3706\n",
      "Epoch: 171/1000 | Batch 0009/0010 | Loss: 3528.6904\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 172/1000 | Batch 0000/0010 | Loss: 3723.8254\n",
      "Epoch: 172/1000 | Batch 0001/0010 | Loss: 3592.4890\n",
      "Epoch: 172/1000 | Batch 0002/0010 | Loss: 3979.2053\n",
      "Epoch: 172/1000 | Batch 0003/0010 | Loss: 3650.8193\n",
      "Epoch: 172/1000 | Batch 0004/0010 | Loss: 3726.4500\n",
      "Epoch: 172/1000 | Batch 0005/0010 | Loss: 3914.7266\n",
      "Epoch: 172/1000 | Batch 0006/0010 | Loss: 3695.3472\n",
      "Epoch: 172/1000 | Batch 0007/0010 | Loss: 4054.8093\n",
      "Epoch: 172/1000 | Batch 0008/0010 | Loss: 3688.8694\n",
      "Epoch: 172/1000 | Batch 0009/0010 | Loss: 4040.1597\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 173/1000 | Batch 0000/0010 | Loss: 3507.1489\n",
      "Epoch: 173/1000 | Batch 0001/0010 | Loss: 3573.5042\n",
      "Epoch: 173/1000 | Batch 0002/0010 | Loss: 3871.9124\n",
      "Epoch: 173/1000 | Batch 0003/0010 | Loss: 3992.3198\n",
      "Epoch: 173/1000 | Batch 0004/0010 | Loss: 3639.5596\n",
      "Epoch: 173/1000 | Batch 0005/0010 | Loss: 4033.4893\n",
      "Epoch: 173/1000 | Batch 0006/0010 | Loss: 4007.8198\n",
      "Epoch: 173/1000 | Batch 0007/0010 | Loss: 3481.4116\n",
      "Epoch: 173/1000 | Batch 0008/0010 | Loss: 3864.0212\n",
      "Epoch: 173/1000 | Batch 0009/0010 | Loss: 4157.5352\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 174/1000 | Batch 0000/0010 | Loss: 3758.4595\n",
      "Epoch: 174/1000 | Batch 0001/0010 | Loss: 3737.1038\n",
      "Epoch: 174/1000 | Batch 0002/0010 | Loss: 3813.0532\n",
      "Epoch: 174/1000 | Batch 0003/0010 | Loss: 3926.9934\n",
      "Epoch: 174/1000 | Batch 0004/0010 | Loss: 4143.4932\n",
      "Epoch: 174/1000 | Batch 0005/0010 | Loss: 3680.3921\n",
      "Epoch: 174/1000 | Batch 0006/0010 | Loss: 3662.7458\n",
      "Epoch: 174/1000 | Batch 0007/0010 | Loss: 3948.4771\n",
      "Epoch: 174/1000 | Batch 0008/0010 | Loss: 3715.5178\n",
      "Epoch: 174/1000 | Batch 0009/0010 | Loss: 3650.0383\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 175/1000 | Batch 0000/0010 | Loss: 3990.0830\n",
      "Epoch: 175/1000 | Batch 0001/0010 | Loss: 3704.8367\n",
      "Epoch: 175/1000 | Batch 0002/0010 | Loss: 3811.5161\n",
      "Epoch: 175/1000 | Batch 0003/0010 | Loss: 3953.3550\n",
      "Epoch: 175/1000 | Batch 0004/0010 | Loss: 3982.6223\n",
      "Epoch: 175/1000 | Batch 0005/0010 | Loss: 3752.6826\n",
      "Epoch: 175/1000 | Batch 0006/0010 | Loss: 3567.3816\n",
      "Epoch: 175/1000 | Batch 0007/0010 | Loss: 3796.5396\n",
      "Epoch: 175/1000 | Batch 0008/0010 | Loss: 3506.7832\n",
      "Epoch: 175/1000 | Batch 0009/0010 | Loss: 3979.1743\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 176/1000 | Batch 0000/0010 | Loss: 3884.5698\n",
      "Epoch: 176/1000 | Batch 0001/0010 | Loss: 3735.3733\n",
      "Epoch: 176/1000 | Batch 0002/0010 | Loss: 3973.8833\n",
      "Epoch: 176/1000 | Batch 0003/0010 | Loss: 3308.8579\n",
      "Epoch: 176/1000 | Batch 0004/0010 | Loss: 3610.5950\n",
      "Epoch: 176/1000 | Batch 0005/0010 | Loss: 4006.1794\n",
      "Epoch: 176/1000 | Batch 0006/0010 | Loss: 3740.8401\n",
      "Epoch: 176/1000 | Batch 0007/0010 | Loss: 3718.4177\n",
      "Epoch: 176/1000 | Batch 0008/0010 | Loss: 4030.1338\n",
      "Epoch: 176/1000 | Batch 0009/0010 | Loss: 4024.2173\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 177/1000 | Batch 0000/0010 | Loss: 3780.9023\n",
      "Epoch: 177/1000 | Batch 0001/0010 | Loss: 3745.8276\n",
      "Epoch: 177/1000 | Batch 0002/0010 | Loss: 3705.4192\n",
      "Epoch: 177/1000 | Batch 0003/0010 | Loss: 3981.8582\n",
      "Epoch: 177/1000 | Batch 0004/0010 | Loss: 4014.9114\n",
      "Epoch: 177/1000 | Batch 0005/0010 | Loss: 3631.1768\n",
      "Epoch: 177/1000 | Batch 0006/0010 | Loss: 3889.5491\n",
      "Epoch: 177/1000 | Batch 0007/0010 | Loss: 3673.5459\n",
      "Epoch: 177/1000 | Batch 0008/0010 | Loss: 3659.2771\n",
      "Epoch: 177/1000 | Batch 0009/0010 | Loss: 3989.2273\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 178/1000 | Batch 0000/0010 | Loss: 3787.6328\n",
      "Epoch: 178/1000 | Batch 0001/0010 | Loss: 3830.9775\n",
      "Epoch: 178/1000 | Batch 0002/0010 | Loss: 3917.3079\n",
      "Epoch: 178/1000 | Batch 0003/0010 | Loss: 3446.9329\n",
      "Epoch: 178/1000 | Batch 0004/0010 | Loss: 3913.7690\n",
      "Epoch: 178/1000 | Batch 0005/0010 | Loss: 4014.0630\n",
      "Epoch: 178/1000 | Batch 0006/0010 | Loss: 3561.2539\n",
      "Epoch: 178/1000 | Batch 0007/0010 | Loss: 3968.3198\n",
      "Epoch: 178/1000 | Batch 0008/0010 | Loss: 3881.7898\n",
      "Epoch: 178/1000 | Batch 0009/0010 | Loss: 3814.0938\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 179/1000 | Batch 0000/0010 | Loss: 3582.6282\n",
      "Epoch: 179/1000 | Batch 0001/0010 | Loss: 3824.7280\n",
      "Epoch: 179/1000 | Batch 0002/0010 | Loss: 3803.3711\n",
      "Epoch: 179/1000 | Batch 0003/0010 | Loss: 3894.2563\n",
      "Epoch: 179/1000 | Batch 0004/0010 | Loss: 3810.3635\n",
      "Epoch: 179/1000 | Batch 0005/0010 | Loss: 3519.4580\n",
      "Epoch: 179/1000 | Batch 0006/0010 | Loss: 3873.0945\n",
      "Epoch: 179/1000 | Batch 0007/0010 | Loss: 4079.4243\n",
      "Epoch: 179/1000 | Batch 0008/0010 | Loss: 3907.4263\n",
      "Epoch: 179/1000 | Batch 0009/0010 | Loss: 3739.8486\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 180/1000 | Batch 0000/0010 | Loss: 3819.8445\n",
      "Epoch: 180/1000 | Batch 0001/0010 | Loss: 3710.1169\n",
      "Epoch: 180/1000 | Batch 0002/0010 | Loss: 3722.1067\n",
      "Epoch: 180/1000 | Batch 0003/0010 | Loss: 3891.4119\n",
      "Epoch: 180/1000 | Batch 0004/0010 | Loss: 3393.2312\n",
      "Epoch: 180/1000 | Batch 0005/0010 | Loss: 3954.5122\n",
      "Epoch: 180/1000 | Batch 0006/0010 | Loss: 3759.8145\n",
      "Epoch: 180/1000 | Batch 0007/0010 | Loss: 4059.1030\n",
      "Epoch: 180/1000 | Batch 0008/0010 | Loss: 4046.2786\n",
      "Epoch: 180/1000 | Batch 0009/0010 | Loss: 3583.6912\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 181/1000 | Batch 0000/0010 | Loss: 3847.7864\n",
      "Epoch: 181/1000 | Batch 0001/0010 | Loss: 3617.0205\n",
      "Epoch: 181/1000 | Batch 0002/0010 | Loss: 3605.5928\n",
      "Epoch: 181/1000 | Batch 0003/0010 | Loss: 3948.5083\n",
      "Epoch: 181/1000 | Batch 0004/0010 | Loss: 3743.1919\n",
      "Epoch: 181/1000 | Batch 0005/0010 | Loss: 4104.6567\n",
      "Epoch: 181/1000 | Batch 0006/0010 | Loss: 3652.6196\n",
      "Epoch: 181/1000 | Batch 0007/0010 | Loss: 4057.5708\n",
      "Epoch: 181/1000 | Batch 0008/0010 | Loss: 3825.7788\n",
      "Epoch: 181/1000 | Batch 0009/0010 | Loss: 3695.3337\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 182/1000 | Batch 0000/0010 | Loss: 3666.4976\n",
      "Epoch: 182/1000 | Batch 0001/0010 | Loss: 3652.8689\n",
      "Epoch: 182/1000 | Batch 0002/0010 | Loss: 3825.5127\n",
      "Epoch: 182/1000 | Batch 0003/0010 | Loss: 4022.1628\n",
      "Epoch: 182/1000 | Batch 0004/0010 | Loss: 3757.3843\n",
      "Epoch: 182/1000 | Batch 0005/0010 | Loss: 3626.3416\n",
      "Epoch: 182/1000 | Batch 0006/0010 | Loss: 3609.7131\n",
      "Epoch: 182/1000 | Batch 0007/0010 | Loss: 3603.4502\n",
      "Epoch: 182/1000 | Batch 0008/0010 | Loss: 4070.9951\n",
      "Epoch: 182/1000 | Batch 0009/0010 | Loss: 4216.3110\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 183/1000 | Batch 0000/0010 | Loss: 3757.7468\n",
      "Epoch: 183/1000 | Batch 0001/0010 | Loss: 3983.4438\n",
      "Epoch: 183/1000 | Batch 0002/0010 | Loss: 3707.4785\n",
      "Epoch: 183/1000 | Batch 0003/0010 | Loss: 3585.3774\n",
      "Epoch: 183/1000 | Batch 0004/0010 | Loss: 3807.7144\n",
      "Epoch: 183/1000 | Batch 0005/0010 | Loss: 4001.0872\n",
      "Epoch: 183/1000 | Batch 0006/0010 | Loss: 3745.8730\n",
      "Epoch: 183/1000 | Batch 0007/0010 | Loss: 3749.0854\n",
      "Epoch: 183/1000 | Batch 0008/0010 | Loss: 3799.2388\n",
      "Epoch: 183/1000 | Batch 0009/0010 | Loss: 3907.9077\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 184/1000 | Batch 0000/0010 | Loss: 3784.4250\n",
      "Epoch: 184/1000 | Batch 0001/0010 | Loss: 3526.6902\n",
      "Epoch: 184/1000 | Batch 0002/0010 | Loss: 3777.2205\n",
      "Epoch: 184/1000 | Batch 0003/0010 | Loss: 3881.0803\n",
      "Epoch: 184/1000 | Batch 0004/0010 | Loss: 3959.8550\n",
      "Epoch: 184/1000 | Batch 0005/0010 | Loss: 4153.0854\n",
      "Epoch: 184/1000 | Batch 0006/0010 | Loss: 3976.6482\n",
      "Epoch: 184/1000 | Batch 0007/0010 | Loss: 3546.3047\n",
      "Epoch: 184/1000 | Batch 0008/0010 | Loss: 3913.8267\n",
      "Epoch: 184/1000 | Batch 0009/0010 | Loss: 3706.3730\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 185/1000 | Batch 0000/0010 | Loss: 3840.2253\n",
      "Epoch: 185/1000 | Batch 0001/0010 | Loss: 3706.7227\n",
      "Epoch: 185/1000 | Batch 0002/0010 | Loss: 3500.7742\n",
      "Epoch: 185/1000 | Batch 0003/0010 | Loss: 3917.7593\n",
      "Epoch: 185/1000 | Batch 0004/0010 | Loss: 3961.6709\n",
      "Epoch: 185/1000 | Batch 0005/0010 | Loss: 3577.3850\n",
      "Epoch: 185/1000 | Batch 0006/0010 | Loss: 4110.0659\n",
      "Epoch: 185/1000 | Batch 0007/0010 | Loss: 3913.0806\n",
      "Epoch: 185/1000 | Batch 0008/0010 | Loss: 3831.3938\n",
      "Epoch: 185/1000 | Batch 0009/0010 | Loss: 3704.6074\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 186/1000 | Batch 0000/0010 | Loss: 3805.7410\n",
      "Epoch: 186/1000 | Batch 0001/0010 | Loss: 3503.0669\n",
      "Epoch: 186/1000 | Batch 0002/0010 | Loss: 3863.3423\n",
      "Epoch: 186/1000 | Batch 0003/0010 | Loss: 3555.7339\n",
      "Epoch: 186/1000 | Batch 0004/0010 | Loss: 3738.2014\n",
      "Epoch: 186/1000 | Batch 0005/0010 | Loss: 4033.0469\n",
      "Epoch: 186/1000 | Batch 0006/0010 | Loss: 4055.3442\n",
      "Epoch: 186/1000 | Batch 0007/0010 | Loss: 3924.7957\n",
      "Epoch: 186/1000 | Batch 0008/0010 | Loss: 3584.9973\n",
      "Epoch: 186/1000 | Batch 0009/0010 | Loss: 3983.8047\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 187/1000 | Batch 0000/0010 | Loss: 3643.8926\n",
      "Epoch: 187/1000 | Batch 0001/0010 | Loss: 3897.5769\n",
      "Epoch: 187/1000 | Batch 0002/0010 | Loss: 3895.3254\n",
      "Epoch: 187/1000 | Batch 0003/0010 | Loss: 3897.0652\n",
      "Epoch: 187/1000 | Batch 0004/0010 | Loss: 3641.8618\n",
      "Epoch: 187/1000 | Batch 0005/0010 | Loss: 3772.5537\n",
      "Epoch: 187/1000 | Batch 0006/0010 | Loss: 3924.1465\n",
      "Epoch: 187/1000 | Batch 0007/0010 | Loss: 3728.5691\n",
      "Epoch: 187/1000 | Batch 0008/0010 | Loss: 3725.2839\n",
      "Epoch: 187/1000 | Batch 0009/0010 | Loss: 3911.3508\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 188/1000 | Batch 0000/0010 | Loss: 3744.1663\n",
      "Epoch: 188/1000 | Batch 0001/0010 | Loss: 3956.4849\n",
      "Epoch: 188/1000 | Batch 0002/0010 | Loss: 3889.7896\n",
      "Epoch: 188/1000 | Batch 0003/0010 | Loss: 3636.9465\n",
      "Epoch: 188/1000 | Batch 0004/0010 | Loss: 3907.3628\n",
      "Epoch: 188/1000 | Batch 0005/0010 | Loss: 3532.8220\n",
      "Epoch: 188/1000 | Batch 0006/0010 | Loss: 3937.0620\n",
      "Epoch: 188/1000 | Batch 0007/0010 | Loss: 3628.4954\n",
      "Epoch: 188/1000 | Batch 0008/0010 | Loss: 3713.5728\n",
      "Epoch: 188/1000 | Batch 0009/0010 | Loss: 4078.7766\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 189/1000 | Batch 0000/0010 | Loss: 3753.8472\n",
      "Epoch: 189/1000 | Batch 0001/0010 | Loss: 3970.9336\n",
      "Epoch: 189/1000 | Batch 0002/0010 | Loss: 3797.7517\n",
      "Epoch: 189/1000 | Batch 0003/0010 | Loss: 4010.6262\n",
      "Epoch: 189/1000 | Batch 0004/0010 | Loss: 3868.1448\n",
      "Epoch: 189/1000 | Batch 0005/0010 | Loss: 3741.1187\n",
      "Epoch: 189/1000 | Batch 0006/0010 | Loss: 3637.6807\n",
      "Epoch: 189/1000 | Batch 0007/0010 | Loss: 3712.7856\n",
      "Epoch: 189/1000 | Batch 0008/0010 | Loss: 3754.7854\n",
      "Epoch: 189/1000 | Batch 0009/0010 | Loss: 3794.0425\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 190/1000 | Batch 0000/0010 | Loss: 3717.5176\n",
      "Epoch: 190/1000 | Batch 0001/0010 | Loss: 3924.7559\n",
      "Epoch: 190/1000 | Batch 0002/0010 | Loss: 3822.1023\n",
      "Epoch: 190/1000 | Batch 0003/0010 | Loss: 3496.1602\n",
      "Epoch: 190/1000 | Batch 0004/0010 | Loss: 3999.9963\n",
      "Epoch: 190/1000 | Batch 0005/0010 | Loss: 3825.6521\n",
      "Epoch: 190/1000 | Batch 0006/0010 | Loss: 3980.1357\n",
      "Epoch: 190/1000 | Batch 0007/0010 | Loss: 3784.3835\n",
      "Epoch: 190/1000 | Batch 0008/0010 | Loss: 3623.0718\n",
      "Epoch: 190/1000 | Batch 0009/0010 | Loss: 3803.5107\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 191/1000 | Batch 0000/0010 | Loss: 3794.5889\n",
      "Epoch: 191/1000 | Batch 0001/0010 | Loss: 3829.6399\n",
      "Epoch: 191/1000 | Batch 0002/0010 | Loss: 3646.8862\n",
      "Epoch: 191/1000 | Batch 0003/0010 | Loss: 3846.6338\n",
      "Epoch: 191/1000 | Batch 0004/0010 | Loss: 3802.6763\n",
      "Epoch: 191/1000 | Batch 0005/0010 | Loss: 3764.9316\n",
      "Epoch: 191/1000 | Batch 0006/0010 | Loss: 3803.7419\n",
      "Epoch: 191/1000 | Batch 0007/0010 | Loss: 3735.8904\n",
      "Epoch: 191/1000 | Batch 0008/0010 | Loss: 3988.1094\n",
      "Epoch: 191/1000 | Batch 0009/0010 | Loss: 3779.2278\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 192/1000 | Batch 0000/0010 | Loss: 3838.5134\n",
      "Epoch: 192/1000 | Batch 0001/0010 | Loss: 3821.3870\n",
      "Epoch: 192/1000 | Batch 0002/0010 | Loss: 3631.0430\n",
      "Epoch: 192/1000 | Batch 0003/0010 | Loss: 4046.4395\n",
      "Epoch: 192/1000 | Batch 0004/0010 | Loss: 3971.3801\n",
      "Epoch: 192/1000 | Batch 0005/0010 | Loss: 3686.4800\n",
      "Epoch: 192/1000 | Batch 0006/0010 | Loss: 3711.8359\n",
      "Epoch: 192/1000 | Batch 0007/0010 | Loss: 3766.1389\n",
      "Epoch: 192/1000 | Batch 0008/0010 | Loss: 3724.9033\n",
      "Epoch: 192/1000 | Batch 0009/0010 | Loss: 3759.6750\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 193/1000 | Batch 0000/0010 | Loss: 3723.5334\n",
      "Epoch: 193/1000 | Batch 0001/0010 | Loss: 3860.8359\n",
      "Epoch: 193/1000 | Batch 0002/0010 | Loss: 3611.3745\n",
      "Epoch: 193/1000 | Batch 0003/0010 | Loss: 4135.4434\n",
      "Epoch: 193/1000 | Batch 0004/0010 | Loss: 3895.7378\n",
      "Epoch: 193/1000 | Batch 0005/0010 | Loss: 3688.1743\n",
      "Epoch: 193/1000 | Batch 0006/0010 | Loss: 3576.2473\n",
      "Epoch: 193/1000 | Batch 0007/0010 | Loss: 3903.9402\n",
      "Epoch: 193/1000 | Batch 0008/0010 | Loss: 3832.4922\n",
      "Epoch: 193/1000 | Batch 0009/0010 | Loss: 3777.9473\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 194/1000 | Batch 0000/0010 | Loss: 3565.3655\n",
      "Epoch: 194/1000 | Batch 0001/0010 | Loss: 3990.5828\n",
      "Epoch: 194/1000 | Batch 0002/0010 | Loss: 3812.9341\n",
      "Epoch: 194/1000 | Batch 0003/0010 | Loss: 3785.6646\n",
      "Epoch: 194/1000 | Batch 0004/0010 | Loss: 3573.7236\n",
      "Epoch: 194/1000 | Batch 0005/0010 | Loss: 3964.7004\n",
      "Epoch: 194/1000 | Batch 0006/0010 | Loss: 3621.6152\n",
      "Epoch: 194/1000 | Batch 0007/0010 | Loss: 3845.0833\n",
      "Epoch: 194/1000 | Batch 0008/0010 | Loss: 4123.2446\n",
      "Epoch: 194/1000 | Batch 0009/0010 | Loss: 3713.1133\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 195/1000 | Batch 0000/0010 | Loss: 3941.1851\n",
      "Epoch: 195/1000 | Batch 0001/0010 | Loss: 3718.3706\n",
      "Epoch: 195/1000 | Batch 0002/0010 | Loss: 3887.7053\n",
      "Epoch: 195/1000 | Batch 0003/0010 | Loss: 3877.1899\n",
      "Epoch: 195/1000 | Batch 0004/0010 | Loss: 3570.7305\n",
      "Epoch: 195/1000 | Batch 0005/0010 | Loss: 3771.2815\n",
      "Epoch: 195/1000 | Batch 0006/0010 | Loss: 3852.7239\n",
      "Epoch: 195/1000 | Batch 0007/0010 | Loss: 3649.1492\n",
      "Epoch: 195/1000 | Batch 0008/0010 | Loss: 3943.8467\n",
      "Epoch: 195/1000 | Batch 0009/0010 | Loss: 3775.0720\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 196/1000 | Batch 0000/0010 | Loss: 3606.1729\n",
      "Epoch: 196/1000 | Batch 0001/0010 | Loss: 3705.3931\n",
      "Epoch: 196/1000 | Batch 0002/0010 | Loss: 3961.0938\n",
      "Epoch: 196/1000 | Batch 0003/0010 | Loss: 3863.7024\n",
      "Epoch: 196/1000 | Batch 0004/0010 | Loss: 3790.6409\n",
      "Epoch: 196/1000 | Batch 0005/0010 | Loss: 3686.3743\n",
      "Epoch: 196/1000 | Batch 0006/0010 | Loss: 3844.7788\n",
      "Epoch: 196/1000 | Batch 0007/0010 | Loss: 3677.1089\n",
      "Epoch: 196/1000 | Batch 0008/0010 | Loss: 3757.7280\n",
      "Epoch: 196/1000 | Batch 0009/0010 | Loss: 4096.7354\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 197/1000 | Batch 0000/0010 | Loss: 3651.6895\n",
      "Epoch: 197/1000 | Batch 0001/0010 | Loss: 3612.8291\n",
      "Epoch: 197/1000 | Batch 0002/0010 | Loss: 3512.2771\n",
      "Epoch: 197/1000 | Batch 0003/0010 | Loss: 4036.9719\n",
      "Epoch: 197/1000 | Batch 0004/0010 | Loss: 3768.0845\n",
      "Epoch: 197/1000 | Batch 0005/0010 | Loss: 3796.4482\n",
      "Epoch: 197/1000 | Batch 0006/0010 | Loss: 3935.3616\n",
      "Epoch: 197/1000 | Batch 0007/0010 | Loss: 4099.4380\n",
      "Epoch: 197/1000 | Batch 0008/0010 | Loss: 3690.7971\n",
      "Epoch: 197/1000 | Batch 0009/0010 | Loss: 3906.5105\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 198/1000 | Batch 0000/0010 | Loss: 3981.4390\n",
      "Epoch: 198/1000 | Batch 0001/0010 | Loss: 3840.4402\n",
      "Epoch: 198/1000 | Batch 0002/0010 | Loss: 3738.5996\n",
      "Epoch: 198/1000 | Batch 0003/0010 | Loss: 3559.9839\n",
      "Epoch: 198/1000 | Batch 0004/0010 | Loss: 4020.3357\n",
      "Epoch: 198/1000 | Batch 0005/0010 | Loss: 3855.5225\n",
      "Epoch: 198/1000 | Batch 0006/0010 | Loss: 3677.2661\n",
      "Epoch: 198/1000 | Batch 0007/0010 | Loss: 3783.7168\n",
      "Epoch: 198/1000 | Batch 0008/0010 | Loss: 3850.9231\n",
      "Epoch: 198/1000 | Batch 0009/0010 | Loss: 3796.6243\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 199/1000 | Batch 0000/0010 | Loss: 3805.6301\n",
      "Epoch: 199/1000 | Batch 0001/0010 | Loss: 3625.4609\n",
      "Epoch: 199/1000 | Batch 0002/0010 | Loss: 3839.6069\n",
      "Epoch: 199/1000 | Batch 0003/0010 | Loss: 3800.5088\n",
      "Epoch: 199/1000 | Batch 0004/0010 | Loss: 3818.8611\n",
      "Epoch: 199/1000 | Batch 0005/0010 | Loss: 3879.7419\n",
      "Epoch: 199/1000 | Batch 0006/0010 | Loss: 3734.4814\n",
      "Epoch: 199/1000 | Batch 0007/0010 | Loss: 3720.4915\n",
      "Epoch: 199/1000 | Batch 0008/0010 | Loss: 3958.3567\n",
      "Epoch: 199/1000 | Batch 0009/0010 | Loss: 3835.1011\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 200/1000 | Batch 0000/0010 | Loss: 3856.3984\n",
      "Epoch: 200/1000 | Batch 0001/0010 | Loss: 4023.3315\n",
      "Epoch: 200/1000 | Batch 0002/0010 | Loss: 3716.8813\n",
      "Epoch: 200/1000 | Batch 0003/0010 | Loss: 3599.6409\n",
      "Epoch: 200/1000 | Batch 0004/0010 | Loss: 3806.6072\n",
      "Epoch: 200/1000 | Batch 0005/0010 | Loss: 3927.0015\n",
      "Epoch: 200/1000 | Batch 0006/0010 | Loss: 3731.1121\n",
      "Epoch: 200/1000 | Batch 0007/0010 | Loss: 3730.0046\n",
      "Epoch: 200/1000 | Batch 0008/0010 | Loss: 3924.1807\n",
      "Epoch: 200/1000 | Batch 0009/0010 | Loss: 3686.1584\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 201/1000 | Batch 0000/0010 | Loss: 3888.3745\n",
      "Epoch: 201/1000 | Batch 0001/0010 | Loss: 3701.8027\n",
      "Epoch: 201/1000 | Batch 0002/0010 | Loss: 4009.0090\n",
      "Epoch: 201/1000 | Batch 0003/0010 | Loss: 3842.2729\n",
      "Epoch: 201/1000 | Batch 0004/0010 | Loss: 3704.2717\n",
      "Epoch: 201/1000 | Batch 0005/0010 | Loss: 3858.2349\n",
      "Epoch: 201/1000 | Batch 0006/0010 | Loss: 3851.5405\n",
      "Epoch: 201/1000 | Batch 0007/0010 | Loss: 3835.8706\n",
      "Epoch: 201/1000 | Batch 0008/0010 | Loss: 3429.9109\n",
      "Epoch: 201/1000 | Batch 0009/0010 | Loss: 3853.7786\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 202/1000 | Batch 0000/0010 | Loss: 3876.0430\n",
      "Epoch: 202/1000 | Batch 0001/0010 | Loss: 3632.3403\n",
      "Epoch: 202/1000 | Batch 0002/0010 | Loss: 3819.1538\n",
      "Epoch: 202/1000 | Batch 0003/0010 | Loss: 3680.3911\n",
      "Epoch: 202/1000 | Batch 0004/0010 | Loss: 3725.9746\n",
      "Epoch: 202/1000 | Batch 0005/0010 | Loss: 3813.6104\n",
      "Epoch: 202/1000 | Batch 0006/0010 | Loss: 3801.6045\n",
      "Epoch: 202/1000 | Batch 0007/0010 | Loss: 3920.7822\n",
      "Epoch: 202/1000 | Batch 0008/0010 | Loss: 3884.0376\n",
      "Epoch: 202/1000 | Batch 0009/0010 | Loss: 3898.5720\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 203/1000 | Batch 0000/0010 | Loss: 3935.7026\n",
      "Epoch: 203/1000 | Batch 0001/0010 | Loss: 3702.3760\n",
      "Epoch: 203/1000 | Batch 0002/0010 | Loss: 3906.2798\n",
      "Epoch: 203/1000 | Batch 0003/0010 | Loss: 3644.2844\n",
      "Epoch: 203/1000 | Batch 0004/0010 | Loss: 3456.2612\n",
      "Epoch: 203/1000 | Batch 0005/0010 | Loss: 3979.1697\n",
      "Epoch: 203/1000 | Batch 0006/0010 | Loss: 3692.8105\n",
      "Epoch: 203/1000 | Batch 0007/0010 | Loss: 4133.8472\n",
      "Epoch: 203/1000 | Batch 0008/0010 | Loss: 3941.2812\n",
      "Epoch: 203/1000 | Batch 0009/0010 | Loss: 3672.3936\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 204/1000 | Batch 0000/0010 | Loss: 3890.5710\n",
      "Epoch: 204/1000 | Batch 0001/0010 | Loss: 4016.1252\n",
      "Epoch: 204/1000 | Batch 0002/0010 | Loss: 4124.7817\n",
      "Epoch: 204/1000 | Batch 0003/0010 | Loss: 3645.5288\n",
      "Epoch: 204/1000 | Batch 0004/0010 | Loss: 3623.9397\n",
      "Epoch: 204/1000 | Batch 0005/0010 | Loss: 3676.5488\n",
      "Epoch: 204/1000 | Batch 0006/0010 | Loss: 3737.3835\n",
      "Epoch: 204/1000 | Batch 0007/0010 | Loss: 3693.9519\n",
      "Epoch: 204/1000 | Batch 0008/0010 | Loss: 3970.4512\n",
      "Epoch: 204/1000 | Batch 0009/0010 | Loss: 3527.2773\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 205/1000 | Batch 0000/0010 | Loss: 3802.3428\n",
      "Epoch: 205/1000 | Batch 0001/0010 | Loss: 3892.2983\n",
      "Epoch: 205/1000 | Batch 0002/0010 | Loss: 3839.8723\n",
      "Epoch: 205/1000 | Batch 0003/0010 | Loss: 3682.5850\n",
      "Epoch: 205/1000 | Batch 0004/0010 | Loss: 3704.3477\n",
      "Epoch: 205/1000 | Batch 0005/0010 | Loss: 3858.0681\n",
      "Epoch: 205/1000 | Batch 0006/0010 | Loss: 3808.2905\n",
      "Epoch: 205/1000 | Batch 0007/0010 | Loss: 3812.0103\n",
      "Epoch: 205/1000 | Batch 0008/0010 | Loss: 3798.2749\n",
      "Epoch: 205/1000 | Batch 0009/0010 | Loss: 3802.0710\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 206/1000 | Batch 0000/0010 | Loss: 3347.2261\n",
      "Epoch: 206/1000 | Batch 0001/0010 | Loss: 3993.0452\n",
      "Epoch: 206/1000 | Batch 0002/0010 | Loss: 3616.4670\n",
      "Epoch: 206/1000 | Batch 0003/0010 | Loss: 4374.0322\n",
      "Epoch: 206/1000 | Batch 0004/0010 | Loss: 3856.2585\n",
      "Epoch: 206/1000 | Batch 0005/0010 | Loss: 3532.6426\n",
      "Epoch: 206/1000 | Batch 0006/0010 | Loss: 3869.9492\n",
      "Epoch: 206/1000 | Batch 0007/0010 | Loss: 3908.5691\n",
      "Epoch: 206/1000 | Batch 0008/0010 | Loss: 3778.1223\n",
      "Epoch: 206/1000 | Batch 0009/0010 | Loss: 3786.6416\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 207/1000 | Batch 0000/0010 | Loss: 3688.1665\n",
      "Epoch: 207/1000 | Batch 0001/0010 | Loss: 3635.2107\n",
      "Epoch: 207/1000 | Batch 0002/0010 | Loss: 3605.2371\n",
      "Epoch: 207/1000 | Batch 0003/0010 | Loss: 3672.5742\n",
      "Epoch: 207/1000 | Batch 0004/0010 | Loss: 4093.8208\n",
      "Epoch: 207/1000 | Batch 0005/0010 | Loss: 4004.2913\n",
      "Epoch: 207/1000 | Batch 0006/0010 | Loss: 3858.8213\n",
      "Epoch: 207/1000 | Batch 0007/0010 | Loss: 3586.9863\n",
      "Epoch: 207/1000 | Batch 0008/0010 | Loss: 4134.8208\n",
      "Epoch: 207/1000 | Batch 0009/0010 | Loss: 3821.1016\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 208/1000 | Batch 0000/0010 | Loss: 3876.5383\n",
      "Epoch: 208/1000 | Batch 0001/0010 | Loss: 3750.3433\n",
      "Epoch: 208/1000 | Batch 0002/0010 | Loss: 3894.4111\n",
      "Epoch: 208/1000 | Batch 0003/0010 | Loss: 3951.8052\n",
      "Epoch: 208/1000 | Batch 0004/0010 | Loss: 3663.3467\n",
      "Epoch: 208/1000 | Batch 0005/0010 | Loss: 3877.5691\n",
      "Epoch: 208/1000 | Batch 0006/0010 | Loss: 3760.9871\n",
      "Epoch: 208/1000 | Batch 0007/0010 | Loss: 3631.4805\n",
      "Epoch: 208/1000 | Batch 0008/0010 | Loss: 3733.6091\n",
      "Epoch: 208/1000 | Batch 0009/0010 | Loss: 3922.6204\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 209/1000 | Batch 0000/0010 | Loss: 3709.3257\n",
      "Epoch: 209/1000 | Batch 0001/0010 | Loss: 4094.3950\n",
      "Epoch: 209/1000 | Batch 0002/0010 | Loss: 3680.7854\n",
      "Epoch: 209/1000 | Batch 0003/0010 | Loss: 4022.2920\n",
      "Epoch: 209/1000 | Batch 0004/0010 | Loss: 3638.7607\n",
      "Epoch: 209/1000 | Batch 0005/0010 | Loss: 3983.3076\n",
      "Epoch: 209/1000 | Batch 0006/0010 | Loss: 3439.8982\n",
      "Epoch: 209/1000 | Batch 0007/0010 | Loss: 3626.1284\n",
      "Epoch: 209/1000 | Batch 0008/0010 | Loss: 3792.1006\n",
      "Epoch: 209/1000 | Batch 0009/0010 | Loss: 3911.9729\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 210/1000 | Batch 0000/0010 | Loss: 3725.7258\n",
      "Epoch: 210/1000 | Batch 0001/0010 | Loss: 3617.1233\n",
      "Epoch: 210/1000 | Batch 0002/0010 | Loss: 3696.9099\n",
      "Epoch: 210/1000 | Batch 0003/0010 | Loss: 3506.7573\n",
      "Epoch: 210/1000 | Batch 0004/0010 | Loss: 3809.1899\n",
      "Epoch: 210/1000 | Batch 0005/0010 | Loss: 3897.7727\n",
      "Epoch: 210/1000 | Batch 0006/0010 | Loss: 4071.7927\n",
      "Epoch: 210/1000 | Batch 0007/0010 | Loss: 3921.5156\n",
      "Epoch: 210/1000 | Batch 0008/0010 | Loss: 3777.8538\n",
      "Epoch: 210/1000 | Batch 0009/0010 | Loss: 4012.6667\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 211/1000 | Batch 0000/0010 | Loss: 3874.9199\n",
      "Epoch: 211/1000 | Batch 0001/0010 | Loss: 3587.3735\n",
      "Epoch: 211/1000 | Batch 0002/0010 | Loss: 3957.9976\n",
      "Epoch: 211/1000 | Batch 0003/0010 | Loss: 3419.4536\n",
      "Epoch: 211/1000 | Batch 0004/0010 | Loss: 3780.2881\n",
      "Epoch: 211/1000 | Batch 0005/0010 | Loss: 3863.6340\n",
      "Epoch: 211/1000 | Batch 0006/0010 | Loss: 3905.3804\n",
      "Epoch: 211/1000 | Batch 0007/0010 | Loss: 3944.9055\n",
      "Epoch: 211/1000 | Batch 0008/0010 | Loss: 3910.3096\n",
      "Epoch: 211/1000 | Batch 0009/0010 | Loss: 3801.8521\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 212/1000 | Batch 0000/0010 | Loss: 3906.1802\n",
      "Epoch: 212/1000 | Batch 0001/0010 | Loss: 3492.7188\n",
      "Epoch: 212/1000 | Batch 0002/0010 | Loss: 3942.1594\n",
      "Epoch: 212/1000 | Batch 0003/0010 | Loss: 3657.9207\n",
      "Epoch: 212/1000 | Batch 0004/0010 | Loss: 3847.3767\n",
      "Epoch: 212/1000 | Batch 0005/0010 | Loss: 3782.3687\n",
      "Epoch: 212/1000 | Batch 0006/0010 | Loss: 3649.6206\n",
      "Epoch: 212/1000 | Batch 0007/0010 | Loss: 3751.4312\n",
      "Epoch: 212/1000 | Batch 0008/0010 | Loss: 4007.0791\n",
      "Epoch: 212/1000 | Batch 0009/0010 | Loss: 3907.4739\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 213/1000 | Batch 0000/0010 | Loss: 3984.4634\n",
      "Epoch: 213/1000 | Batch 0001/0010 | Loss: 3530.2878\n",
      "Epoch: 213/1000 | Batch 0002/0010 | Loss: 4162.6665\n",
      "Epoch: 213/1000 | Batch 0003/0010 | Loss: 3857.7612\n",
      "Epoch: 213/1000 | Batch 0004/0010 | Loss: 3769.3879\n",
      "Epoch: 213/1000 | Batch 0005/0010 | Loss: 3760.5537\n",
      "Epoch: 213/1000 | Batch 0006/0010 | Loss: 3757.1951\n",
      "Epoch: 213/1000 | Batch 0007/0010 | Loss: 3886.9709\n",
      "Epoch: 213/1000 | Batch 0008/0010 | Loss: 3750.7214\n",
      "Epoch: 213/1000 | Batch 0009/0010 | Loss: 3441.1689\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 214/1000 | Batch 0000/0010 | Loss: 3692.6377\n",
      "Epoch: 214/1000 | Batch 0001/0010 | Loss: 4025.6763\n",
      "Epoch: 214/1000 | Batch 0002/0010 | Loss: 3796.9934\n",
      "Epoch: 214/1000 | Batch 0003/0010 | Loss: 3664.4292\n",
      "Epoch: 214/1000 | Batch 0004/0010 | Loss: 4064.7112\n",
      "Epoch: 214/1000 | Batch 0005/0010 | Loss: 3787.4094\n",
      "Epoch: 214/1000 | Batch 0006/0010 | Loss: 3997.2468\n",
      "Epoch: 214/1000 | Batch 0007/0010 | Loss: 3609.3191\n",
      "Epoch: 214/1000 | Batch 0008/0010 | Loss: 3726.4326\n",
      "Epoch: 214/1000 | Batch 0009/0010 | Loss: 3556.9243\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 215/1000 | Batch 0000/0010 | Loss: 3966.2161\n",
      "Epoch: 215/1000 | Batch 0001/0010 | Loss: 3880.1855\n",
      "Epoch: 215/1000 | Batch 0002/0010 | Loss: 3739.5330\n",
      "Epoch: 215/1000 | Batch 0003/0010 | Loss: 3754.0684\n",
      "Epoch: 215/1000 | Batch 0004/0010 | Loss: 3613.2693\n",
      "Epoch: 215/1000 | Batch 0005/0010 | Loss: 3740.5647\n",
      "Epoch: 215/1000 | Batch 0006/0010 | Loss: 3642.0581\n",
      "Epoch: 215/1000 | Batch 0007/0010 | Loss: 3791.6848\n",
      "Epoch: 215/1000 | Batch 0008/0010 | Loss: 3916.4294\n",
      "Epoch: 215/1000 | Batch 0009/0010 | Loss: 4002.8723\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 216/1000 | Batch 0000/0010 | Loss: 3638.4756\n",
      "Epoch: 216/1000 | Batch 0001/0010 | Loss: 3784.8518\n",
      "Epoch: 216/1000 | Batch 0002/0010 | Loss: 3660.8782\n",
      "Epoch: 216/1000 | Batch 0003/0010 | Loss: 3841.3616\n",
      "Epoch: 216/1000 | Batch 0004/0010 | Loss: 4317.1987\n",
      "Epoch: 216/1000 | Batch 0005/0010 | Loss: 3815.0469\n",
      "Epoch: 216/1000 | Batch 0006/0010 | Loss: 3750.3894\n",
      "Epoch: 216/1000 | Batch 0007/0010 | Loss: 3759.9829\n",
      "Epoch: 216/1000 | Batch 0008/0010 | Loss: 3520.9746\n",
      "Epoch: 216/1000 | Batch 0009/0010 | Loss: 3846.7993\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 217/1000 | Batch 0000/0010 | Loss: 3738.0317\n",
      "Epoch: 217/1000 | Batch 0001/0010 | Loss: 3628.9409\n",
      "Epoch: 217/1000 | Batch 0002/0010 | Loss: 3871.7861\n",
      "Epoch: 217/1000 | Batch 0003/0010 | Loss: 3804.3274\n",
      "Epoch: 217/1000 | Batch 0004/0010 | Loss: 3779.5212\n",
      "Epoch: 217/1000 | Batch 0005/0010 | Loss: 3842.7065\n",
      "Epoch: 217/1000 | Batch 0006/0010 | Loss: 3816.9487\n",
      "Epoch: 217/1000 | Batch 0007/0010 | Loss: 3807.6331\n",
      "Epoch: 217/1000 | Batch 0008/0010 | Loss: 3649.3333\n",
      "Epoch: 217/1000 | Batch 0009/0010 | Loss: 4112.1807\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 218/1000 | Batch 0000/0010 | Loss: 3555.0107\n",
      "Epoch: 218/1000 | Batch 0001/0010 | Loss: 4061.3123\n",
      "Epoch: 218/1000 | Batch 0002/0010 | Loss: 3541.5190\n",
      "Epoch: 218/1000 | Batch 0003/0010 | Loss: 3712.4766\n",
      "Epoch: 218/1000 | Batch 0004/0010 | Loss: 3754.2617\n",
      "Epoch: 218/1000 | Batch 0005/0010 | Loss: 4055.2891\n",
      "Epoch: 218/1000 | Batch 0006/0010 | Loss: 3770.0037\n",
      "Epoch: 218/1000 | Batch 0007/0010 | Loss: 3869.4370\n",
      "Epoch: 218/1000 | Batch 0008/0010 | Loss: 3819.6323\n",
      "Epoch: 218/1000 | Batch 0009/0010 | Loss: 3936.6829\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 219/1000 | Batch 0000/0010 | Loss: 3523.7742\n",
      "Epoch: 219/1000 | Batch 0001/0010 | Loss: 3734.7678\n",
      "Epoch: 219/1000 | Batch 0002/0010 | Loss: 3875.3442\n",
      "Epoch: 219/1000 | Batch 0003/0010 | Loss: 3833.9958\n",
      "Epoch: 219/1000 | Batch 0004/0010 | Loss: 3807.4551\n",
      "Epoch: 219/1000 | Batch 0005/0010 | Loss: 3910.6028\n",
      "Epoch: 219/1000 | Batch 0006/0010 | Loss: 4064.6228\n",
      "Epoch: 219/1000 | Batch 0007/0010 | Loss: 3858.1169\n",
      "Epoch: 219/1000 | Batch 0008/0010 | Loss: 3956.2581\n",
      "Epoch: 219/1000 | Batch 0009/0010 | Loss: 3488.0530\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 220/1000 | Batch 0000/0010 | Loss: 3692.5457\n",
      "Epoch: 220/1000 | Batch 0001/0010 | Loss: 3782.6514\n",
      "Epoch: 220/1000 | Batch 0002/0010 | Loss: 3789.1628\n",
      "Epoch: 220/1000 | Batch 0003/0010 | Loss: 3687.1768\n",
      "Epoch: 220/1000 | Batch 0004/0010 | Loss: 3842.2307\n",
      "Epoch: 220/1000 | Batch 0005/0010 | Loss: 3921.4634\n",
      "Epoch: 220/1000 | Batch 0006/0010 | Loss: 3805.7461\n",
      "Epoch: 220/1000 | Batch 0007/0010 | Loss: 3817.7676\n",
      "Epoch: 220/1000 | Batch 0008/0010 | Loss: 3772.9771\n",
      "Epoch: 220/1000 | Batch 0009/0010 | Loss: 3925.9282\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 221/1000 | Batch 0000/0010 | Loss: 3659.4199\n",
      "Epoch: 221/1000 | Batch 0001/0010 | Loss: 3795.8508\n",
      "Epoch: 221/1000 | Batch 0002/0010 | Loss: 3991.5149\n",
      "Epoch: 221/1000 | Batch 0003/0010 | Loss: 3952.9990\n",
      "Epoch: 221/1000 | Batch 0004/0010 | Loss: 3815.4695\n",
      "Epoch: 221/1000 | Batch 0005/0010 | Loss: 3857.5317\n",
      "Epoch: 221/1000 | Batch 0006/0010 | Loss: 3897.9929\n",
      "Epoch: 221/1000 | Batch 0007/0010 | Loss: 3714.4941\n",
      "Epoch: 221/1000 | Batch 0008/0010 | Loss: 3322.0242\n",
      "Epoch: 221/1000 | Batch 0009/0010 | Loss: 4113.9927\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 222/1000 | Batch 0000/0010 | Loss: 3794.8962\n",
      "Epoch: 222/1000 | Batch 0001/0010 | Loss: 3797.3228\n",
      "Epoch: 222/1000 | Batch 0002/0010 | Loss: 3995.5869\n",
      "Epoch: 222/1000 | Batch 0003/0010 | Loss: 3942.1624\n",
      "Epoch: 222/1000 | Batch 0004/0010 | Loss: 3738.9294\n",
      "Epoch: 222/1000 | Batch 0005/0010 | Loss: 3724.3777\n",
      "Epoch: 222/1000 | Batch 0006/0010 | Loss: 3667.9749\n",
      "Epoch: 222/1000 | Batch 0007/0010 | Loss: 3771.5229\n",
      "Epoch: 222/1000 | Batch 0008/0010 | Loss: 3705.9771\n",
      "Epoch: 222/1000 | Batch 0009/0010 | Loss: 3928.2947\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 223/1000 | Batch 0000/0010 | Loss: 3584.6868\n",
      "Epoch: 223/1000 | Batch 0001/0010 | Loss: 4100.8633\n",
      "Epoch: 223/1000 | Batch 0002/0010 | Loss: 3707.7710\n",
      "Epoch: 223/1000 | Batch 0003/0010 | Loss: 3883.0344\n",
      "Epoch: 223/1000 | Batch 0004/0010 | Loss: 3722.0901\n",
      "Epoch: 223/1000 | Batch 0005/0010 | Loss: 3715.3000\n",
      "Epoch: 223/1000 | Batch 0006/0010 | Loss: 4161.1851\n",
      "Epoch: 223/1000 | Batch 0007/0010 | Loss: 3747.6853\n",
      "Epoch: 223/1000 | Batch 0008/0010 | Loss: 3517.8474\n",
      "Epoch: 223/1000 | Batch 0009/0010 | Loss: 3909.7837\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 224/1000 | Batch 0000/0010 | Loss: 3363.0176\n",
      "Epoch: 224/1000 | Batch 0001/0010 | Loss: 3778.8054\n",
      "Epoch: 224/1000 | Batch 0002/0010 | Loss: 3799.5466\n",
      "Epoch: 224/1000 | Batch 0003/0010 | Loss: 4087.6531\n",
      "Epoch: 224/1000 | Batch 0004/0010 | Loss: 3735.4109\n",
      "Epoch: 224/1000 | Batch 0005/0010 | Loss: 3694.9900\n",
      "Epoch: 224/1000 | Batch 0006/0010 | Loss: 3898.2417\n",
      "Epoch: 224/1000 | Batch 0007/0010 | Loss: 3681.6482\n",
      "Epoch: 224/1000 | Batch 0008/0010 | Loss: 3762.5435\n",
      "Epoch: 224/1000 | Batch 0009/0010 | Loss: 4077.3757\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 225/1000 | Batch 0000/0010 | Loss: 3756.4321\n",
      "Epoch: 225/1000 | Batch 0001/0010 | Loss: 4062.2209\n",
      "Epoch: 225/1000 | Batch 0002/0010 | Loss: 3935.7383\n",
      "Epoch: 225/1000 | Batch 0003/0010 | Loss: 3703.4976\n",
      "Epoch: 225/1000 | Batch 0004/0010 | Loss: 3713.0776\n",
      "Epoch: 225/1000 | Batch 0005/0010 | Loss: 3675.1824\n",
      "Epoch: 225/1000 | Batch 0006/0010 | Loss: 3657.2939\n",
      "Epoch: 225/1000 | Batch 0007/0010 | Loss: 3800.7917\n",
      "Epoch: 225/1000 | Batch 0008/0010 | Loss: 3796.3154\n",
      "Epoch: 225/1000 | Batch 0009/0010 | Loss: 3939.4370\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 226/1000 | Batch 0000/0010 | Loss: 3387.5439\n",
      "Epoch: 226/1000 | Batch 0001/0010 | Loss: 3740.4541\n",
      "Epoch: 226/1000 | Batch 0002/0010 | Loss: 4036.5466\n",
      "Epoch: 226/1000 | Batch 0003/0010 | Loss: 3801.1548\n",
      "Epoch: 226/1000 | Batch 0004/0010 | Loss: 3903.5608\n",
      "Epoch: 226/1000 | Batch 0005/0010 | Loss: 3936.3450\n",
      "Epoch: 226/1000 | Batch 0006/0010 | Loss: 3744.9419\n",
      "Epoch: 226/1000 | Batch 0007/0010 | Loss: 3699.3906\n",
      "Epoch: 226/1000 | Batch 0008/0010 | Loss: 3964.1108\n",
      "Epoch: 226/1000 | Batch 0009/0010 | Loss: 3681.0603\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 227/1000 | Batch 0000/0010 | Loss: 3603.2546\n",
      "Epoch: 227/1000 | Batch 0001/0010 | Loss: 3651.4858\n",
      "Epoch: 227/1000 | Batch 0002/0010 | Loss: 4052.7451\n",
      "Epoch: 227/1000 | Batch 0003/0010 | Loss: 4040.7068\n",
      "Epoch: 227/1000 | Batch 0004/0010 | Loss: 3533.8975\n",
      "Epoch: 227/1000 | Batch 0005/0010 | Loss: 3668.8860\n",
      "Epoch: 227/1000 | Batch 0006/0010 | Loss: 3967.8501\n",
      "Epoch: 227/1000 | Batch 0007/0010 | Loss: 3730.8586\n",
      "Epoch: 227/1000 | Batch 0008/0010 | Loss: 3542.5684\n",
      "Epoch: 227/1000 | Batch 0009/0010 | Loss: 4093.7712\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 228/1000 | Batch 0000/0010 | Loss: 3790.9429\n",
      "Epoch: 228/1000 | Batch 0001/0010 | Loss: 3434.2141\n",
      "Epoch: 228/1000 | Batch 0002/0010 | Loss: 3394.9304\n",
      "Epoch: 228/1000 | Batch 0003/0010 | Loss: 3651.3521\n",
      "Epoch: 228/1000 | Batch 0004/0010 | Loss: 3849.8071\n",
      "Epoch: 228/1000 | Batch 0005/0010 | Loss: 3969.9551\n",
      "Epoch: 228/1000 | Batch 0006/0010 | Loss: 3932.5220\n",
      "Epoch: 228/1000 | Batch 0007/0010 | Loss: 4166.3096\n",
      "Epoch: 228/1000 | Batch 0008/0010 | Loss: 3711.4963\n",
      "Epoch: 228/1000 | Batch 0009/0010 | Loss: 3999.9639\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 229/1000 | Batch 0000/0010 | Loss: 3596.3198\n",
      "Epoch: 229/1000 | Batch 0001/0010 | Loss: 3660.0615\n",
      "Epoch: 229/1000 | Batch 0002/0010 | Loss: 3841.5352\n",
      "Epoch: 229/1000 | Batch 0003/0010 | Loss: 3677.5579\n",
      "Epoch: 229/1000 | Batch 0004/0010 | Loss: 3793.6624\n",
      "Epoch: 229/1000 | Batch 0005/0010 | Loss: 3628.7097\n",
      "Epoch: 229/1000 | Batch 0006/0010 | Loss: 3691.2463\n",
      "Epoch: 229/1000 | Batch 0007/0010 | Loss: 4164.4878\n",
      "Epoch: 229/1000 | Batch 0008/0010 | Loss: 3916.0952\n",
      "Epoch: 229/1000 | Batch 0009/0010 | Loss: 3993.1460\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 230/1000 | Batch 0000/0010 | Loss: 3575.9763\n",
      "Epoch: 230/1000 | Batch 0001/0010 | Loss: 3787.5286\n",
      "Epoch: 230/1000 | Batch 0002/0010 | Loss: 3795.2573\n",
      "Epoch: 230/1000 | Batch 0003/0010 | Loss: 3946.3140\n",
      "Epoch: 230/1000 | Batch 0004/0010 | Loss: 3816.0430\n",
      "Epoch: 230/1000 | Batch 0005/0010 | Loss: 4045.6855\n",
      "Epoch: 230/1000 | Batch 0006/0010 | Loss: 3961.2915\n",
      "Epoch: 230/1000 | Batch 0007/0010 | Loss: 3677.8442\n",
      "Epoch: 230/1000 | Batch 0008/0010 | Loss: 3816.8269\n",
      "Epoch: 230/1000 | Batch 0009/0010 | Loss: 3444.0249\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 231/1000 | Batch 0000/0010 | Loss: 3749.2229\n",
      "Epoch: 231/1000 | Batch 0001/0010 | Loss: 3823.6787\n",
      "Epoch: 231/1000 | Batch 0002/0010 | Loss: 3600.1711\n",
      "Epoch: 231/1000 | Batch 0003/0010 | Loss: 3752.3706\n",
      "Epoch: 231/1000 | Batch 0004/0010 | Loss: 3784.2227\n",
      "Epoch: 231/1000 | Batch 0005/0010 | Loss: 3994.3157\n",
      "Epoch: 231/1000 | Batch 0006/0010 | Loss: 3828.4229\n",
      "Epoch: 231/1000 | Batch 0007/0010 | Loss: 3696.6917\n",
      "Epoch: 231/1000 | Batch 0008/0010 | Loss: 3702.2227\n",
      "Epoch: 231/1000 | Batch 0009/0010 | Loss: 4146.2202\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 232/1000 | Batch 0000/0010 | Loss: 3880.4629\n",
      "Epoch: 232/1000 | Batch 0001/0010 | Loss: 3613.5393\n",
      "Epoch: 232/1000 | Batch 0002/0010 | Loss: 3855.7898\n",
      "Epoch: 232/1000 | Batch 0003/0010 | Loss: 3579.8191\n",
      "Epoch: 232/1000 | Batch 0004/0010 | Loss: 3722.8826\n",
      "Epoch: 232/1000 | Batch 0005/0010 | Loss: 3688.9832\n",
      "Epoch: 232/1000 | Batch 0006/0010 | Loss: 3678.9160\n",
      "Epoch: 232/1000 | Batch 0007/0010 | Loss: 4094.4978\n",
      "Epoch: 232/1000 | Batch 0008/0010 | Loss: 3591.9490\n",
      "Epoch: 232/1000 | Batch 0009/0010 | Loss: 4242.5752\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 233/1000 | Batch 0000/0010 | Loss: 3712.9045\n",
      "Epoch: 233/1000 | Batch 0001/0010 | Loss: 3751.4648\n",
      "Epoch: 233/1000 | Batch 0002/0010 | Loss: 3755.9021\n",
      "Epoch: 233/1000 | Batch 0003/0010 | Loss: 3609.8135\n",
      "Epoch: 233/1000 | Batch 0004/0010 | Loss: 3547.9424\n",
      "Epoch: 233/1000 | Batch 0005/0010 | Loss: 3877.9038\n",
      "Epoch: 233/1000 | Batch 0006/0010 | Loss: 3823.8472\n",
      "Epoch: 233/1000 | Batch 0007/0010 | Loss: 3908.6028\n",
      "Epoch: 233/1000 | Batch 0008/0010 | Loss: 4074.6545\n",
      "Epoch: 233/1000 | Batch 0009/0010 | Loss: 3899.9839\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 234/1000 | Batch 0000/0010 | Loss: 3694.4905\n",
      "Epoch: 234/1000 | Batch 0001/0010 | Loss: 3746.5984\n",
      "Epoch: 234/1000 | Batch 0002/0010 | Loss: 3970.9097\n",
      "Epoch: 234/1000 | Batch 0003/0010 | Loss: 3751.5208\n",
      "Epoch: 234/1000 | Batch 0004/0010 | Loss: 4137.6772\n",
      "Epoch: 234/1000 | Batch 0005/0010 | Loss: 3715.3281\n",
      "Epoch: 234/1000 | Batch 0006/0010 | Loss: 3645.8999\n",
      "Epoch: 234/1000 | Batch 0007/0010 | Loss: 3765.3682\n",
      "Epoch: 234/1000 | Batch 0008/0010 | Loss: 3493.4685\n",
      "Epoch: 234/1000 | Batch 0009/0010 | Loss: 3978.1182\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 235/1000 | Batch 0000/0010 | Loss: 4119.3887\n",
      "Epoch: 235/1000 | Batch 0001/0010 | Loss: 4079.7993\n",
      "Epoch: 235/1000 | Batch 0002/0010 | Loss: 3715.1184\n",
      "Epoch: 235/1000 | Batch 0003/0010 | Loss: 3596.2354\n",
      "Epoch: 235/1000 | Batch 0004/0010 | Loss: 3805.6997\n",
      "Epoch: 235/1000 | Batch 0005/0010 | Loss: 3871.2839\n",
      "Epoch: 235/1000 | Batch 0006/0010 | Loss: 3905.2446\n",
      "Epoch: 235/1000 | Batch 0007/0010 | Loss: 3747.7371\n",
      "Epoch: 235/1000 | Batch 0008/0010 | Loss: 3508.4307\n",
      "Epoch: 235/1000 | Batch 0009/0010 | Loss: 3822.7021\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 236/1000 | Batch 0000/0010 | Loss: 3957.5547\n",
      "Epoch: 236/1000 | Batch 0001/0010 | Loss: 3957.4856\n",
      "Epoch: 236/1000 | Batch 0002/0010 | Loss: 3813.5801\n",
      "Epoch: 236/1000 | Batch 0003/0010 | Loss: 3518.3171\n",
      "Epoch: 236/1000 | Batch 0004/0010 | Loss: 3866.2043\n",
      "Epoch: 236/1000 | Batch 0005/0010 | Loss: 3611.0437\n",
      "Epoch: 236/1000 | Batch 0006/0010 | Loss: 3715.3716\n",
      "Epoch: 236/1000 | Batch 0007/0010 | Loss: 3677.7161\n",
      "Epoch: 236/1000 | Batch 0008/0010 | Loss: 3901.1992\n",
      "Epoch: 236/1000 | Batch 0009/0010 | Loss: 3964.1711\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 237/1000 | Batch 0000/0010 | Loss: 3764.1028\n",
      "Epoch: 237/1000 | Batch 0001/0010 | Loss: 3643.0410\n",
      "Epoch: 237/1000 | Batch 0002/0010 | Loss: 3666.3801\n",
      "Epoch: 237/1000 | Batch 0003/0010 | Loss: 3683.3943\n",
      "Epoch: 237/1000 | Batch 0004/0010 | Loss: 3773.1287\n",
      "Epoch: 237/1000 | Batch 0005/0010 | Loss: 4064.8992\n",
      "Epoch: 237/1000 | Batch 0006/0010 | Loss: 3752.6697\n",
      "Epoch: 237/1000 | Batch 0007/0010 | Loss: 3849.1274\n",
      "Epoch: 237/1000 | Batch 0008/0010 | Loss: 4013.2017\n",
      "Epoch: 237/1000 | Batch 0009/0010 | Loss: 3796.4624\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 238/1000 | Batch 0000/0010 | Loss: 3886.6250\n",
      "Epoch: 238/1000 | Batch 0001/0010 | Loss: 3970.3066\n",
      "Epoch: 238/1000 | Batch 0002/0010 | Loss: 3884.0137\n",
      "Epoch: 238/1000 | Batch 0003/0010 | Loss: 3864.1313\n",
      "Epoch: 238/1000 | Batch 0004/0010 | Loss: 3966.4744\n",
      "Epoch: 238/1000 | Batch 0005/0010 | Loss: 3663.0061\n",
      "Epoch: 238/1000 | Batch 0006/0010 | Loss: 3565.5261\n",
      "Epoch: 238/1000 | Batch 0007/0010 | Loss: 3771.1643\n",
      "Epoch: 238/1000 | Batch 0008/0010 | Loss: 3585.6272\n",
      "Epoch: 238/1000 | Batch 0009/0010 | Loss: 3710.5205\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 239/1000 | Batch 0000/0010 | Loss: 3699.6143\n",
      "Epoch: 239/1000 | Batch 0001/0010 | Loss: 3795.6191\n",
      "Epoch: 239/1000 | Batch 0002/0010 | Loss: 3670.2542\n",
      "Epoch: 239/1000 | Batch 0003/0010 | Loss: 3742.7959\n",
      "Epoch: 239/1000 | Batch 0004/0010 | Loss: 3875.6365\n",
      "Epoch: 239/1000 | Batch 0005/0010 | Loss: 3733.6814\n",
      "Epoch: 239/1000 | Batch 0006/0010 | Loss: 3947.8772\n",
      "Epoch: 239/1000 | Batch 0007/0010 | Loss: 3691.0359\n",
      "Epoch: 239/1000 | Batch 0008/0010 | Loss: 3837.6814\n",
      "Epoch: 239/1000 | Batch 0009/0010 | Loss: 3865.3235\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 240/1000 | Batch 0000/0010 | Loss: 3777.5930\n",
      "Epoch: 240/1000 | Batch 0001/0010 | Loss: 3754.8992\n",
      "Epoch: 240/1000 | Batch 0002/0010 | Loss: 3681.2805\n",
      "Epoch: 240/1000 | Batch 0003/0010 | Loss: 3941.4395\n",
      "Epoch: 240/1000 | Batch 0004/0010 | Loss: 3617.4702\n",
      "Epoch: 240/1000 | Batch 0005/0010 | Loss: 3973.8071\n",
      "Epoch: 240/1000 | Batch 0006/0010 | Loss: 3610.8584\n",
      "Epoch: 240/1000 | Batch 0007/0010 | Loss: 3955.7222\n",
      "Epoch: 240/1000 | Batch 0008/0010 | Loss: 3714.1392\n",
      "Epoch: 240/1000 | Batch 0009/0010 | Loss: 3853.8635\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 241/1000 | Batch 0000/0010 | Loss: 3953.0964\n",
      "Epoch: 241/1000 | Batch 0001/0010 | Loss: 3774.1121\n",
      "Epoch: 241/1000 | Batch 0002/0010 | Loss: 3701.8794\n",
      "Epoch: 241/1000 | Batch 0003/0010 | Loss: 3933.9563\n",
      "Epoch: 241/1000 | Batch 0004/0010 | Loss: 3906.5283\n",
      "Epoch: 241/1000 | Batch 0005/0010 | Loss: 3453.6006\n",
      "Epoch: 241/1000 | Batch 0006/0010 | Loss: 3340.4067\n",
      "Epoch: 241/1000 | Batch 0007/0010 | Loss: 3977.3455\n",
      "Epoch: 241/1000 | Batch 0008/0010 | Loss: 4043.9568\n",
      "Epoch: 241/1000 | Batch 0009/0010 | Loss: 3824.9504\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 242/1000 | Batch 0000/0010 | Loss: 3991.6145\n",
      "Epoch: 242/1000 | Batch 0001/0010 | Loss: 3546.6025\n",
      "Epoch: 242/1000 | Batch 0002/0010 | Loss: 3767.3208\n",
      "Epoch: 242/1000 | Batch 0003/0010 | Loss: 3917.6355\n",
      "Epoch: 242/1000 | Batch 0004/0010 | Loss: 3781.9612\n",
      "Epoch: 242/1000 | Batch 0005/0010 | Loss: 3929.7866\n",
      "Epoch: 242/1000 | Batch 0006/0010 | Loss: 3712.6689\n",
      "Epoch: 242/1000 | Batch 0007/0010 | Loss: 3904.8918\n",
      "Epoch: 242/1000 | Batch 0008/0010 | Loss: 3694.1826\n",
      "Epoch: 242/1000 | Batch 0009/0010 | Loss: 3721.5273\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 243/1000 | Batch 0000/0010 | Loss: 3757.9897\n",
      "Epoch: 243/1000 | Batch 0001/0010 | Loss: 3721.8513\n",
      "Epoch: 243/1000 | Batch 0002/0010 | Loss: 3866.7715\n",
      "Epoch: 243/1000 | Batch 0003/0010 | Loss: 3581.1130\n",
      "Epoch: 243/1000 | Batch 0004/0010 | Loss: 3782.4438\n",
      "Epoch: 243/1000 | Batch 0005/0010 | Loss: 4048.7197\n",
      "Epoch: 243/1000 | Batch 0006/0010 | Loss: 3864.6738\n",
      "Epoch: 243/1000 | Batch 0007/0010 | Loss: 3831.7542\n",
      "Epoch: 243/1000 | Batch 0008/0010 | Loss: 3862.9429\n",
      "Epoch: 243/1000 | Batch 0009/0010 | Loss: 3761.5645\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 244/1000 | Batch 0000/0010 | Loss: 3422.8479\n",
      "Epoch: 244/1000 | Batch 0001/0010 | Loss: 3895.7910\n",
      "Epoch: 244/1000 | Batch 0002/0010 | Loss: 3629.7373\n",
      "Epoch: 244/1000 | Batch 0003/0010 | Loss: 4029.3469\n",
      "Epoch: 244/1000 | Batch 0004/0010 | Loss: 3686.1436\n",
      "Epoch: 244/1000 | Batch 0005/0010 | Loss: 3671.8938\n",
      "Epoch: 244/1000 | Batch 0006/0010 | Loss: 3745.5222\n",
      "Epoch: 244/1000 | Batch 0007/0010 | Loss: 3829.1304\n",
      "Epoch: 244/1000 | Batch 0008/0010 | Loss: 4021.2903\n",
      "Epoch: 244/1000 | Batch 0009/0010 | Loss: 4095.1404\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 245/1000 | Batch 0000/0010 | Loss: 3896.5664\n",
      "Epoch: 245/1000 | Batch 0001/0010 | Loss: 3901.0117\n",
      "Epoch: 245/1000 | Batch 0002/0010 | Loss: 3639.9512\n",
      "Epoch: 245/1000 | Batch 0003/0010 | Loss: 3828.3752\n",
      "Epoch: 245/1000 | Batch 0004/0010 | Loss: 3969.1038\n",
      "Epoch: 245/1000 | Batch 0005/0010 | Loss: 3944.8335\n",
      "Epoch: 245/1000 | Batch 0006/0010 | Loss: 3602.2881\n",
      "Epoch: 245/1000 | Batch 0007/0010 | Loss: 3763.6821\n",
      "Epoch: 245/1000 | Batch 0008/0010 | Loss: 3730.0845\n",
      "Epoch: 245/1000 | Batch 0009/0010 | Loss: 3773.0798\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 246/1000 | Batch 0000/0010 | Loss: 3428.4443\n",
      "Epoch: 246/1000 | Batch 0001/0010 | Loss: 3911.3806\n",
      "Epoch: 246/1000 | Batch 0002/0010 | Loss: 3912.8005\n",
      "Epoch: 246/1000 | Batch 0003/0010 | Loss: 3788.4849\n",
      "Epoch: 246/1000 | Batch 0004/0010 | Loss: 3921.6843\n",
      "Epoch: 246/1000 | Batch 0005/0010 | Loss: 3853.8040\n",
      "Epoch: 246/1000 | Batch 0006/0010 | Loss: 3704.4353\n",
      "Epoch: 246/1000 | Batch 0007/0010 | Loss: 4013.4885\n",
      "Epoch: 246/1000 | Batch 0008/0010 | Loss: 3793.6592\n",
      "Epoch: 246/1000 | Batch 0009/0010 | Loss: 3623.8525\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 247/1000 | Batch 0000/0010 | Loss: 3826.4541\n",
      "Epoch: 247/1000 | Batch 0001/0010 | Loss: 4049.5513\n",
      "Epoch: 247/1000 | Batch 0002/0010 | Loss: 3632.5168\n",
      "Epoch: 247/1000 | Batch 0003/0010 | Loss: 3598.0229\n",
      "Epoch: 247/1000 | Batch 0004/0010 | Loss: 3646.4768\n",
      "Epoch: 247/1000 | Batch 0005/0010 | Loss: 3861.4153\n",
      "Epoch: 247/1000 | Batch 0006/0010 | Loss: 3726.9810\n",
      "Epoch: 247/1000 | Batch 0007/0010 | Loss: 4298.2671\n",
      "Epoch: 247/1000 | Batch 0008/0010 | Loss: 3766.0728\n",
      "Epoch: 247/1000 | Batch 0009/0010 | Loss: 3652.7786\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 248/1000 | Batch 0000/0010 | Loss: 3832.2812\n",
      "Epoch: 248/1000 | Batch 0001/0010 | Loss: 3689.4094\n",
      "Epoch: 248/1000 | Batch 0002/0010 | Loss: 3835.9404\n",
      "Epoch: 248/1000 | Batch 0003/0010 | Loss: 3970.2427\n",
      "Epoch: 248/1000 | Batch 0004/0010 | Loss: 3615.2654\n",
      "Epoch: 248/1000 | Batch 0005/0010 | Loss: 3840.1736\n",
      "Epoch: 248/1000 | Batch 0006/0010 | Loss: 3728.8022\n",
      "Epoch: 248/1000 | Batch 0007/0010 | Loss: 3668.7590\n",
      "Epoch: 248/1000 | Batch 0008/0010 | Loss: 3808.7031\n",
      "Epoch: 248/1000 | Batch 0009/0010 | Loss: 3960.8650\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 249/1000 | Batch 0000/0010 | Loss: 3801.7976\n",
      "Epoch: 249/1000 | Batch 0001/0010 | Loss: 3660.0637\n",
      "Epoch: 249/1000 | Batch 0002/0010 | Loss: 3770.8884\n",
      "Epoch: 249/1000 | Batch 0003/0010 | Loss: 3743.1389\n",
      "Epoch: 249/1000 | Batch 0004/0010 | Loss: 3788.9692\n",
      "Epoch: 249/1000 | Batch 0005/0010 | Loss: 3713.0356\n",
      "Epoch: 249/1000 | Batch 0006/0010 | Loss: 4079.2642\n",
      "Epoch: 249/1000 | Batch 0007/0010 | Loss: 3930.5278\n",
      "Epoch: 249/1000 | Batch 0008/0010 | Loss: 3747.8516\n",
      "Epoch: 249/1000 | Batch 0009/0010 | Loss: 3686.0830\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 250/1000 | Batch 0000/0010 | Loss: 3699.9336\n",
      "Epoch: 250/1000 | Batch 0001/0010 | Loss: 3783.6785\n",
      "Epoch: 250/1000 | Batch 0002/0010 | Loss: 4093.1853\n",
      "Epoch: 250/1000 | Batch 0003/0010 | Loss: 3930.4697\n",
      "Epoch: 250/1000 | Batch 0004/0010 | Loss: 3640.1465\n",
      "Epoch: 250/1000 | Batch 0005/0010 | Loss: 3918.4792\n",
      "Epoch: 250/1000 | Batch 0006/0010 | Loss: 3813.3198\n",
      "Epoch: 250/1000 | Batch 0007/0010 | Loss: 3918.1006\n",
      "Epoch: 250/1000 | Batch 0008/0010 | Loss: 3761.7651\n",
      "Epoch: 250/1000 | Batch 0009/0010 | Loss: 3438.8215\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 251/1000 | Batch 0000/0010 | Loss: 3833.9756\n",
      "Epoch: 251/1000 | Batch 0001/0010 | Loss: 3835.8958\n",
      "Epoch: 251/1000 | Batch 0002/0010 | Loss: 3615.5457\n",
      "Epoch: 251/1000 | Batch 0003/0010 | Loss: 3990.9209\n",
      "Epoch: 251/1000 | Batch 0004/0010 | Loss: 3732.3286\n",
      "Epoch: 251/1000 | Batch 0005/0010 | Loss: 3781.7678\n",
      "Epoch: 251/1000 | Batch 0006/0010 | Loss: 3888.3672\n",
      "Epoch: 251/1000 | Batch 0007/0010 | Loss: 4112.9287\n",
      "Epoch: 251/1000 | Batch 0008/0010 | Loss: 3507.0366\n",
      "Epoch: 251/1000 | Batch 0009/0010 | Loss: 3550.6292\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 252/1000 | Batch 0000/0010 | Loss: 3901.6685\n",
      "Epoch: 252/1000 | Batch 0001/0010 | Loss: 3652.4924\n",
      "Epoch: 252/1000 | Batch 0002/0010 | Loss: 3890.8477\n",
      "Epoch: 252/1000 | Batch 0003/0010 | Loss: 3816.0774\n",
      "Epoch: 252/1000 | Batch 0004/0010 | Loss: 3768.3132\n",
      "Epoch: 252/1000 | Batch 0005/0010 | Loss: 3582.1716\n",
      "Epoch: 252/1000 | Batch 0006/0010 | Loss: 3760.5698\n",
      "Epoch: 252/1000 | Batch 0007/0010 | Loss: 3776.9092\n",
      "Epoch: 252/1000 | Batch 0008/0010 | Loss: 3822.2998\n",
      "Epoch: 252/1000 | Batch 0009/0010 | Loss: 3910.7742\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 253/1000 | Batch 0000/0010 | Loss: 3592.2073\n",
      "Epoch: 253/1000 | Batch 0001/0010 | Loss: 3822.0310\n",
      "Epoch: 253/1000 | Batch 0002/0010 | Loss: 3883.0920\n",
      "Epoch: 253/1000 | Batch 0003/0010 | Loss: 3835.1428\n",
      "Epoch: 253/1000 | Batch 0004/0010 | Loss: 3374.7668\n",
      "Epoch: 253/1000 | Batch 0005/0010 | Loss: 3693.8643\n",
      "Epoch: 253/1000 | Batch 0006/0010 | Loss: 3828.4158\n",
      "Epoch: 253/1000 | Batch 0007/0010 | Loss: 3785.4548\n",
      "Epoch: 253/1000 | Batch 0008/0010 | Loss: 4275.2368\n",
      "Epoch: 253/1000 | Batch 0009/0010 | Loss: 3775.6279\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 254/1000 | Batch 0000/0010 | Loss: 3902.2705\n",
      "Epoch: 254/1000 | Batch 0001/0010 | Loss: 3721.9685\n",
      "Epoch: 254/1000 | Batch 0002/0010 | Loss: 3982.9663\n",
      "Epoch: 254/1000 | Batch 0003/0010 | Loss: 3817.5046\n",
      "Epoch: 254/1000 | Batch 0004/0010 | Loss: 3807.2576\n",
      "Epoch: 254/1000 | Batch 0005/0010 | Loss: 3562.2842\n",
      "Epoch: 254/1000 | Batch 0006/0010 | Loss: 3782.3442\n",
      "Epoch: 254/1000 | Batch 0007/0010 | Loss: 3683.2554\n",
      "Epoch: 254/1000 | Batch 0008/0010 | Loss: 3822.5215\n",
      "Epoch: 254/1000 | Batch 0009/0010 | Loss: 3703.2231\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 255/1000 | Batch 0000/0010 | Loss: 3621.7222\n",
      "Epoch: 255/1000 | Batch 0001/0010 | Loss: 3998.7227\n",
      "Epoch: 255/1000 | Batch 0002/0010 | Loss: 3922.6123\n",
      "Epoch: 255/1000 | Batch 0003/0010 | Loss: 3568.8877\n",
      "Epoch: 255/1000 | Batch 0004/0010 | Loss: 3658.9541\n",
      "Epoch: 255/1000 | Batch 0005/0010 | Loss: 3812.5300\n",
      "Epoch: 255/1000 | Batch 0006/0010 | Loss: 3856.4836\n",
      "Epoch: 255/1000 | Batch 0007/0010 | Loss: 3754.0640\n",
      "Epoch: 255/1000 | Batch 0008/0010 | Loss: 3653.9231\n",
      "Epoch: 255/1000 | Batch 0009/0010 | Loss: 4027.1833\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 256/1000 | Batch 0000/0010 | Loss: 3892.4104\n",
      "Epoch: 256/1000 | Batch 0001/0010 | Loss: 3894.2029\n",
      "Epoch: 256/1000 | Batch 0002/0010 | Loss: 3831.3418\n",
      "Epoch: 256/1000 | Batch 0003/0010 | Loss: 3962.4236\n",
      "Epoch: 256/1000 | Batch 0004/0010 | Loss: 3622.4382\n",
      "Epoch: 256/1000 | Batch 0005/0010 | Loss: 3712.6621\n",
      "Epoch: 256/1000 | Batch 0006/0010 | Loss: 3778.9409\n",
      "Epoch: 256/1000 | Batch 0007/0010 | Loss: 3645.3145\n",
      "Epoch: 256/1000 | Batch 0008/0010 | Loss: 3926.6667\n",
      "Epoch: 256/1000 | Batch 0009/0010 | Loss: 3610.1960\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 257/1000 | Batch 0000/0010 | Loss: 3653.5586\n",
      "Epoch: 257/1000 | Batch 0001/0010 | Loss: 3875.4875\n",
      "Epoch: 257/1000 | Batch 0002/0010 | Loss: 3769.6702\n",
      "Epoch: 257/1000 | Batch 0003/0010 | Loss: 3796.4727\n",
      "Epoch: 257/1000 | Batch 0004/0010 | Loss: 3876.8875\n",
      "Epoch: 257/1000 | Batch 0005/0010 | Loss: 3585.5532\n",
      "Epoch: 257/1000 | Batch 0006/0010 | Loss: 3799.7842\n",
      "Epoch: 257/1000 | Batch 0007/0010 | Loss: 3847.5435\n",
      "Epoch: 257/1000 | Batch 0008/0010 | Loss: 3649.8757\n",
      "Epoch: 257/1000 | Batch 0009/0010 | Loss: 4017.1851\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 258/1000 | Batch 0000/0010 | Loss: 3596.8687\n",
      "Epoch: 258/1000 | Batch 0001/0010 | Loss: 3873.3555\n",
      "Epoch: 258/1000 | Batch 0002/0010 | Loss: 3514.8230\n",
      "Epoch: 258/1000 | Batch 0003/0010 | Loss: 3826.6926\n",
      "Epoch: 258/1000 | Batch 0004/0010 | Loss: 3653.3850\n",
      "Epoch: 258/1000 | Batch 0005/0010 | Loss: 3618.6575\n",
      "Epoch: 258/1000 | Batch 0006/0010 | Loss: 3848.6252\n",
      "Epoch: 258/1000 | Batch 0007/0010 | Loss: 4130.6284\n",
      "Epoch: 258/1000 | Batch 0008/0010 | Loss: 3967.9773\n",
      "Epoch: 258/1000 | Batch 0009/0010 | Loss: 3788.8186\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 259/1000 | Batch 0000/0010 | Loss: 3516.8936\n",
      "Epoch: 259/1000 | Batch 0001/0010 | Loss: 3823.4583\n",
      "Epoch: 259/1000 | Batch 0002/0010 | Loss: 3790.4470\n",
      "Epoch: 259/1000 | Batch 0003/0010 | Loss: 3712.6682\n",
      "Epoch: 259/1000 | Batch 0004/0010 | Loss: 3845.6621\n",
      "Epoch: 259/1000 | Batch 0005/0010 | Loss: 4118.8550\n",
      "Epoch: 259/1000 | Batch 0006/0010 | Loss: 3872.1582\n",
      "Epoch: 259/1000 | Batch 0007/0010 | Loss: 3751.4880\n",
      "Epoch: 259/1000 | Batch 0008/0010 | Loss: 3653.9412\n",
      "Epoch: 259/1000 | Batch 0009/0010 | Loss: 3773.8533\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 260/1000 | Batch 0000/0010 | Loss: 3539.3489\n",
      "Epoch: 260/1000 | Batch 0001/0010 | Loss: 3857.6597\n",
      "Epoch: 260/1000 | Batch 0002/0010 | Loss: 3958.7217\n",
      "Epoch: 260/1000 | Batch 0003/0010 | Loss: 3490.1980\n",
      "Epoch: 260/1000 | Batch 0004/0010 | Loss: 3871.1567\n",
      "Epoch: 260/1000 | Batch 0005/0010 | Loss: 3723.7705\n",
      "Epoch: 260/1000 | Batch 0006/0010 | Loss: 3892.0779\n",
      "Epoch: 260/1000 | Batch 0007/0010 | Loss: 4043.2827\n",
      "Epoch: 260/1000 | Batch 0008/0010 | Loss: 3952.9043\n",
      "Epoch: 260/1000 | Batch 0009/0010 | Loss: 3473.1262\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 261/1000 | Batch 0000/0010 | Loss: 3631.5745\n",
      "Epoch: 261/1000 | Batch 0001/0010 | Loss: 3748.1655\n",
      "Epoch: 261/1000 | Batch 0002/0010 | Loss: 3901.3914\n",
      "Epoch: 261/1000 | Batch 0003/0010 | Loss: 3436.9504\n",
      "Epoch: 261/1000 | Batch 0004/0010 | Loss: 3885.6516\n",
      "Epoch: 261/1000 | Batch 0005/0010 | Loss: 4054.3159\n",
      "Epoch: 261/1000 | Batch 0006/0010 | Loss: 3826.4177\n",
      "Epoch: 261/1000 | Batch 0007/0010 | Loss: 3790.5000\n",
      "Epoch: 261/1000 | Batch 0008/0010 | Loss: 3630.2590\n",
      "Epoch: 261/1000 | Batch 0009/0010 | Loss: 3852.9316\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 262/1000 | Batch 0000/0010 | Loss: 3590.9609\n",
      "Epoch: 262/1000 | Batch 0001/0010 | Loss: 4091.3091\n",
      "Epoch: 262/1000 | Batch 0002/0010 | Loss: 3554.6025\n",
      "Epoch: 262/1000 | Batch 0003/0010 | Loss: 3625.7500\n",
      "Epoch: 262/1000 | Batch 0004/0010 | Loss: 3675.3665\n",
      "Epoch: 262/1000 | Batch 0005/0010 | Loss: 3584.1121\n",
      "Epoch: 262/1000 | Batch 0006/0010 | Loss: 3838.0874\n",
      "Epoch: 262/1000 | Batch 0007/0010 | Loss: 4176.7778\n",
      "Epoch: 262/1000 | Batch 0008/0010 | Loss: 3796.3940\n",
      "Epoch: 262/1000 | Batch 0009/0010 | Loss: 3867.1328\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 263/1000 | Batch 0000/0010 | Loss: 4015.8572\n",
      "Epoch: 263/1000 | Batch 0001/0010 | Loss: 3932.0640\n",
      "Epoch: 263/1000 | Batch 0002/0010 | Loss: 3669.2546\n",
      "Epoch: 263/1000 | Batch 0003/0010 | Loss: 3430.9263\n",
      "Epoch: 263/1000 | Batch 0004/0010 | Loss: 3828.0857\n",
      "Epoch: 263/1000 | Batch 0005/0010 | Loss: 3705.4390\n",
      "Epoch: 263/1000 | Batch 0006/0010 | Loss: 3792.4011\n",
      "Epoch: 263/1000 | Batch 0007/0010 | Loss: 3842.9036\n",
      "Epoch: 263/1000 | Batch 0008/0010 | Loss: 3839.9602\n",
      "Epoch: 263/1000 | Batch 0009/0010 | Loss: 3749.3953\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 264/1000 | Batch 0000/0010 | Loss: 3692.4033\n",
      "Epoch: 264/1000 | Batch 0001/0010 | Loss: 3559.9863\n",
      "Epoch: 264/1000 | Batch 0002/0010 | Loss: 3853.9089\n",
      "Epoch: 264/1000 | Batch 0003/0010 | Loss: 3896.8149\n",
      "Epoch: 264/1000 | Batch 0004/0010 | Loss: 3861.6648\n",
      "Epoch: 264/1000 | Batch 0005/0010 | Loss: 3601.7246\n",
      "Epoch: 264/1000 | Batch 0006/0010 | Loss: 3808.0540\n",
      "Epoch: 264/1000 | Batch 0007/0010 | Loss: 3981.7200\n",
      "Epoch: 264/1000 | Batch 0008/0010 | Loss: 3872.2092\n",
      "Epoch: 264/1000 | Batch 0009/0010 | Loss: 3684.0713\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 265/1000 | Batch 0000/0010 | Loss: 3942.5942\n",
      "Epoch: 265/1000 | Batch 0001/0010 | Loss: 3620.8057\n",
      "Epoch: 265/1000 | Batch 0002/0010 | Loss: 3729.9226\n",
      "Epoch: 265/1000 | Batch 0003/0010 | Loss: 3740.3870\n",
      "Epoch: 265/1000 | Batch 0004/0010 | Loss: 3867.4353\n",
      "Epoch: 265/1000 | Batch 0005/0010 | Loss: 3692.6042\n",
      "Epoch: 265/1000 | Batch 0006/0010 | Loss: 3924.6458\n",
      "Epoch: 265/1000 | Batch 0007/0010 | Loss: 3572.9258\n",
      "Epoch: 265/1000 | Batch 0008/0010 | Loss: 3801.5454\n",
      "Epoch: 265/1000 | Batch 0009/0010 | Loss: 3839.0166\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 266/1000 | Batch 0000/0010 | Loss: 3708.1379\n",
      "Epoch: 266/1000 | Batch 0001/0010 | Loss: 3709.1121\n",
      "Epoch: 266/1000 | Batch 0002/0010 | Loss: 3824.3113\n",
      "Epoch: 266/1000 | Batch 0003/0010 | Loss: 4040.2466\n",
      "Epoch: 266/1000 | Batch 0004/0010 | Loss: 4056.3970\n",
      "Epoch: 266/1000 | Batch 0005/0010 | Loss: 3703.1047\n",
      "Epoch: 266/1000 | Batch 0006/0010 | Loss: 3462.0249\n",
      "Epoch: 266/1000 | Batch 0007/0010 | Loss: 3718.5132\n",
      "Epoch: 266/1000 | Batch 0008/0010 | Loss: 3861.1370\n",
      "Epoch: 266/1000 | Batch 0009/0010 | Loss: 3617.7148\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 267/1000 | Batch 0000/0010 | Loss: 3779.2490\n",
      "Epoch: 267/1000 | Batch 0001/0010 | Loss: 3955.6721\n",
      "Epoch: 267/1000 | Batch 0002/0010 | Loss: 3731.0232\n",
      "Epoch: 267/1000 | Batch 0003/0010 | Loss: 3599.5596\n",
      "Epoch: 267/1000 | Batch 0004/0010 | Loss: 3912.4832\n",
      "Epoch: 267/1000 | Batch 0005/0010 | Loss: 3568.8308\n",
      "Epoch: 267/1000 | Batch 0006/0010 | Loss: 3738.7959\n",
      "Epoch: 267/1000 | Batch 0007/0010 | Loss: 3690.3782\n",
      "Epoch: 267/1000 | Batch 0008/0010 | Loss: 4169.2402\n",
      "Epoch: 267/1000 | Batch 0009/0010 | Loss: 3748.0703\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 268/1000 | Batch 0000/0010 | Loss: 3745.0400\n",
      "Epoch: 268/1000 | Batch 0001/0010 | Loss: 3982.9988\n",
      "Epoch: 268/1000 | Batch 0002/0010 | Loss: 3946.5703\n",
      "Epoch: 268/1000 | Batch 0003/0010 | Loss: 3899.9050\n",
      "Epoch: 268/1000 | Batch 0004/0010 | Loss: 3243.6270\n",
      "Epoch: 268/1000 | Batch 0005/0010 | Loss: 3839.4133\n",
      "Epoch: 268/1000 | Batch 0006/0010 | Loss: 3816.2629\n",
      "Epoch: 268/1000 | Batch 0007/0010 | Loss: 3943.6829\n",
      "Epoch: 268/1000 | Batch 0008/0010 | Loss: 3821.0916\n",
      "Epoch: 268/1000 | Batch 0009/0010 | Loss: 3699.3145\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 269/1000 | Batch 0000/0010 | Loss: 3479.7476\n",
      "Epoch: 269/1000 | Batch 0001/0010 | Loss: 3876.1013\n",
      "Epoch: 269/1000 | Batch 0002/0010 | Loss: 3600.9341\n",
      "Epoch: 269/1000 | Batch 0003/0010 | Loss: 3823.4253\n",
      "Epoch: 269/1000 | Batch 0004/0010 | Loss: 3947.7803\n",
      "Epoch: 269/1000 | Batch 0005/0010 | Loss: 3541.9255\n",
      "Epoch: 269/1000 | Batch 0006/0010 | Loss: 3954.0757\n",
      "Epoch: 269/1000 | Batch 0007/0010 | Loss: 3986.3245\n",
      "Epoch: 269/1000 | Batch 0008/0010 | Loss: 3766.2463\n",
      "Epoch: 269/1000 | Batch 0009/0010 | Loss: 3792.0774\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 270/1000 | Batch 0000/0010 | Loss: 3655.2705\n",
      "Epoch: 270/1000 | Batch 0001/0010 | Loss: 3576.9746\n",
      "Epoch: 270/1000 | Batch 0002/0010 | Loss: 3684.1843\n",
      "Epoch: 270/1000 | Batch 0003/0010 | Loss: 3543.3994\n",
      "Epoch: 270/1000 | Batch 0004/0010 | Loss: 3769.7876\n",
      "Epoch: 270/1000 | Batch 0005/0010 | Loss: 3818.5725\n",
      "Epoch: 270/1000 | Batch 0006/0010 | Loss: 3845.7039\n",
      "Epoch: 270/1000 | Batch 0007/0010 | Loss: 4180.1611\n",
      "Epoch: 270/1000 | Batch 0008/0010 | Loss: 4168.1001\n",
      "Epoch: 270/1000 | Batch 0009/0010 | Loss: 3592.4036\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 271/1000 | Batch 0000/0010 | Loss: 3637.3711\n",
      "Epoch: 271/1000 | Batch 0001/0010 | Loss: 3691.0742\n",
      "Epoch: 271/1000 | Batch 0002/0010 | Loss: 3753.5010\n",
      "Epoch: 271/1000 | Batch 0003/0010 | Loss: 3601.2415\n",
      "Epoch: 271/1000 | Batch 0004/0010 | Loss: 3674.8823\n",
      "Epoch: 271/1000 | Batch 0005/0010 | Loss: 3967.6455\n",
      "Epoch: 271/1000 | Batch 0006/0010 | Loss: 3769.8184\n",
      "Epoch: 271/1000 | Batch 0007/0010 | Loss: 3831.1257\n",
      "Epoch: 271/1000 | Batch 0008/0010 | Loss: 3759.0669\n",
      "Epoch: 271/1000 | Batch 0009/0010 | Loss: 4097.7817\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 272/1000 | Batch 0000/0010 | Loss: 3672.7273\n",
      "Epoch: 272/1000 | Batch 0001/0010 | Loss: 3828.2354\n",
      "Epoch: 272/1000 | Batch 0002/0010 | Loss: 3908.8145\n",
      "Epoch: 272/1000 | Batch 0003/0010 | Loss: 3842.5671\n",
      "Epoch: 272/1000 | Batch 0004/0010 | Loss: 3739.6143\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#%% run pilot experiments\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# This function trains VAE or CVAE, or GAN, WGAN, WGANGP, MAF, GLOW, RealNVP \u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#      with several pilot size given data, model, batch_size, learning_rate, epoch, off_aug and pre_model\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Gaussian_head_num: how many folds of Gaussianhead augmentation needed. Default value is 9, Only take effect when off_aug == \"Gaussian_head\"\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# pre_model:         transfer learning input model. If pre_model == None, no transfer learning\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[43mPilotExperiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataname\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSKCMPositive_4\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpilot_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mVAE1-10\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_frac\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m                \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.0005\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_model\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m                \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43moff_aug\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stop_num\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m                \u001b[49m\u001b[43mAE_head_num\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mGaussian_head_num\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m9\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/SyNG-BTS/.conda/lib/python3.11/site-packages/syng_bts/python/Experiments_new.py:380\u001b[39m, in \u001b[36mPilotExperiment\u001b[39m\u001b[34m(dataname, pilot_size, model, batch_frac, learning_rate, epoch, early_stop_num, off_aug, AE_head_num, Gaussian_head_num, pre_model)\u001b[39m\n\u001b[32m    377\u001b[39m     log_pd.to_csv(Path(losspath), index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mAE\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m modelname:\n\u001b[32m--> \u001b[39m\u001b[32m380\u001b[39m     log_dict = \u001b[43mtraining_AEs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m        \u001b[49m\u001b[43msavepath\u001b[49m\u001b[43m=\u001b[49m\u001b[43msavepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# path to save reconstructed samples\u001b[39;49;00m\n\u001b[32m    382\u001b[39m \u001b[43m        \u001b[49m\u001b[43msavepathnew\u001b[49m\u001b[43m=\u001b[49m\u001b[43msavepathnew\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# path to save newly generated samples\u001b[39;49;00m\n\u001b[32m    383\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrawdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrawdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# raw data tensor with samples in row, features in column\u001b[39;49;00m\n\u001b[32m    384\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrawlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrawlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# abels for each sample, n_samples * 1, will not be used in AE or VAE\u001b[39;49;00m\n\u001b[32m    385\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mround\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrawdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_frac\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# batch size\u001b[39;49;00m\n\u001b[32m    386\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodelname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodelname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# choose from \"VAE\", \"AE\"\u001b[39;49;00m\n\u001b[32m    388\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# maximum number of epochs if early stop is not triggered\u001b[39;49;00m\n\u001b[32m    389\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkl_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkl_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# only take effect if model name is VAE, default value is\u001b[39;49;00m\n\u001b[32m    391\u001b[39m \u001b[43m        \u001b[49m\u001b[43mearly_stop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mearly_stop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# whether use early stopping rule\u001b[39;49;00m\n\u001b[32m    392\u001b[39m \u001b[43m        \u001b[49m\u001b[43mearly_stop_num\u001b[49m\u001b[43m=\u001b[49m\u001b[43mearly_stop_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# stop training if loss does not improve for early_stop_num epochs\u001b[39;49;00m\n\u001b[32m    393\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpre_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpre_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# load pre-trained model from transfer learning\u001b[39;49;00m\n\u001b[32m    394\u001b[39m \u001b[43m        \u001b[49m\u001b[43msave_model\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# save model for transfer learning, specify the path if want to save model\u001b[39;49;00m\n\u001b[32m    395\u001b[39m \u001b[43m        \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mMSE\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# only choose WMSE if you know the weights, ow. choose MSE by default\u001b[39;49;00m\n\u001b[32m    396\u001b[39m \u001b[43m        \u001b[49m\u001b[43msave_recons\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# whether save reconstructed data, if True, savepath must be provided\u001b[39;49;00m\n\u001b[32m    397\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnew_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# how many new samples you want to generate\u001b[39;49;00m\n\u001b[32m    398\u001b[39m \u001b[43m        \u001b[49m\u001b[43msave_new\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# whether save new samples, if True, savepathnew must be provided\u001b[39;49;00m\n\u001b[32m    399\u001b[39m \u001b[43m        \u001b[49m\u001b[43mplot\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# whether plot reconstructed samples' heatmap\u001b[39;00m\n\u001b[32m    402\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mVAEs model training for one pilot size one draw finished.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    403\u001b[39m     log_pd = pd.DataFrame(\n\u001b[32m    404\u001b[39m         {\n\u001b[32m    405\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mkl\u001b[39m\u001b[33m\"\u001b[39m: log_dict[\u001b[33m\"\u001b[39m\u001b[33mtrain_kl_loss_per_batch\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    406\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mrecons\u001b[39m\u001b[33m\"\u001b[39m: log_dict[\u001b[33m\"\u001b[39m\u001b[33mtrain_reconstruction_loss_per_batch\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    407\u001b[39m         }\n\u001b[32m    408\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/SyNG-BTS/.conda/lib/python3.11/site-packages/syng_bts/python/helper_training_new.py:116\u001b[39m, in \u001b[36mtraining_AEs\u001b[39m\u001b[34m(savepath, savepathnew, rawdata, rawlabels, batch_size, random_seed, modelname, num_epochs, learning_rate, pre_model, save_model, kl_weight, early_stop, early_stop_num, loss_fn, save_recons, new_size, save_new, plot)\u001b[39m\n\u001b[32m    114\u001b[39m             plt.show()\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m modelname==\u001b[33m\"\u001b[39m\u001b[33mVAE\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m     log_dict, best_model = \u001b[43mht\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_VAE\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m                                     \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[43m                                     \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[43m                                     \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    120\u001b[39m \u001b[43m                                     \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m                                     \u001b[49m\u001b[43mearly_stop\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m                                     \u001b[49m\u001b[43mearly_stop_num\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stop_num\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m                                     \u001b[49m\u001b[43mskip_epoch_stats\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m                                     \u001b[49m\u001b[43mreconstruction_term_weight\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m                                     \u001b[49m\u001b[43mkl_weight\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mkl_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m                                     \u001b[49m\u001b[43mlogging_interval\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m                                     \u001b[49m\u001b[43msave_model\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    129\u001b[39m     plot_training_loss(log_dict[\u001b[33m'\u001b[39m\u001b[33mtrain_reconstruction_loss_per_batch\u001b[39m\u001b[33m'\u001b[39m], num_epochs, custom_label = \u001b[33m\"\u001b[39m\u001b[33m (reconstruction)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    130\u001b[39m     plt.show()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/SyNG-BTS/.conda/lib/python3.11/site-packages/syng_bts/python/helper_train_new.py:184\u001b[39m, in \u001b[36mtrain_VAE\u001b[39m\u001b[34m(num_epochs, model, optimizer, train_loader, early_stop, early_stop_num, loss_fn, logging_interval, skip_epoch_stats, reconstruction_term_weight, kl_weight, save_model)\u001b[39m\n\u001b[32m    181\u001b[39m loss.backward()\n\u001b[32m    183\u001b[39m \u001b[38;5;66;03m# UPDATE MODEL PARAMETERS\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[38;5;66;03m# LOGGING\u001b[39;00m\n\u001b[32m    187\u001b[39m log_dict[\u001b[33m'\u001b[39m\u001b[33mtrain_combined_loss_per_batch\u001b[39m\u001b[33m'\u001b[39m].append(loss.item())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/SyNG-BTS/.conda/lib/python3.11/site-packages/torch/optim/optimizer.py:516\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    511\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    512\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    513\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    514\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m516\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    519\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/SyNG-BTS/.conda/lib/python3.11/site-packages/torch/optim/optimizer.py:81\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     79\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     80\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     83\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/SyNG-BTS/.conda/lib/python3.11/site-packages/torch/optim/adam.py:247\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    235\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    237\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    238\u001b[39m         group,\n\u001b[32m    239\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    244\u001b[39m         state_steps,\n\u001b[32m    245\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdecoupled_weight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/SyNG-BTS/.conda/lib/python3.11/site-packages/torch/optim/optimizer.py:149\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/SyNG-BTS/.conda/lib/python3.11/site-packages/torch/optim/adam.py:949\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    946\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    947\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m949\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/SyNG-BTS/.conda/lib/python3.11/site-packages/torch/optim/adam.py:464\u001b[39m, in \u001b[36m_single_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[39m\n\u001b[32m    462\u001b[39m         exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=\u001b[32m1\u001b[39m - beta2)\n\u001b[32m    463\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m     \u001b[43mexp_avg_sq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m.addcmul_(grad, grad, value=\u001b[32m1\u001b[39m - beta2)\n\u001b[32m    466\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n\u001b[32m    467\u001b[39m     step = step_t\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#%% run pilot experiments\n",
    "# This function trains VAE or CVAE, or GAN, WGAN, WGANGP, MAF, GLOW, RealNVP \n",
    "#      with several pilot size given data, model, batch_size, learning_rate, epoch, off_aug and pre_model\n",
    "#      for each pilot size, there will be 5 draws, \n",
    "#      for each draw, the data is augmented to 5 times the original sample size.\n",
    "# dataname :         pure data name without .csv. Eg: SKCMPositive_3\n",
    "# pilot_size:        a set including potential pilot sizes\n",
    "# model:             name of the model to be trained\n",
    "# batch_frac:        batch fraction\n",
    "# learning_rate:     learning rate \n",
    "# epoch:             choose from None (early_stop), or any interger, if choose None, early_stop_num will take effect\n",
    "# early_stop_num:    if loss does not improve for early_stop_num epochs, the training will stop. Default value is 30. Only take effect when epoch == \"early_stop\"\n",
    "# off_aug:           choose from AE_head, Gaussian_head, None. if choose AE_head, AE_head_num will take effect. If choose Gaussian_head, Gaussian_head_num will take effect. If choose None, no offline augmentation\n",
    "# AE_head_num:       how many folds of AEhead augmentation needed. Default value is 2, Only take effect when off_aug == \"AE_head\"\n",
    "# Gaussian_head_num: how many folds of Gaussianhead augmentation needed. Default value is 9, Only take effect when off_aug == \"Gaussian_head\"\n",
    "# pre_model:         transfer learning input model. If pre_model == None, no transfer learning\n",
    "\n",
    "PilotExperiment(dataname = \"SKCMPositive_4\", pilot_size = [100],\n",
    "                model = \"VAE1-10\", batch_frac = 0.1, \n",
    "                learning_rate = 0.0005, pre_model = None,\n",
    "                epoch = None,  off_aug = None, early_stop_num = 30,\n",
    "                AE_head_num = 2, Gaussian_head_num = 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Read data, path is ../Case/LIHCSubtype/LIHCSubtypeFamInd_train294_DESeq.csv\n",
      "2. Determine the model is CVAE1-10 with kl-weight = 10\n",
      "3. Determine the training parameters are epoch = 3000 off_aug = No learing rate = 0.0005 batch_frac = 0.1\n",
      "3. Training starts ......\n",
      "Epoch: 001/3000 | Batch 0000/0011 | Loss: 24849.2695\n",
      "Epoch: 001/3000 | Batch 0001/0011 | Loss: 24773.7480\n",
      "Epoch: 001/3000 | Batch 0002/0011 | Loss: 24855.5684\n",
      "Epoch: 001/3000 | Batch 0003/0011 | Loss: 24610.9629\n",
      "Epoch: 001/3000 | Batch 0004/0011 | Loss: 24577.9395\n",
      "Epoch: 001/3000 | Batch 0005/0011 | Loss: 24874.4258\n",
      "Epoch: 001/3000 | Batch 0006/0011 | Loss: 24508.5527\n",
      "Epoch: 001/3000 | Batch 0007/0011 | Loss: 24185.0684\n",
      "Epoch: 001/3000 | Batch 0008/0011 | Loss: 24420.8340\n",
      "Epoch: 001/3000 | Batch 0009/0011 | Loss: 24157.8555\n",
      "Epoch: 001/3000 | Batch 0010/0011 | Loss: 24392.4824\n",
      "Time elapsed: 0.00 min\n",
      "Epoch: 002/3000 | Batch 0000/0011 | Loss: 23504.1055\n",
      "Epoch: 002/3000 | Batch 0001/0011 | Loss: 23652.3809\n",
      "Epoch: 002/3000 | Batch 0002/0011 | Loss: 23083.0586\n",
      "Epoch: 002/3000 | Batch 0003/0011 | Loss: 22480.3203\n",
      "Epoch: 002/3000 | Batch 0004/0011 | Loss: 21822.6191\n",
      "Epoch: 002/3000 | Batch 0005/0011 | Loss: 20925.4844\n",
      "Epoch: 002/3000 | Batch 0006/0011 | Loss: 20274.2500\n",
      "Epoch: 002/3000 | Batch 0007/0011 | Loss: 18834.5723\n",
      "Epoch: 002/3000 | Batch 0008/0011 | Loss: 17634.4648\n",
      "Epoch: 002/3000 | Batch 0009/0011 | Loss: 15893.9609\n",
      "Epoch: 002/3000 | Batch 0010/0011 | Loss: 14237.6406\n",
      "Time elapsed: 0.00 min\n",
      "Epoch: 003/3000 | Batch 0000/0011 | Loss: 13428.8545\n",
      "Epoch: 003/3000 | Batch 0001/0011 | Loss: 11838.9072\n",
      "Epoch: 003/3000 | Batch 0002/0011 | Loss: 10960.1729\n",
      "Epoch: 003/3000 | Batch 0003/0011 | Loss: 9904.4863\n",
      "Epoch: 003/3000 | Batch 0004/0011 | Loss: 9023.6934\n",
      "Epoch: 003/3000 | Batch 0005/0011 | Loss: 8219.1865\n",
      "Epoch: 003/3000 | Batch 0006/0011 | Loss: 7639.0146\n",
      "Epoch: 003/3000 | Batch 0007/0011 | Loss: 6907.1074\n",
      "Epoch: 003/3000 | Batch 0008/0011 | Loss: 6490.9526\n",
      "Epoch: 003/3000 | Batch 0009/0011 | Loss: 6076.0103\n",
      "Epoch: 003/3000 | Batch 0010/0011 | Loss: 5671.1826\n",
      "Time elapsed: 0.00 min\n",
      "Epoch: 004/3000 | Batch 0000/0011 | Loss: 5439.3188\n",
      "Epoch: 004/3000 | Batch 0001/0011 | Loss: 5178.7964\n",
      "Epoch: 004/3000 | Batch 0002/0011 | Loss: 4863.4888\n",
      "Epoch: 004/3000 | Batch 0003/0011 | Loss: 4868.7275\n",
      "Epoch: 004/3000 | Batch 0004/0011 | Loss: 4556.2227\n",
      "Epoch: 004/3000 | Batch 0005/0011 | Loss: 4485.9795\n",
      "Epoch: 004/3000 | Batch 0006/0011 | Loss: 4315.1665\n",
      "Epoch: 004/3000 | Batch 0007/0011 | Loss: 4170.7847\n",
      "Epoch: 004/3000 | Batch 0008/0011 | Loss: 4271.5493\n",
      "Epoch: 004/3000 | Batch 0009/0011 | Loss: 4096.4268\n",
      "Epoch: 004/3000 | Batch 0010/0011 | Loss: 4075.6108\n",
      "Time elapsed: 0.00 min\n",
      "Epoch: 005/3000 | Batch 0000/0011 | Loss: 3914.0566\n",
      "Epoch: 005/3000 | Batch 0001/0011 | Loss: 4112.1953\n",
      "Epoch: 005/3000 | Batch 0002/0011 | Loss: 3947.2725\n",
      "Epoch: 005/3000 | Batch 0003/0011 | Loss: 3808.0403\n",
      "Epoch: 005/3000 | Batch 0004/0011 | Loss: 3940.5747\n",
      "Epoch: 005/3000 | Batch 0005/0011 | Loss: 3698.2522\n",
      "Epoch: 005/3000 | Batch 0006/0011 | Loss: 3827.8491\n",
      "Epoch: 005/3000 | Batch 0007/0011 | Loss: 3760.6206\n",
      "Epoch: 005/3000 | Batch 0008/0011 | Loss: 3776.2854\n",
      "Epoch: 005/3000 | Batch 0009/0011 | Loss: 3617.3186\n",
      "Epoch: 005/3000 | Batch 0010/0011 | Loss: 4086.4590\n",
      "Time elapsed: 0.00 min\n",
      "Epoch: 006/3000 | Batch 0000/0011 | Loss: 3709.1941\n",
      "Epoch: 006/3000 | Batch 0001/0011 | Loss: 3749.0483\n",
      "Epoch: 006/3000 | Batch 0002/0011 | Loss: 3665.4937\n",
      "Epoch: 006/3000 | Batch 0003/0011 | Loss: 3796.5715\n",
      "Epoch: 006/3000 | Batch 0004/0011 | Loss: 3728.1606\n",
      "Epoch: 006/3000 | Batch 0005/0011 | Loss: 3824.1562\n",
      "Epoch: 006/3000 | Batch 0006/0011 | Loss: 3723.2295\n",
      "Epoch: 006/3000 | Batch 0007/0011 | Loss: 3697.3247\n",
      "Epoch: 006/3000 | Batch 0008/0011 | Loss: 3887.6094\n",
      "Epoch: 006/3000 | Batch 0009/0011 | Loss: 4021.1113\n",
      "Epoch: 006/3000 | Batch 0010/0011 | Loss: 3987.2246\n",
      "Time elapsed: 0.00 min\n",
      "Epoch: 007/3000 | Batch 0000/0011 | Loss: 3602.0916\n",
      "Epoch: 007/3000 | Batch 0001/0011 | Loss: 4022.3901\n",
      "Epoch: 007/3000 | Batch 0002/0011 | Loss: 3751.6626\n",
      "Epoch: 007/3000 | Batch 0003/0011 | Loss: 3636.6797\n",
      "Epoch: 007/3000 | Batch 0004/0011 | Loss: 3711.8601\n",
      "Epoch: 007/3000 | Batch 0005/0011 | Loss: 3866.3347\n",
      "Epoch: 007/3000 | Batch 0006/0011 | Loss: 3939.0911\n",
      "Epoch: 007/3000 | Batch 0007/0011 | Loss: 3701.3887\n",
      "Epoch: 007/3000 | Batch 0008/0011 | Loss: 3686.3926\n",
      "Epoch: 007/3000 | Batch 0009/0011 | Loss: 3637.5649\n",
      "Epoch: 007/3000 | Batch 0010/0011 | Loss: 3359.5647\n",
      "Time elapsed: 0.00 min\n",
      "Epoch: 008/3000 | Batch 0000/0011 | Loss: 3716.1436\n",
      "Epoch: 008/3000 | Batch 0001/0011 | Loss: 3625.7749\n",
      "Epoch: 008/3000 | Batch 0002/0011 | Loss: 3517.0964\n",
      "Epoch: 008/3000 | Batch 0003/0011 | Loss: 3561.9180\n",
      "Epoch: 008/3000 | Batch 0004/0011 | Loss: 3623.5173\n",
      "Epoch: 008/3000 | Batch 0005/0011 | Loss: 3678.8467\n",
      "Epoch: 008/3000 | Batch 0006/0011 | Loss: 3602.7607\n",
      "Epoch: 008/3000 | Batch 0007/0011 | Loss: 3485.5046\n",
      "Epoch: 008/3000 | Batch 0008/0011 | Loss: 3621.3416\n",
      "Epoch: 008/3000 | Batch 0009/0011 | Loss: 3498.7092\n",
      "Epoch: 008/3000 | Batch 0010/0011 | Loss: 3276.1074\n",
      "Time elapsed: 0.00 min\n",
      "Epoch: 009/3000 | Batch 0000/0011 | Loss: 3579.1943\n",
      "Epoch: 009/3000 | Batch 0001/0011 | Loss: 3634.0134\n",
      "Epoch: 009/3000 | Batch 0002/0011 | Loss: 3551.1096\n",
      "Epoch: 009/3000 | Batch 0003/0011 | Loss: 3528.9492\n",
      "Epoch: 009/3000 | Batch 0004/0011 | Loss: 3540.5330\n",
      "Epoch: 009/3000 | Batch 0005/0011 | Loss: 3626.9231\n",
      "Epoch: 009/3000 | Batch 0006/0011 | Loss: 3477.3845\n",
      "Epoch: 009/3000 | Batch 0007/0011 | Loss: 3579.7227\n",
      "Epoch: 009/3000 | Batch 0008/0011 | Loss: 3497.1562\n",
      "Epoch: 009/3000 | Batch 0009/0011 | Loss: 3703.9226\n",
      "Epoch: 009/3000 | Batch 0010/0011 | Loss: 3501.1948\n",
      "Time elapsed: 0.00 min\n",
      "Epoch: 010/3000 | Batch 0000/0011 | Loss: 3482.0923\n",
      "Epoch: 010/3000 | Batch 0001/0011 | Loss: 3546.9080\n",
      "Epoch: 010/3000 | Batch 0002/0011 | Loss: 3500.8281\n",
      "Epoch: 010/3000 | Batch 0003/0011 | Loss: 3569.7715\n",
      "Epoch: 010/3000 | Batch 0004/0011 | Loss: 3565.6624\n",
      "Epoch: 010/3000 | Batch 0005/0011 | Loss: 3493.3184\n",
      "Epoch: 010/3000 | Batch 0006/0011 | Loss: 3538.2131\n",
      "Epoch: 010/3000 | Batch 0007/0011 | Loss: 3493.1106\n",
      "Epoch: 010/3000 | Batch 0008/0011 | Loss: 3514.5371\n",
      "Epoch: 010/3000 | Batch 0009/0011 | Loss: 3549.0232\n",
      "Epoch: 010/3000 | Batch 0010/0011 | Loss: 3378.7449\n",
      "Time elapsed: 0.00 min\n",
      "Epoch: 011/3000 | Batch 0000/0011 | Loss: 3562.8606\n",
      "Epoch: 011/3000 | Batch 0001/0011 | Loss: 3482.0425\n",
      "Epoch: 011/3000 | Batch 0002/0011 | Loss: 3602.4614\n",
      "Epoch: 011/3000 | Batch 0003/0011 | Loss: 3521.0432\n",
      "Epoch: 011/3000 | Batch 0004/0011 | Loss: 3510.3831\n",
      "Epoch: 011/3000 | Batch 0005/0011 | Loss: 3583.0952\n",
      "Epoch: 011/3000 | Batch 0006/0011 | Loss: 3491.9302\n",
      "Epoch: 011/3000 | Batch 0007/0011 | Loss: 3414.1689\n",
      "Epoch: 011/3000 | Batch 0008/0011 | Loss: 3480.0300\n",
      "Epoch: 011/3000 | Batch 0009/0011 | Loss: 3583.5735\n",
      "Epoch: 011/3000 | Batch 0010/0011 | Loss: 3469.3516\n",
      "Time elapsed: 0.00 min\n",
      "Epoch: 012/3000 | Batch 0000/0011 | Loss: 3492.6318\n",
      "Epoch: 012/3000 | Batch 0001/0011 | Loss: 3534.5938\n",
      "Epoch: 012/3000 | Batch 0002/0011 | Loss: 3529.3196\n",
      "Epoch: 012/3000 | Batch 0003/0011 | Loss: 3447.0945\n",
      "Epoch: 012/3000 | Batch 0004/0011 | Loss: 3409.6238\n",
      "Epoch: 012/3000 | Batch 0005/0011 | Loss: 3566.8420\n",
      "Epoch: 012/3000 | Batch 0006/0011 | Loss: 3528.4536\n",
      "Epoch: 012/3000 | Batch 0007/0011 | Loss: 3572.2727\n",
      "Epoch: 012/3000 | Batch 0008/0011 | Loss: 3520.8220\n",
      "Epoch: 012/3000 | Batch 0009/0011 | Loss: 3467.1956\n",
      "Epoch: 012/3000 | Batch 0010/0011 | Loss: 3634.5933\n",
      "Time elapsed: 0.00 min\n",
      "Epoch: 013/3000 | Batch 0000/0011 | Loss: 3475.6409\n",
      "Epoch: 013/3000 | Batch 0001/0011 | Loss: 3507.9238\n",
      "Epoch: 013/3000 | Batch 0002/0011 | Loss: 3504.7400\n",
      "Epoch: 013/3000 | Batch 0003/0011 | Loss: 3529.3145\n",
      "Epoch: 013/3000 | Batch 0004/0011 | Loss: 3474.8657\n",
      "Epoch: 013/3000 | Batch 0005/0011 | Loss: 3477.7959\n",
      "Epoch: 013/3000 | Batch 0006/0011 | Loss: 3477.3652\n",
      "Epoch: 013/3000 | Batch 0007/0011 | Loss: 3515.5066\n",
      "Epoch: 013/3000 | Batch 0008/0011 | Loss: 3437.8110\n",
      "Epoch: 013/3000 | Batch 0009/0011 | Loss: 3494.1294\n",
      "Epoch: 013/3000 | Batch 0010/0011 | Loss: 3314.0293\n",
      "Time elapsed: 0.00 min\n",
      "Epoch: 014/3000 | Batch 0000/0011 | Loss: 3624.2427\n",
      "Epoch: 014/3000 | Batch 0001/0011 | Loss: 3533.4639\n",
      "Epoch: 014/3000 | Batch 0002/0011 | Loss: 3426.9158\n",
      "Epoch: 014/3000 | Batch 0003/0011 | Loss: 3481.2866\n",
      "Epoch: 014/3000 | Batch 0004/0011 | Loss: 3455.1958\n",
      "Epoch: 014/3000 | Batch 0005/0011 | Loss: 3478.6990\n",
      "Epoch: 014/3000 | Batch 0006/0011 | Loss: 3451.5486\n",
      "Epoch: 014/3000 | Batch 0007/0011 | Loss: 3523.5437\n",
      "Epoch: 014/3000 | Batch 0008/0011 | Loss: 3379.0151\n",
      "Epoch: 014/3000 | Batch 0009/0011 | Loss: 3449.6191\n",
      "Epoch: 014/3000 | Batch 0010/0011 | Loss: 3456.9126\n",
      "Time elapsed: 0.00 min\n",
      "Epoch: 015/3000 | Batch 0000/0011 | Loss: 3461.9966\n",
      "Epoch: 015/3000 | Batch 0001/0011 | Loss: 3530.9556\n",
      "Epoch: 015/3000 | Batch 0002/0011 | Loss: 3404.6311\n",
      "Epoch: 015/3000 | Batch 0003/0011 | Loss: 3459.3589\n",
      "Epoch: 015/3000 | Batch 0004/0011 | Loss: 3435.1121\n",
      "Epoch: 015/3000 | Batch 0005/0011 | Loss: 3539.1860\n",
      "Epoch: 015/3000 | Batch 0006/0011 | Loss: 3435.9636\n",
      "Epoch: 015/3000 | Batch 0007/0011 | Loss: 3481.8823\n",
      "Epoch: 015/3000 | Batch 0008/0011 | Loss: 3451.9580\n",
      "Epoch: 015/3000 | Batch 0009/0011 | Loss: 3456.3245\n",
      "Epoch: 015/3000 | Batch 0010/0011 | Loss: 3219.6597\n",
      "Time elapsed: 0.00 min\n",
      "Epoch: 016/3000 | Batch 0000/0011 | Loss: 3395.2158\n",
      "Epoch: 016/3000 | Batch 0001/0011 | Loss: 3533.3699\n",
      "Epoch: 016/3000 | Batch 0002/0011 | Loss: 3521.5720\n",
      "Epoch: 016/3000 | Batch 0003/0011 | Loss: 3452.8125\n",
      "Epoch: 016/3000 | Batch 0004/0011 | Loss: 3445.8533\n",
      "Epoch: 016/3000 | Batch 0005/0011 | Loss: 3414.3364\n",
      "Epoch: 016/3000 | Batch 0006/0011 | Loss: 3431.4678\n",
      "Epoch: 016/3000 | Batch 0007/0011 | Loss: 3446.6079\n",
      "Epoch: 016/3000 | Batch 0008/0011 | Loss: 3441.8901\n",
      "Epoch: 016/3000 | Batch 0009/0011 | Loss: 3456.4573\n",
      "Epoch: 016/3000 | Batch 0010/0011 | Loss: 3432.3496\n",
      "Time elapsed: 0.00 min\n",
      "Epoch: 017/3000 | Batch 0000/0011 | Loss: 3505.1572\n",
      "Epoch: 017/3000 | Batch 0001/0011 | Loss: 3386.8511\n",
      "Epoch: 017/3000 | Batch 0002/0011 | Loss: 3457.1746\n",
      "Epoch: 017/3000 | Batch 0003/0011 | Loss: 3520.3237\n",
      "Epoch: 017/3000 | Batch 0004/0011 | Loss: 3460.1133\n",
      "Epoch: 017/3000 | Batch 0005/0011 | Loss: 3423.7874\n",
      "Epoch: 017/3000 | Batch 0006/0011 | Loss: 3450.9968\n",
      "Epoch: 017/3000 | Batch 0007/0011 | Loss: 3447.4868\n",
      "Epoch: 017/3000 | Batch 0008/0011 | Loss: 3506.7681\n",
      "Epoch: 017/3000 | Batch 0009/0011 | Loss: 3393.3057\n",
      "Epoch: 017/3000 | Batch 0010/0011 | Loss: 3616.2139\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 018/3000 | Batch 0000/0011 | Loss: 3345.3945\n",
      "Epoch: 018/3000 | Batch 0001/0011 | Loss: 3428.2358\n",
      "Epoch: 018/3000 | Batch 0002/0011 | Loss: 3479.4319\n",
      "Epoch: 018/3000 | Batch 0003/0011 | Loss: 3475.0605\n",
      "Epoch: 018/3000 | Batch 0004/0011 | Loss: 3415.7219\n",
      "Epoch: 018/3000 | Batch 0005/0011 | Loss: 3529.2466\n",
      "Epoch: 018/3000 | Batch 0006/0011 | Loss: 3463.4756\n",
      "Epoch: 018/3000 | Batch 0007/0011 | Loss: 3422.5659\n",
      "Epoch: 018/3000 | Batch 0008/0011 | Loss: 3425.4014\n",
      "Epoch: 018/3000 | Batch 0009/0011 | Loss: 3468.6887\n",
      "Epoch: 018/3000 | Batch 0010/0011 | Loss: 3482.3235\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 019/3000 | Batch 0000/0011 | Loss: 3452.6912\n",
      "Epoch: 019/3000 | Batch 0001/0011 | Loss: 3434.0991\n",
      "Epoch: 019/3000 | Batch 0002/0011 | Loss: 3337.9817\n",
      "Epoch: 019/3000 | Batch 0003/0011 | Loss: 3436.8433\n",
      "Epoch: 019/3000 | Batch 0004/0011 | Loss: 3409.6035\n",
      "Epoch: 019/3000 | Batch 0005/0011 | Loss: 3424.9106\n",
      "Epoch: 019/3000 | Batch 0006/0011 | Loss: 3488.9319\n",
      "Epoch: 019/3000 | Batch 0007/0011 | Loss: 3411.0742\n",
      "Epoch: 019/3000 | Batch 0008/0011 | Loss: 3385.1575\n",
      "Epoch: 019/3000 | Batch 0009/0011 | Loss: 3503.4719\n",
      "Epoch: 019/3000 | Batch 0010/0011 | Loss: 3524.3396\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 020/3000 | Batch 0000/0011 | Loss: 3377.4302\n",
      "Epoch: 020/3000 | Batch 0001/0011 | Loss: 3437.6165\n",
      "Epoch: 020/3000 | Batch 0002/0011 | Loss: 3421.1431\n",
      "Epoch: 020/3000 | Batch 0003/0011 | Loss: 3409.7007\n",
      "Epoch: 020/3000 | Batch 0004/0011 | Loss: 3366.8499\n",
      "Epoch: 020/3000 | Batch 0005/0011 | Loss: 3390.2866\n",
      "Epoch: 020/3000 | Batch 0006/0011 | Loss: 3411.7732\n",
      "Epoch: 020/3000 | Batch 0007/0011 | Loss: 3388.5503\n",
      "Epoch: 020/3000 | Batch 0008/0011 | Loss: 3386.0408\n",
      "Epoch: 020/3000 | Batch 0009/0011 | Loss: 3506.2029\n",
      "Epoch: 020/3000 | Batch 0010/0011 | Loss: 3410.1294\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 021/3000 | Batch 0000/0011 | Loss: 3404.7671\n",
      "Epoch: 021/3000 | Batch 0001/0011 | Loss: 3348.7080\n",
      "Epoch: 021/3000 | Batch 0002/0011 | Loss: 3488.3149\n",
      "Epoch: 021/3000 | Batch 0003/0011 | Loss: 3374.9592\n",
      "Epoch: 021/3000 | Batch 0004/0011 | Loss: 3552.4651\n",
      "Epoch: 021/3000 | Batch 0005/0011 | Loss: 3410.1040\n",
      "Epoch: 021/3000 | Batch 0006/0011 | Loss: 3378.6792\n",
      "Epoch: 021/3000 | Batch 0007/0011 | Loss: 3426.0500\n",
      "Epoch: 021/3000 | Batch 0008/0011 | Loss: 3376.0833\n",
      "Epoch: 021/3000 | Batch 0009/0011 | Loss: 3416.3586\n",
      "Epoch: 021/3000 | Batch 0010/0011 | Loss: 3346.7622\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 022/3000 | Batch 0000/0011 | Loss: 3381.5410\n",
      "Epoch: 022/3000 | Batch 0001/0011 | Loss: 3345.5508\n",
      "Epoch: 022/3000 | Batch 0002/0011 | Loss: 3471.7988\n",
      "Epoch: 022/3000 | Batch 0003/0011 | Loss: 3344.4214\n",
      "Epoch: 022/3000 | Batch 0004/0011 | Loss: 3362.3167\n",
      "Epoch: 022/3000 | Batch 0005/0011 | Loss: 3347.4670\n",
      "Epoch: 022/3000 | Batch 0006/0011 | Loss: 3388.5271\n",
      "Epoch: 022/3000 | Batch 0007/0011 | Loss: 3349.2397\n",
      "Epoch: 022/3000 | Batch 0008/0011 | Loss: 3475.4395\n",
      "Epoch: 022/3000 | Batch 0009/0011 | Loss: 3512.4731\n",
      "Epoch: 022/3000 | Batch 0010/0011 | Loss: 3291.0767\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 023/3000 | Batch 0000/0011 | Loss: 3411.8716\n",
      "Epoch: 023/3000 | Batch 0001/0011 | Loss: 3399.6655\n",
      "Epoch: 023/3000 | Batch 0002/0011 | Loss: 3375.7883\n",
      "Epoch: 023/3000 | Batch 0003/0011 | Loss: 3335.8489\n",
      "Epoch: 023/3000 | Batch 0004/0011 | Loss: 3381.6941\n",
      "Epoch: 023/3000 | Batch 0005/0011 | Loss: 3411.0496\n",
      "Epoch: 023/3000 | Batch 0006/0011 | Loss: 3494.7886\n",
      "Epoch: 023/3000 | Batch 0007/0011 | Loss: 3324.8662\n",
      "Epoch: 023/3000 | Batch 0008/0011 | Loss: 3395.7344\n",
      "Epoch: 023/3000 | Batch 0009/0011 | Loss: 3465.1174\n",
      "Epoch: 023/3000 | Batch 0010/0011 | Loss: 3543.3281\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 024/3000 | Batch 0000/0011 | Loss: 3413.2781\n",
      "Epoch: 024/3000 | Batch 0001/0011 | Loss: 3395.8550\n",
      "Epoch: 024/3000 | Batch 0002/0011 | Loss: 3440.5444\n",
      "Epoch: 024/3000 | Batch 0003/0011 | Loss: 3408.7290\n",
      "Epoch: 024/3000 | Batch 0004/0011 | Loss: 3343.3840\n",
      "Epoch: 024/3000 | Batch 0005/0011 | Loss: 3325.9490\n",
      "Epoch: 024/3000 | Batch 0006/0011 | Loss: 3455.3669\n",
      "Epoch: 024/3000 | Batch 0007/0011 | Loss: 3416.0081\n",
      "Epoch: 024/3000 | Batch 0008/0011 | Loss: 3432.3774\n",
      "Epoch: 024/3000 | Batch 0009/0011 | Loss: 3328.7085\n",
      "Epoch: 024/3000 | Batch 0010/0011 | Loss: 3461.0586\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 025/3000 | Batch 0000/0011 | Loss: 3332.6528\n",
      "Epoch: 025/3000 | Batch 0001/0011 | Loss: 3379.3132\n",
      "Epoch: 025/3000 | Batch 0002/0011 | Loss: 3316.0923\n",
      "Epoch: 025/3000 | Batch 0003/0011 | Loss: 3424.8953\n",
      "Epoch: 025/3000 | Batch 0004/0011 | Loss: 3429.4873\n",
      "Epoch: 025/3000 | Batch 0005/0011 | Loss: 3346.5522\n",
      "Epoch: 025/3000 | Batch 0006/0011 | Loss: 3378.2615\n",
      "Epoch: 025/3000 | Batch 0007/0011 | Loss: 3402.7390\n",
      "Epoch: 025/3000 | Batch 0008/0011 | Loss: 3433.3787\n",
      "Epoch: 025/3000 | Batch 0009/0011 | Loss: 3338.9585\n",
      "Epoch: 025/3000 | Batch 0010/0011 | Loss: 3269.9182\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 026/3000 | Batch 0000/0011 | Loss: 3342.4578\n",
      "Epoch: 026/3000 | Batch 0001/0011 | Loss: 3370.7729\n",
      "Epoch: 026/3000 | Batch 0002/0011 | Loss: 3492.9561\n",
      "Epoch: 026/3000 | Batch 0003/0011 | Loss: 3334.3875\n",
      "Epoch: 026/3000 | Batch 0004/0011 | Loss: 3326.3762\n",
      "Epoch: 026/3000 | Batch 0005/0011 | Loss: 3296.2539\n",
      "Epoch: 026/3000 | Batch 0006/0011 | Loss: 3309.1436\n",
      "Epoch: 026/3000 | Batch 0007/0011 | Loss: 3290.3403\n",
      "Epoch: 026/3000 | Batch 0008/0011 | Loss: 3347.0183\n",
      "Epoch: 026/3000 | Batch 0009/0011 | Loss: 3461.9697\n",
      "Epoch: 026/3000 | Batch 0010/0011 | Loss: 3249.2617\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 027/3000 | Batch 0000/0011 | Loss: 3335.2292\n",
      "Epoch: 027/3000 | Batch 0001/0011 | Loss: 3308.7222\n",
      "Epoch: 027/3000 | Batch 0002/0011 | Loss: 3391.7605\n",
      "Epoch: 027/3000 | Batch 0003/0011 | Loss: 3328.3303\n",
      "Epoch: 027/3000 | Batch 0004/0011 | Loss: 3328.9004\n",
      "Epoch: 027/3000 | Batch 0005/0011 | Loss: 3367.5654\n",
      "Epoch: 027/3000 | Batch 0006/0011 | Loss: 3268.7568\n",
      "Epoch: 027/3000 | Batch 0007/0011 | Loss: 3362.9255\n",
      "Epoch: 027/3000 | Batch 0008/0011 | Loss: 3415.8044\n",
      "Epoch: 027/3000 | Batch 0009/0011 | Loss: 3329.7629\n",
      "Epoch: 027/3000 | Batch 0010/0011 | Loss: 3251.2710\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 028/3000 | Batch 0000/0011 | Loss: 3288.0579\n",
      "Epoch: 028/3000 | Batch 0001/0011 | Loss: 3281.0840\n",
      "Epoch: 028/3000 | Batch 0002/0011 | Loss: 3331.9421\n",
      "Epoch: 028/3000 | Batch 0003/0011 | Loss: 3382.2725\n",
      "Epoch: 028/3000 | Batch 0004/0011 | Loss: 3354.0884\n",
      "Epoch: 028/3000 | Batch 0005/0011 | Loss: 3314.5845\n",
      "Epoch: 028/3000 | Batch 0006/0011 | Loss: 3326.0505\n",
      "Epoch: 028/3000 | Batch 0007/0011 | Loss: 3323.6343\n",
      "Epoch: 028/3000 | Batch 0008/0011 | Loss: 3401.4819\n",
      "Epoch: 028/3000 | Batch 0009/0011 | Loss: 3270.1516\n",
      "Epoch: 028/3000 | Batch 0010/0011 | Loss: 3293.1277\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 029/3000 | Batch 0000/0011 | Loss: 3304.8384\n",
      "Epoch: 029/3000 | Batch 0001/0011 | Loss: 3334.1582\n",
      "Epoch: 029/3000 | Batch 0002/0011 | Loss: 3309.0247\n",
      "Epoch: 029/3000 | Batch 0003/0011 | Loss: 3295.4111\n",
      "Epoch: 029/3000 | Batch 0004/0011 | Loss: 3267.5635\n",
      "Epoch: 029/3000 | Batch 0005/0011 | Loss: 3421.6714\n",
      "Epoch: 029/3000 | Batch 0006/0011 | Loss: 3345.7153\n",
      "Epoch: 029/3000 | Batch 0007/0011 | Loss: 3309.8982\n",
      "Epoch: 029/3000 | Batch 0008/0011 | Loss: 3218.0271\n",
      "Epoch: 029/3000 | Batch 0009/0011 | Loss: 3230.5432\n",
      "Epoch: 029/3000 | Batch 0010/0011 | Loss: 3402.4487\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 030/3000 | Batch 0000/0011 | Loss: 3245.7312\n",
      "Epoch: 030/3000 | Batch 0001/0011 | Loss: 3291.8882\n",
      "Epoch: 030/3000 | Batch 0002/0011 | Loss: 3376.7539\n",
      "Epoch: 030/3000 | Batch 0003/0011 | Loss: 3256.3298\n",
      "Epoch: 030/3000 | Batch 0004/0011 | Loss: 3243.3042\n",
      "Epoch: 030/3000 | Batch 0005/0011 | Loss: 3274.5059\n",
      "Epoch: 030/3000 | Batch 0006/0011 | Loss: 3298.4561\n",
      "Epoch: 030/3000 | Batch 0007/0011 | Loss: 3317.0803\n",
      "Epoch: 030/3000 | Batch 0008/0011 | Loss: 3286.6453\n",
      "Epoch: 030/3000 | Batch 0009/0011 | Loss: 3275.2068\n",
      "Epoch: 030/3000 | Batch 0010/0011 | Loss: 3355.9946\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 031/3000 | Batch 0000/0011 | Loss: 3231.4568\n",
      "Epoch: 031/3000 | Batch 0001/0011 | Loss: 3267.4995\n",
      "Epoch: 031/3000 | Batch 0002/0011 | Loss: 3335.1108\n",
      "Epoch: 031/3000 | Batch 0003/0011 | Loss: 3278.5449\n",
      "Epoch: 031/3000 | Batch 0004/0011 | Loss: 3301.7158\n",
      "Epoch: 031/3000 | Batch 0005/0011 | Loss: 3241.0608\n",
      "Epoch: 031/3000 | Batch 0006/0011 | Loss: 3439.6111\n",
      "Epoch: 031/3000 | Batch 0007/0011 | Loss: 3291.6028\n",
      "Epoch: 031/3000 | Batch 0008/0011 | Loss: 3264.1587\n",
      "Epoch: 031/3000 | Batch 0009/0011 | Loss: 3240.5793\n",
      "Epoch: 031/3000 | Batch 0010/0011 | Loss: 3200.1311\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 032/3000 | Batch 0000/0011 | Loss: 3226.8364\n",
      "Epoch: 032/3000 | Batch 0001/0011 | Loss: 3333.8953\n",
      "Epoch: 032/3000 | Batch 0002/0011 | Loss: 3317.7166\n",
      "Epoch: 032/3000 | Batch 0003/0011 | Loss: 3420.2935\n",
      "Epoch: 032/3000 | Batch 0004/0011 | Loss: 3359.3630\n",
      "Epoch: 032/3000 | Batch 0005/0011 | Loss: 3244.3662\n",
      "Epoch: 032/3000 | Batch 0006/0011 | Loss: 3242.8569\n",
      "Epoch: 032/3000 | Batch 0007/0011 | Loss: 3252.3323\n",
      "Epoch: 032/3000 | Batch 0008/0011 | Loss: 3312.5537\n",
      "Epoch: 032/3000 | Batch 0009/0011 | Loss: 3233.6438\n",
      "Epoch: 032/3000 | Batch 0010/0011 | Loss: 3228.7578\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 033/3000 | Batch 0000/0011 | Loss: 3330.7039\n",
      "Epoch: 033/3000 | Batch 0001/0011 | Loss: 3310.8542\n",
      "Epoch: 033/3000 | Batch 0002/0011 | Loss: 3315.7244\n",
      "Epoch: 033/3000 | Batch 0003/0011 | Loss: 3279.5825\n",
      "Epoch: 033/3000 | Batch 0004/0011 | Loss: 3290.0894\n",
      "Epoch: 033/3000 | Batch 0005/0011 | Loss: 3318.4810\n",
      "Epoch: 033/3000 | Batch 0006/0011 | Loss: 3276.3110\n",
      "Epoch: 033/3000 | Batch 0007/0011 | Loss: 3227.4282\n",
      "Epoch: 033/3000 | Batch 0008/0011 | Loss: 3274.6531\n",
      "Epoch: 033/3000 | Batch 0009/0011 | Loss: 3244.6541\n",
      "Epoch: 033/3000 | Batch 0010/0011 | Loss: 3126.5222\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 034/3000 | Batch 0000/0011 | Loss: 3272.9075\n",
      "Epoch: 034/3000 | Batch 0001/0011 | Loss: 3278.8669\n",
      "Epoch: 034/3000 | Batch 0002/0011 | Loss: 3232.0298\n",
      "Epoch: 034/3000 | Batch 0003/0011 | Loss: 3301.0222\n",
      "Epoch: 034/3000 | Batch 0004/0011 | Loss: 3209.5864\n",
      "Epoch: 034/3000 | Batch 0005/0011 | Loss: 3299.5110\n",
      "Epoch: 034/3000 | Batch 0006/0011 | Loss: 3301.5002\n",
      "Epoch: 034/3000 | Batch 0007/0011 | Loss: 3278.9507\n",
      "Epoch: 034/3000 | Batch 0008/0011 | Loss: 3289.2988\n",
      "Epoch: 034/3000 | Batch 0009/0011 | Loss: 3365.0542\n",
      "Epoch: 034/3000 | Batch 0010/0011 | Loss: 3221.5845\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 035/3000 | Batch 0000/0011 | Loss: 3291.6907\n",
      "Epoch: 035/3000 | Batch 0001/0011 | Loss: 3280.8672\n",
      "Epoch: 035/3000 | Batch 0002/0011 | Loss: 3260.5078\n",
      "Epoch: 035/3000 | Batch 0003/0011 | Loss: 3234.0315\n",
      "Epoch: 035/3000 | Batch 0004/0011 | Loss: 3315.7695\n",
      "Epoch: 035/3000 | Batch 0005/0011 | Loss: 3258.3398\n",
      "Epoch: 035/3000 | Batch 0006/0011 | Loss: 3291.1997\n",
      "Epoch: 035/3000 | Batch 0007/0011 | Loss: 3269.1377\n",
      "Epoch: 035/3000 | Batch 0008/0011 | Loss: 3281.9832\n",
      "Epoch: 035/3000 | Batch 0009/0011 | Loss: 3253.6423\n",
      "Epoch: 035/3000 | Batch 0010/0011 | Loss: 3296.5129\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 036/3000 | Batch 0000/0011 | Loss: 3251.4172\n",
      "Epoch: 036/3000 | Batch 0001/0011 | Loss: 3263.5632\n",
      "Epoch: 036/3000 | Batch 0002/0011 | Loss: 3328.4614\n",
      "Epoch: 036/3000 | Batch 0003/0011 | Loss: 3263.2271\n",
      "Epoch: 036/3000 | Batch 0004/0011 | Loss: 3297.4531\n",
      "Epoch: 036/3000 | Batch 0005/0011 | Loss: 3247.9883\n",
      "Epoch: 036/3000 | Batch 0006/0011 | Loss: 3322.7957\n",
      "Epoch: 036/3000 | Batch 0007/0011 | Loss: 3264.3484\n",
      "Epoch: 036/3000 | Batch 0008/0011 | Loss: 3342.0325\n",
      "Epoch: 036/3000 | Batch 0009/0011 | Loss: 3244.2510\n",
      "Epoch: 036/3000 | Batch 0010/0011 | Loss: 3287.6750\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 037/3000 | Batch 0000/0011 | Loss: 3239.1997\n",
      "Epoch: 037/3000 | Batch 0001/0011 | Loss: 3196.1873\n",
      "Epoch: 037/3000 | Batch 0002/0011 | Loss: 3330.9250\n",
      "Epoch: 037/3000 | Batch 0003/0011 | Loss: 3309.0234\n",
      "Epoch: 037/3000 | Batch 0004/0011 | Loss: 3296.0779\n",
      "Epoch: 037/3000 | Batch 0005/0011 | Loss: 3256.9109\n",
      "Epoch: 037/3000 | Batch 0006/0011 | Loss: 3313.5442\n",
      "Epoch: 037/3000 | Batch 0007/0011 | Loss: 3201.1975\n",
      "Epoch: 037/3000 | Batch 0008/0011 | Loss: 3288.0400\n",
      "Epoch: 037/3000 | Batch 0009/0011 | Loss: 3269.7700\n",
      "Epoch: 037/3000 | Batch 0010/0011 | Loss: 3354.6895\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 038/3000 | Batch 0000/0011 | Loss: 3237.0193\n",
      "Epoch: 038/3000 | Batch 0001/0011 | Loss: 3201.9568\n",
      "Epoch: 038/3000 | Batch 0002/0011 | Loss: 3231.8643\n",
      "Epoch: 038/3000 | Batch 0003/0011 | Loss: 3226.2039\n",
      "Epoch: 038/3000 | Batch 0004/0011 | Loss: 3247.6802\n",
      "Epoch: 038/3000 | Batch 0005/0011 | Loss: 3289.0823\n",
      "Epoch: 038/3000 | Batch 0006/0011 | Loss: 3255.9670\n",
      "Epoch: 038/3000 | Batch 0007/0011 | Loss: 3324.5166\n",
      "Epoch: 038/3000 | Batch 0008/0011 | Loss: 3299.1958\n",
      "Epoch: 038/3000 | Batch 0009/0011 | Loss: 3236.4346\n",
      "Epoch: 038/3000 | Batch 0010/0011 | Loss: 3331.2512\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 039/3000 | Batch 0000/0011 | Loss: 3216.0283\n",
      "Epoch: 039/3000 | Batch 0001/0011 | Loss: 3395.5845\n",
      "Epoch: 039/3000 | Batch 0002/0011 | Loss: 3265.8892\n",
      "Epoch: 039/3000 | Batch 0003/0011 | Loss: 3268.9607\n",
      "Epoch: 039/3000 | Batch 0004/0011 | Loss: 3153.9829\n",
      "Epoch: 039/3000 | Batch 0005/0011 | Loss: 3357.4863\n",
      "Epoch: 039/3000 | Batch 0006/0011 | Loss: 3361.2563\n",
      "Epoch: 039/3000 | Batch 0007/0011 | Loss: 3336.3074\n",
      "Epoch: 039/3000 | Batch 0008/0011 | Loss: 3273.9856\n",
      "Epoch: 039/3000 | Batch 0009/0011 | Loss: 3204.5159\n",
      "Epoch: 039/3000 | Batch 0010/0011 | Loss: 3129.9590\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 040/3000 | Batch 0000/0011 | Loss: 3245.8281\n",
      "Epoch: 040/3000 | Batch 0001/0011 | Loss: 3279.5210\n",
      "Epoch: 040/3000 | Batch 0002/0011 | Loss: 3277.7058\n",
      "Epoch: 040/3000 | Batch 0003/0011 | Loss: 3210.0315\n",
      "Epoch: 040/3000 | Batch 0004/0011 | Loss: 3285.9673\n",
      "Epoch: 040/3000 | Batch 0005/0011 | Loss: 3206.9270\n",
      "Epoch: 040/3000 | Batch 0006/0011 | Loss: 3201.5876\n",
      "Epoch: 040/3000 | Batch 0007/0011 | Loss: 3258.6384\n",
      "Epoch: 040/3000 | Batch 0008/0011 | Loss: 3272.3127\n",
      "Epoch: 040/3000 | Batch 0009/0011 | Loss: 3243.8784\n",
      "Epoch: 040/3000 | Batch 0010/0011 | Loss: 3526.8958\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 041/3000 | Batch 0000/0011 | Loss: 3262.4119\n",
      "Epoch: 041/3000 | Batch 0001/0011 | Loss: 3311.3669\n",
      "Epoch: 041/3000 | Batch 0002/0011 | Loss: 3241.8071\n",
      "Epoch: 041/3000 | Batch 0003/0011 | Loss: 3227.8342\n",
      "Epoch: 041/3000 | Batch 0004/0011 | Loss: 3248.8918\n",
      "Epoch: 041/3000 | Batch 0005/0011 | Loss: 3292.7417\n",
      "Epoch: 041/3000 | Batch 0006/0011 | Loss: 3222.9573\n",
      "Epoch: 041/3000 | Batch 0007/0011 | Loss: 3257.1792\n",
      "Epoch: 041/3000 | Batch 0008/0011 | Loss: 3195.7686\n",
      "Epoch: 041/3000 | Batch 0009/0011 | Loss: 3224.2715\n",
      "Epoch: 041/3000 | Batch 0010/0011 | Loss: 3200.9043\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 042/3000 | Batch 0000/0011 | Loss: 3311.9919\n",
      "Epoch: 042/3000 | Batch 0001/0011 | Loss: 3203.2974\n",
      "Epoch: 042/3000 | Batch 0002/0011 | Loss: 3254.0208\n",
      "Epoch: 042/3000 | Batch 0003/0011 | Loss: 3192.1721\n",
      "Epoch: 042/3000 | Batch 0004/0011 | Loss: 3191.0002\n",
      "Epoch: 042/3000 | Batch 0005/0011 | Loss: 3304.3435\n",
      "Epoch: 042/3000 | Batch 0006/0011 | Loss: 3321.6833\n",
      "Epoch: 042/3000 | Batch 0007/0011 | Loss: 3229.0425\n",
      "Epoch: 042/3000 | Batch 0008/0011 | Loss: 3254.1506\n",
      "Epoch: 042/3000 | Batch 0009/0011 | Loss: 3230.2556\n",
      "Epoch: 042/3000 | Batch 0010/0011 | Loss: 3241.1609\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 043/3000 | Batch 0000/0011 | Loss: 3172.8418\n",
      "Epoch: 043/3000 | Batch 0001/0011 | Loss: 3360.3213\n",
      "Epoch: 043/3000 | Batch 0002/0011 | Loss: 3302.9641\n",
      "Epoch: 043/3000 | Batch 0003/0011 | Loss: 3240.1338\n",
      "Epoch: 043/3000 | Batch 0004/0011 | Loss: 3245.2180\n",
      "Epoch: 043/3000 | Batch 0005/0011 | Loss: 3263.9980\n",
      "Epoch: 043/3000 | Batch 0006/0011 | Loss: 3195.5574\n",
      "Epoch: 043/3000 | Batch 0007/0011 | Loss: 3295.7158\n",
      "Epoch: 043/3000 | Batch 0008/0011 | Loss: 3245.7510\n",
      "Epoch: 043/3000 | Batch 0009/0011 | Loss: 3233.6716\n",
      "Epoch: 043/3000 | Batch 0010/0011 | Loss: 3274.2317\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 044/3000 | Batch 0000/0011 | Loss: 3204.9541\n",
      "Epoch: 044/3000 | Batch 0001/0011 | Loss: 3227.0588\n",
      "Epoch: 044/3000 | Batch 0002/0011 | Loss: 3346.5498\n",
      "Epoch: 044/3000 | Batch 0003/0011 | Loss: 3233.3940\n",
      "Epoch: 044/3000 | Batch 0004/0011 | Loss: 3243.5488\n",
      "Epoch: 044/3000 | Batch 0005/0011 | Loss: 3214.6289\n",
      "Epoch: 044/3000 | Batch 0006/0011 | Loss: 3207.4817\n",
      "Epoch: 044/3000 | Batch 0007/0011 | Loss: 3279.2004\n",
      "Epoch: 044/3000 | Batch 0008/0011 | Loss: 3242.0310\n",
      "Epoch: 044/3000 | Batch 0009/0011 | Loss: 3244.4763\n",
      "Epoch: 044/3000 | Batch 0010/0011 | Loss: 3282.1733\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 045/3000 | Batch 0000/0011 | Loss: 3267.5093\n",
      "Epoch: 045/3000 | Batch 0001/0011 | Loss: 3253.0454\n",
      "Epoch: 045/3000 | Batch 0002/0011 | Loss: 3253.1338\n",
      "Epoch: 045/3000 | Batch 0003/0011 | Loss: 3302.1177\n",
      "Epoch: 045/3000 | Batch 0004/0011 | Loss: 3179.7708\n",
      "Epoch: 045/3000 | Batch 0005/0011 | Loss: 3294.4561\n",
      "Epoch: 045/3000 | Batch 0006/0011 | Loss: 3293.6440\n",
      "Epoch: 045/3000 | Batch 0007/0011 | Loss: 3246.8733\n",
      "Epoch: 045/3000 | Batch 0008/0011 | Loss: 3208.5112\n",
      "Epoch: 045/3000 | Batch 0009/0011 | Loss: 3283.8730\n",
      "Epoch: 045/3000 | Batch 0010/0011 | Loss: 3047.5334\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 046/3000 | Batch 0000/0011 | Loss: 3183.3687\n",
      "Epoch: 046/3000 | Batch 0001/0011 | Loss: 3321.9260\n",
      "Epoch: 046/3000 | Batch 0002/0011 | Loss: 3251.6277\n",
      "Epoch: 046/3000 | Batch 0003/0011 | Loss: 3234.1946\n",
      "Epoch: 046/3000 | Batch 0004/0011 | Loss: 3248.9519\n",
      "Epoch: 046/3000 | Batch 0005/0011 | Loss: 3245.2788\n",
      "Epoch: 046/3000 | Batch 0006/0011 | Loss: 3240.0955\n",
      "Epoch: 046/3000 | Batch 0007/0011 | Loss: 3281.8462\n",
      "Epoch: 046/3000 | Batch 0008/0011 | Loss: 3256.0457\n",
      "Epoch: 046/3000 | Batch 0009/0011 | Loss: 3172.3071\n",
      "Epoch: 046/3000 | Batch 0010/0011 | Loss: 3318.6995\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 047/3000 | Batch 0000/0011 | Loss: 3243.4299\n",
      "Epoch: 047/3000 | Batch 0001/0011 | Loss: 3220.4915\n",
      "Epoch: 047/3000 | Batch 0002/0011 | Loss: 3263.9565\n",
      "Epoch: 047/3000 | Batch 0003/0011 | Loss: 3221.8699\n",
      "Epoch: 047/3000 | Batch 0004/0011 | Loss: 3247.9507\n",
      "Epoch: 047/3000 | Batch 0005/0011 | Loss: 3227.2542\n",
      "Epoch: 047/3000 | Batch 0006/0011 | Loss: 3170.1016\n",
      "Epoch: 047/3000 | Batch 0007/0011 | Loss: 3244.8916\n",
      "Epoch: 047/3000 | Batch 0008/0011 | Loss: 3297.2026\n",
      "Epoch: 047/3000 | Batch 0009/0011 | Loss: 3309.6938\n",
      "Epoch: 047/3000 | Batch 0010/0011 | Loss: 3468.2124\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 048/3000 | Batch 0000/0011 | Loss: 3142.1914\n",
      "Epoch: 048/3000 | Batch 0001/0011 | Loss: 3240.4062\n",
      "Epoch: 048/3000 | Batch 0002/0011 | Loss: 3269.3171\n",
      "Epoch: 048/3000 | Batch 0003/0011 | Loss: 3256.0742\n",
      "Epoch: 048/3000 | Batch 0004/0011 | Loss: 3216.3091\n",
      "Epoch: 048/3000 | Batch 0005/0011 | Loss: 3226.9460\n",
      "Epoch: 048/3000 | Batch 0006/0011 | Loss: 3201.6572\n",
      "Epoch: 048/3000 | Batch 0007/0011 | Loss: 3277.6533\n",
      "Epoch: 048/3000 | Batch 0008/0011 | Loss: 3262.6689\n",
      "Epoch: 048/3000 | Batch 0009/0011 | Loss: 3196.7910\n",
      "Epoch: 048/3000 | Batch 0010/0011 | Loss: 3441.0212\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 049/3000 | Batch 0000/0011 | Loss: 3256.8630\n",
      "Epoch: 049/3000 | Batch 0001/0011 | Loss: 3182.3284\n",
      "Epoch: 049/3000 | Batch 0002/0011 | Loss: 3186.2803\n",
      "Epoch: 049/3000 | Batch 0003/0011 | Loss: 3269.0159\n",
      "Epoch: 049/3000 | Batch 0004/0011 | Loss: 3281.0071\n",
      "Epoch: 049/3000 | Batch 0005/0011 | Loss: 3267.2002\n",
      "Epoch: 049/3000 | Batch 0006/0011 | Loss: 3337.6980\n",
      "Epoch: 049/3000 | Batch 0007/0011 | Loss: 3234.9824\n",
      "Epoch: 049/3000 | Batch 0008/0011 | Loss: 3205.1990\n",
      "Epoch: 049/3000 | Batch 0009/0011 | Loss: 3197.6113\n",
      "Epoch: 049/3000 | Batch 0010/0011 | Loss: 3224.9370\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 050/3000 | Batch 0000/0011 | Loss: 3378.9939\n",
      "Epoch: 050/3000 | Batch 0001/0011 | Loss: 3214.7942\n",
      "Epoch: 050/3000 | Batch 0002/0011 | Loss: 3264.7854\n",
      "Epoch: 050/3000 | Batch 0003/0011 | Loss: 3216.8076\n",
      "Epoch: 050/3000 | Batch 0004/0011 | Loss: 3203.1558\n",
      "Epoch: 050/3000 | Batch 0005/0011 | Loss: 3224.0596\n",
      "Epoch: 050/3000 | Batch 0006/0011 | Loss: 3302.7427\n",
      "Epoch: 050/3000 | Batch 0007/0011 | Loss: 3174.7214\n",
      "Epoch: 050/3000 | Batch 0008/0011 | Loss: 3180.5967\n",
      "Epoch: 050/3000 | Batch 0009/0011 | Loss: 3227.0388\n",
      "Epoch: 050/3000 | Batch 0010/0011 | Loss: 3198.6978\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 051/3000 | Batch 0000/0011 | Loss: 3160.1692\n",
      "Epoch: 051/3000 | Batch 0001/0011 | Loss: 3316.4663\n",
      "Epoch: 051/3000 | Batch 0002/0011 | Loss: 3255.2253\n",
      "Epoch: 051/3000 | Batch 0003/0011 | Loss: 3274.9138\n",
      "Epoch: 051/3000 | Batch 0004/0011 | Loss: 3292.4739\n",
      "Epoch: 051/3000 | Batch 0005/0011 | Loss: 3225.8804\n",
      "Epoch: 051/3000 | Batch 0006/0011 | Loss: 3320.3943\n",
      "Epoch: 051/3000 | Batch 0007/0011 | Loss: 3254.1750\n",
      "Epoch: 051/3000 | Batch 0008/0011 | Loss: 3232.9014\n",
      "Epoch: 051/3000 | Batch 0009/0011 | Loss: 3235.4312\n",
      "Epoch: 051/3000 | Batch 0010/0011 | Loss: 3191.6069\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 052/3000 | Batch 0000/0011 | Loss: 3206.0237\n",
      "Epoch: 052/3000 | Batch 0001/0011 | Loss: 3191.3594\n",
      "Epoch: 052/3000 | Batch 0002/0011 | Loss: 3324.8384\n",
      "Epoch: 052/3000 | Batch 0003/0011 | Loss: 3355.0388\n",
      "Epoch: 052/3000 | Batch 0004/0011 | Loss: 3214.9451\n",
      "Epoch: 052/3000 | Batch 0005/0011 | Loss: 3190.2373\n",
      "Epoch: 052/3000 | Batch 0006/0011 | Loss: 3216.5815\n",
      "Epoch: 052/3000 | Batch 0007/0011 | Loss: 3277.8252\n",
      "Epoch: 052/3000 | Batch 0008/0011 | Loss: 3200.7444\n",
      "Epoch: 052/3000 | Batch 0009/0011 | Loss: 3191.3545\n",
      "Epoch: 052/3000 | Batch 0010/0011 | Loss: 3160.1958\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 053/3000 | Batch 0000/0011 | Loss: 3277.1802\n",
      "Epoch: 053/3000 | Batch 0001/0011 | Loss: 3239.6147\n",
      "Epoch: 053/3000 | Batch 0002/0011 | Loss: 3274.7236\n",
      "Epoch: 053/3000 | Batch 0003/0011 | Loss: 3169.6270\n",
      "Epoch: 053/3000 | Batch 0004/0011 | Loss: 3226.7866\n",
      "Epoch: 053/3000 | Batch 0005/0011 | Loss: 3230.4700\n",
      "Epoch: 053/3000 | Batch 0006/0011 | Loss: 3242.2725\n",
      "Epoch: 053/3000 | Batch 0007/0011 | Loss: 3245.8018\n",
      "Epoch: 053/3000 | Batch 0008/0011 | Loss: 3217.4763\n",
      "Epoch: 053/3000 | Batch 0009/0011 | Loss: 3215.0732\n",
      "Epoch: 053/3000 | Batch 0010/0011 | Loss: 3174.7258\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 054/3000 | Batch 0000/0011 | Loss: 3211.4963\n",
      "Epoch: 054/3000 | Batch 0001/0011 | Loss: 3237.4915\n",
      "Epoch: 054/3000 | Batch 0002/0011 | Loss: 3178.6465\n",
      "Epoch: 054/3000 | Batch 0003/0011 | Loss: 3192.0498\n",
      "Epoch: 054/3000 | Batch 0004/0011 | Loss: 3262.7402\n",
      "Epoch: 054/3000 | Batch 0005/0011 | Loss: 3201.5801\n",
      "Epoch: 054/3000 | Batch 0006/0011 | Loss: 3261.3694\n",
      "Epoch: 054/3000 | Batch 0007/0011 | Loss: 3309.2698\n",
      "Epoch: 054/3000 | Batch 0008/0011 | Loss: 3159.3181\n",
      "Epoch: 054/3000 | Batch 0009/0011 | Loss: 3253.7041\n",
      "Epoch: 054/3000 | Batch 0010/0011 | Loss: 3403.0728\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 055/3000 | Batch 0000/0011 | Loss: 3263.0435\n",
      "Epoch: 055/3000 | Batch 0001/0011 | Loss: 3159.8196\n",
      "Epoch: 055/3000 | Batch 0002/0011 | Loss: 3189.5308\n",
      "Epoch: 055/3000 | Batch 0003/0011 | Loss: 3245.8003\n",
      "Epoch: 055/3000 | Batch 0004/0011 | Loss: 3196.6562\n",
      "Epoch: 055/3000 | Batch 0005/0011 | Loss: 3231.2754\n",
      "Epoch: 055/3000 | Batch 0006/0011 | Loss: 3213.1589\n",
      "Epoch: 055/3000 | Batch 0007/0011 | Loss: 3304.2905\n",
      "Epoch: 055/3000 | Batch 0008/0011 | Loss: 3190.8257\n",
      "Epoch: 055/3000 | Batch 0009/0011 | Loss: 3241.2661\n",
      "Epoch: 055/3000 | Batch 0010/0011 | Loss: 3124.9482\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 056/3000 | Batch 0000/0011 | Loss: 3227.6401\n",
      "Epoch: 056/3000 | Batch 0001/0011 | Loss: 3210.4683\n",
      "Epoch: 056/3000 | Batch 0002/0011 | Loss: 3324.6262\n",
      "Epoch: 056/3000 | Batch 0003/0011 | Loss: 3106.8784\n",
      "Epoch: 056/3000 | Batch 0004/0011 | Loss: 3196.1558\n",
      "Epoch: 056/3000 | Batch 0005/0011 | Loss: 3178.4026\n",
      "Epoch: 056/3000 | Batch 0006/0011 | Loss: 3276.1843\n",
      "Epoch: 056/3000 | Batch 0007/0011 | Loss: 3174.2839\n",
      "Epoch: 056/3000 | Batch 0008/0011 | Loss: 3189.7634\n",
      "Epoch: 056/3000 | Batch 0009/0011 | Loss: 3201.3228\n",
      "Epoch: 056/3000 | Batch 0010/0011 | Loss: 3580.5535\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 057/3000 | Batch 0000/0011 | Loss: 3220.1199\n",
      "Epoch: 057/3000 | Batch 0001/0011 | Loss: 3161.5308\n",
      "Epoch: 057/3000 | Batch 0002/0011 | Loss: 3203.6699\n",
      "Epoch: 057/3000 | Batch 0003/0011 | Loss: 3208.1152\n",
      "Epoch: 057/3000 | Batch 0004/0011 | Loss: 3244.6472\n",
      "Epoch: 057/3000 | Batch 0005/0011 | Loss: 3262.9131\n",
      "Epoch: 057/3000 | Batch 0006/0011 | Loss: 3247.8379\n",
      "Epoch: 057/3000 | Batch 0007/0011 | Loss: 3175.9412\n",
      "Epoch: 057/3000 | Batch 0008/0011 | Loss: 3301.0259\n",
      "Epoch: 057/3000 | Batch 0009/0011 | Loss: 3169.6970\n",
      "Epoch: 057/3000 | Batch 0010/0011 | Loss: 3302.7361\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 058/3000 | Batch 0000/0011 | Loss: 3222.1736\n",
      "Epoch: 058/3000 | Batch 0001/0011 | Loss: 3198.7646\n",
      "Epoch: 058/3000 | Batch 0002/0011 | Loss: 3290.1506\n",
      "Epoch: 058/3000 | Batch 0003/0011 | Loss: 3179.9961\n",
      "Epoch: 058/3000 | Batch 0004/0011 | Loss: 3203.1226\n",
      "Epoch: 058/3000 | Batch 0005/0011 | Loss: 3220.4780\n",
      "Epoch: 058/3000 | Batch 0006/0011 | Loss: 3254.5974\n",
      "Epoch: 058/3000 | Batch 0007/0011 | Loss: 3187.3420\n",
      "Epoch: 058/3000 | Batch 0008/0011 | Loss: 3205.7549\n",
      "Epoch: 058/3000 | Batch 0009/0011 | Loss: 3340.2207\n",
      "Epoch: 058/3000 | Batch 0010/0011 | Loss: 3071.7681\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 059/3000 | Batch 0000/0011 | Loss: 3202.9177\n",
      "Epoch: 059/3000 | Batch 0001/0011 | Loss: 3180.1160\n",
      "Epoch: 059/3000 | Batch 0002/0011 | Loss: 3256.0452\n",
      "Epoch: 059/3000 | Batch 0003/0011 | Loss: 3186.1472\n",
      "Epoch: 059/3000 | Batch 0004/0011 | Loss: 3282.4614\n",
      "Epoch: 059/3000 | Batch 0005/0011 | Loss: 3222.5674\n",
      "Epoch: 059/3000 | Batch 0006/0011 | Loss: 3267.3801\n",
      "Epoch: 059/3000 | Batch 0007/0011 | Loss: 3222.8699\n",
      "Epoch: 059/3000 | Batch 0008/0011 | Loss: 3206.0928\n",
      "Epoch: 059/3000 | Batch 0009/0011 | Loss: 3166.0945\n",
      "Epoch: 059/3000 | Batch 0010/0011 | Loss: 3252.2231\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 060/3000 | Batch 0000/0011 | Loss: 3193.8040\n",
      "Epoch: 060/3000 | Batch 0001/0011 | Loss: 3169.1323\n",
      "Epoch: 060/3000 | Batch 0002/0011 | Loss: 3312.7930\n",
      "Epoch: 060/3000 | Batch 0003/0011 | Loss: 3241.4224\n",
      "Epoch: 060/3000 | Batch 0004/0011 | Loss: 3192.8281\n",
      "Epoch: 060/3000 | Batch 0005/0011 | Loss: 3218.1245\n",
      "Epoch: 060/3000 | Batch 0006/0011 | Loss: 3190.1609\n",
      "Epoch: 060/3000 | Batch 0007/0011 | Loss: 3198.0962\n",
      "Epoch: 060/3000 | Batch 0008/0011 | Loss: 3265.7849\n",
      "Epoch: 060/3000 | Batch 0009/0011 | Loss: 3216.1973\n",
      "Epoch: 060/3000 | Batch 0010/0011 | Loss: 3237.6396\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 061/3000 | Batch 0000/0011 | Loss: 3198.6445\n",
      "Epoch: 061/3000 | Batch 0001/0011 | Loss: 3135.7637\n",
      "Epoch: 061/3000 | Batch 0002/0011 | Loss: 3279.8337\n",
      "Epoch: 061/3000 | Batch 0003/0011 | Loss: 3300.5564\n",
      "Epoch: 061/3000 | Batch 0004/0011 | Loss: 3223.2092\n",
      "Epoch: 061/3000 | Batch 0005/0011 | Loss: 3170.9802\n",
      "Epoch: 061/3000 | Batch 0006/0011 | Loss: 3236.0015\n",
      "Epoch: 061/3000 | Batch 0007/0011 | Loss: 3175.9370\n",
      "Epoch: 061/3000 | Batch 0008/0011 | Loss: 3251.2720\n",
      "Epoch: 061/3000 | Batch 0009/0011 | Loss: 3168.3652\n",
      "Epoch: 061/3000 | Batch 0010/0011 | Loss: 3193.6230\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 062/3000 | Batch 0000/0011 | Loss: 3218.5984\n",
      "Epoch: 062/3000 | Batch 0001/0011 | Loss: 3212.2332\n",
      "Epoch: 062/3000 | Batch 0002/0011 | Loss: 3183.3752\n",
      "Epoch: 062/3000 | Batch 0003/0011 | Loss: 3254.3145\n",
      "Epoch: 062/3000 | Batch 0004/0011 | Loss: 3140.2842\n",
      "Epoch: 062/3000 | Batch 0005/0011 | Loss: 3216.8474\n",
      "Epoch: 062/3000 | Batch 0006/0011 | Loss: 3269.1624\n",
      "Epoch: 062/3000 | Batch 0007/0011 | Loss: 3147.4131\n",
      "Epoch: 062/3000 | Batch 0008/0011 | Loss: 3262.3464\n",
      "Epoch: 062/3000 | Batch 0009/0011 | Loss: 3204.9438\n",
      "Epoch: 062/3000 | Batch 0010/0011 | Loss: 3160.2510\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 063/3000 | Batch 0000/0011 | Loss: 3205.1060\n",
      "Epoch: 063/3000 | Batch 0001/0011 | Loss: 3170.9158\n",
      "Epoch: 063/3000 | Batch 0002/0011 | Loss: 3234.9775\n",
      "Epoch: 063/3000 | Batch 0003/0011 | Loss: 3259.7007\n",
      "Epoch: 063/3000 | Batch 0004/0011 | Loss: 3177.4836\n",
      "Epoch: 063/3000 | Batch 0005/0011 | Loss: 3194.7278\n",
      "Epoch: 063/3000 | Batch 0006/0011 | Loss: 3244.8857\n",
      "Epoch: 063/3000 | Batch 0007/0011 | Loss: 3195.0085\n",
      "Epoch: 063/3000 | Batch 0008/0011 | Loss: 3242.9541\n",
      "Epoch: 063/3000 | Batch 0009/0011 | Loss: 3151.8237\n",
      "Epoch: 063/3000 | Batch 0010/0011 | Loss: 3151.3599\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 064/3000 | Batch 0000/0011 | Loss: 3322.5415\n",
      "Epoch: 064/3000 | Batch 0001/0011 | Loss: 3231.0027\n",
      "Epoch: 064/3000 | Batch 0002/0011 | Loss: 3171.1377\n",
      "Epoch: 064/3000 | Batch 0003/0011 | Loss: 3222.9619\n",
      "Epoch: 064/3000 | Batch 0004/0011 | Loss: 3201.2388\n",
      "Epoch: 064/3000 | Batch 0005/0011 | Loss: 3172.4773\n",
      "Epoch: 064/3000 | Batch 0006/0011 | Loss: 3217.2383\n",
      "Epoch: 064/3000 | Batch 0007/0011 | Loss: 3159.0828\n",
      "Epoch: 064/3000 | Batch 0008/0011 | Loss: 3216.1497\n",
      "Epoch: 064/3000 | Batch 0009/0011 | Loss: 3208.2827\n",
      "Epoch: 064/3000 | Batch 0010/0011 | Loss: 3159.1177\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 065/3000 | Batch 0000/0011 | Loss: 3197.7524\n",
      "Epoch: 065/3000 | Batch 0001/0011 | Loss: 3206.1597\n",
      "Epoch: 065/3000 | Batch 0002/0011 | Loss: 3225.2222\n",
      "Epoch: 065/3000 | Batch 0003/0011 | Loss: 3149.1064\n",
      "Epoch: 065/3000 | Batch 0004/0011 | Loss: 3190.7749\n",
      "Epoch: 065/3000 | Batch 0005/0011 | Loss: 3262.0449\n",
      "Epoch: 065/3000 | Batch 0006/0011 | Loss: 3161.5454\n",
      "Epoch: 065/3000 | Batch 0007/0011 | Loss: 3216.8616\n",
      "Epoch: 065/3000 | Batch 0008/0011 | Loss: 3354.2878\n",
      "Epoch: 065/3000 | Batch 0009/0011 | Loss: 3153.7454\n",
      "Epoch: 065/3000 | Batch 0010/0011 | Loss: 3332.3086\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 066/3000 | Batch 0000/0011 | Loss: 3229.7454\n",
      "Epoch: 066/3000 | Batch 0001/0011 | Loss: 3215.7568\n",
      "Epoch: 066/3000 | Batch 0002/0011 | Loss: 3164.1470\n",
      "Epoch: 066/3000 | Batch 0003/0011 | Loss: 3173.5378\n",
      "Epoch: 066/3000 | Batch 0004/0011 | Loss: 3163.3777\n",
      "Epoch: 066/3000 | Batch 0005/0011 | Loss: 3297.1492\n",
      "Epoch: 066/3000 | Batch 0006/0011 | Loss: 3278.3640\n",
      "Epoch: 066/3000 | Batch 0007/0011 | Loss: 3233.4360\n",
      "Epoch: 066/3000 | Batch 0008/0011 | Loss: 3208.6838\n",
      "Epoch: 066/3000 | Batch 0009/0011 | Loss: 3197.7454\n",
      "Epoch: 066/3000 | Batch 0010/0011 | Loss: 3297.8250\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 067/3000 | Batch 0000/0011 | Loss: 3220.7251\n",
      "Epoch: 067/3000 | Batch 0001/0011 | Loss: 3159.5129\n",
      "Epoch: 067/3000 | Batch 0002/0011 | Loss: 3248.1877\n",
      "Epoch: 067/3000 | Batch 0003/0011 | Loss: 3215.6157\n",
      "Epoch: 067/3000 | Batch 0004/0011 | Loss: 3261.8828\n",
      "Epoch: 067/3000 | Batch 0005/0011 | Loss: 3198.6799\n",
      "Epoch: 067/3000 | Batch 0006/0011 | Loss: 3202.1997\n",
      "Epoch: 067/3000 | Batch 0007/0011 | Loss: 3202.5757\n",
      "Epoch: 067/3000 | Batch 0008/0011 | Loss: 3224.0938\n",
      "Epoch: 067/3000 | Batch 0009/0011 | Loss: 3226.4614\n",
      "Epoch: 067/3000 | Batch 0010/0011 | Loss: 3042.0544\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 068/3000 | Batch 0000/0011 | Loss: 3238.5190\n",
      "Epoch: 068/3000 | Batch 0001/0011 | Loss: 3189.1829\n",
      "Epoch: 068/3000 | Batch 0002/0011 | Loss: 3148.9836\n",
      "Epoch: 068/3000 | Batch 0003/0011 | Loss: 3269.6819\n",
      "Epoch: 068/3000 | Batch 0004/0011 | Loss: 3223.2251\n",
      "Epoch: 068/3000 | Batch 0005/0011 | Loss: 3179.2051\n",
      "Epoch: 068/3000 | Batch 0006/0011 | Loss: 3190.9556\n",
      "Epoch: 068/3000 | Batch 0007/0011 | Loss: 3186.0151\n",
      "Epoch: 068/3000 | Batch 0008/0011 | Loss: 3198.0244\n",
      "Epoch: 068/3000 | Batch 0009/0011 | Loss: 3151.7061\n",
      "Epoch: 068/3000 | Batch 0010/0011 | Loss: 3313.9551\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 069/3000 | Batch 0000/0011 | Loss: 3165.0991\n",
      "Epoch: 069/3000 | Batch 0001/0011 | Loss: 3244.3291\n",
      "Epoch: 069/3000 | Batch 0002/0011 | Loss: 3293.9282\n",
      "Epoch: 069/3000 | Batch 0003/0011 | Loss: 3179.8682\n",
      "Epoch: 069/3000 | Batch 0004/0011 | Loss: 3247.9946\n",
      "Epoch: 069/3000 | Batch 0005/0011 | Loss: 3257.8713\n",
      "Epoch: 069/3000 | Batch 0006/0011 | Loss: 3298.7207\n",
      "Epoch: 069/3000 | Batch 0007/0011 | Loss: 3195.9766\n",
      "Epoch: 069/3000 | Batch 0008/0011 | Loss: 3177.0317\n",
      "Epoch: 069/3000 | Batch 0009/0011 | Loss: 3172.1001\n",
      "Epoch: 069/3000 | Batch 0010/0011 | Loss: 3029.7891\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 070/3000 | Batch 0000/0011 | Loss: 3164.9717\n",
      "Epoch: 070/3000 | Batch 0001/0011 | Loss: 3276.2771\n",
      "Epoch: 070/3000 | Batch 0002/0011 | Loss: 3145.7397\n",
      "Epoch: 070/3000 | Batch 0003/0011 | Loss: 3171.3589\n",
      "Epoch: 070/3000 | Batch 0004/0011 | Loss: 3239.3521\n",
      "Epoch: 070/3000 | Batch 0005/0011 | Loss: 3264.2673\n",
      "Epoch: 070/3000 | Batch 0006/0011 | Loss: 3320.8582\n",
      "Epoch: 070/3000 | Batch 0007/0011 | Loss: 3197.3286\n",
      "Epoch: 070/3000 | Batch 0008/0011 | Loss: 3172.7734\n",
      "Epoch: 070/3000 | Batch 0009/0011 | Loss: 3186.5266\n",
      "Epoch: 070/3000 | Batch 0010/0011 | Loss: 3284.4568\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 071/3000 | Batch 0000/0011 | Loss: 3231.3191\n",
      "Epoch: 071/3000 | Batch 0001/0011 | Loss: 3153.2358\n",
      "Epoch: 071/3000 | Batch 0002/0011 | Loss: 3222.8003\n",
      "Epoch: 071/3000 | Batch 0003/0011 | Loss: 3217.2900\n",
      "Epoch: 071/3000 | Batch 0004/0011 | Loss: 3204.0488\n",
      "Epoch: 071/3000 | Batch 0005/0011 | Loss: 3200.1938\n",
      "Epoch: 071/3000 | Batch 0006/0011 | Loss: 3171.1809\n",
      "Epoch: 071/3000 | Batch 0007/0011 | Loss: 3173.8169\n",
      "Epoch: 071/3000 | Batch 0008/0011 | Loss: 3228.7002\n",
      "Epoch: 071/3000 | Batch 0009/0011 | Loss: 3253.0364\n",
      "Epoch: 071/3000 | Batch 0010/0011 | Loss: 3185.4199\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 072/3000 | Batch 0000/0011 | Loss: 3207.7949\n",
      "Epoch: 072/3000 | Batch 0001/0011 | Loss: 3258.5371\n",
      "Epoch: 072/3000 | Batch 0002/0011 | Loss: 3223.7781\n",
      "Epoch: 072/3000 | Batch 0003/0011 | Loss: 3161.7144\n",
      "Epoch: 072/3000 | Batch 0004/0011 | Loss: 3193.1648\n",
      "Epoch: 072/3000 | Batch 0005/0011 | Loss: 3138.6184\n",
      "Epoch: 072/3000 | Batch 0006/0011 | Loss: 3122.3142\n",
      "Epoch: 072/3000 | Batch 0007/0011 | Loss: 3277.4897\n",
      "Epoch: 072/3000 | Batch 0008/0011 | Loss: 3230.9924\n",
      "Epoch: 072/3000 | Batch 0009/0011 | Loss: 3155.3665\n",
      "Epoch: 072/3000 | Batch 0010/0011 | Loss: 3137.1289\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 073/3000 | Batch 0000/0011 | Loss: 3245.1467\n",
      "Epoch: 073/3000 | Batch 0001/0011 | Loss: 3156.9019\n",
      "Epoch: 073/3000 | Batch 0002/0011 | Loss: 3245.6257\n",
      "Epoch: 073/3000 | Batch 0003/0011 | Loss: 3249.0078\n",
      "Epoch: 073/3000 | Batch 0004/0011 | Loss: 3143.4050\n",
      "Epoch: 073/3000 | Batch 0005/0011 | Loss: 3131.0869\n",
      "Epoch: 073/3000 | Batch 0006/0011 | Loss: 3229.7717\n",
      "Epoch: 073/3000 | Batch 0007/0011 | Loss: 3177.0830\n",
      "Epoch: 073/3000 | Batch 0008/0011 | Loss: 3197.4958\n",
      "Epoch: 073/3000 | Batch 0009/0011 | Loss: 3267.6497\n",
      "Epoch: 073/3000 | Batch 0010/0011 | Loss: 3037.8677\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 074/3000 | Batch 0000/0011 | Loss: 3109.6169\n",
      "Epoch: 074/3000 | Batch 0001/0011 | Loss: 3151.8540\n",
      "Epoch: 074/3000 | Batch 0002/0011 | Loss: 3257.8037\n",
      "Epoch: 074/3000 | Batch 0003/0011 | Loss: 3202.2502\n",
      "Epoch: 074/3000 | Batch 0004/0011 | Loss: 3191.7007\n",
      "Epoch: 074/3000 | Batch 0005/0011 | Loss: 3188.8044\n",
      "Epoch: 074/3000 | Batch 0006/0011 | Loss: 3273.7739\n",
      "Epoch: 074/3000 | Batch 0007/0011 | Loss: 3205.1445\n",
      "Epoch: 074/3000 | Batch 0008/0011 | Loss: 3152.3472\n",
      "Epoch: 074/3000 | Batch 0009/0011 | Loss: 3233.0935\n",
      "Epoch: 074/3000 | Batch 0010/0011 | Loss: 2975.9355\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 075/3000 | Batch 0000/0011 | Loss: 3235.2053\n",
      "Epoch: 075/3000 | Batch 0001/0011 | Loss: 3162.0044\n",
      "Epoch: 075/3000 | Batch 0002/0011 | Loss: 3234.8430\n",
      "Epoch: 075/3000 | Batch 0003/0011 | Loss: 3157.1841\n",
      "Epoch: 075/3000 | Batch 0004/0011 | Loss: 3202.7930\n",
      "Epoch: 075/3000 | Batch 0005/0011 | Loss: 3224.3811\n",
      "Epoch: 075/3000 | Batch 0006/0011 | Loss: 3167.4355\n",
      "Epoch: 075/3000 | Batch 0007/0011 | Loss: 3195.7695\n",
      "Epoch: 075/3000 | Batch 0008/0011 | Loss: 3196.6077\n",
      "Epoch: 075/3000 | Batch 0009/0011 | Loss: 3196.0266\n",
      "Epoch: 075/3000 | Batch 0010/0011 | Loss: 3209.4976\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 076/3000 | Batch 0000/0011 | Loss: 3215.7566\n",
      "Epoch: 076/3000 | Batch 0001/0011 | Loss: 3302.7236\n",
      "Epoch: 076/3000 | Batch 0002/0011 | Loss: 3232.5750\n",
      "Epoch: 076/3000 | Batch 0003/0011 | Loss: 3186.8103\n",
      "Epoch: 076/3000 | Batch 0004/0011 | Loss: 3248.7969\n",
      "Epoch: 076/3000 | Batch 0005/0011 | Loss: 3214.8328\n",
      "Epoch: 076/3000 | Batch 0006/0011 | Loss: 3171.7627\n",
      "Epoch: 076/3000 | Batch 0007/0011 | Loss: 3099.8977\n",
      "Epoch: 076/3000 | Batch 0008/0011 | Loss: 3153.3406\n",
      "Epoch: 076/3000 | Batch 0009/0011 | Loss: 3279.1230\n",
      "Epoch: 076/3000 | Batch 0010/0011 | Loss: 3355.8447\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 077/3000 | Batch 0000/0011 | Loss: 3220.7629\n",
      "Epoch: 077/3000 | Batch 0001/0011 | Loss: 3260.6841\n",
      "Epoch: 077/3000 | Batch 0002/0011 | Loss: 3171.6294\n",
      "Epoch: 077/3000 | Batch 0003/0011 | Loss: 3259.1084\n",
      "Epoch: 077/3000 | Batch 0004/0011 | Loss: 3201.7500\n",
      "Epoch: 077/3000 | Batch 0005/0011 | Loss: 3153.9099\n",
      "Epoch: 077/3000 | Batch 0006/0011 | Loss: 3203.1882\n",
      "Epoch: 077/3000 | Batch 0007/0011 | Loss: 3114.5486\n",
      "Epoch: 077/3000 | Batch 0008/0011 | Loss: 3165.7773\n",
      "Epoch: 077/3000 | Batch 0009/0011 | Loss: 3178.5112\n",
      "Epoch: 077/3000 | Batch 0010/0011 | Loss: 3142.2986\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 078/3000 | Batch 0000/0011 | Loss: 3187.4460\n",
      "Epoch: 078/3000 | Batch 0001/0011 | Loss: 3169.2385\n",
      "Epoch: 078/3000 | Batch 0002/0011 | Loss: 3199.5874\n",
      "Epoch: 078/3000 | Batch 0003/0011 | Loss: 3132.9463\n",
      "Epoch: 078/3000 | Batch 0004/0011 | Loss: 3305.8008\n",
      "Epoch: 078/3000 | Batch 0005/0011 | Loss: 3223.6870\n",
      "Epoch: 078/3000 | Batch 0006/0011 | Loss: 3256.1621\n",
      "Epoch: 078/3000 | Batch 0007/0011 | Loss: 3165.9172\n",
      "Epoch: 078/3000 | Batch 0008/0011 | Loss: 3208.0134\n",
      "Epoch: 078/3000 | Batch 0009/0011 | Loss: 3202.1902\n",
      "Epoch: 078/3000 | Batch 0010/0011 | Loss: 3189.0813\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 079/3000 | Batch 0000/0011 | Loss: 3130.6963\n",
      "Epoch: 079/3000 | Batch 0001/0011 | Loss: 3197.2783\n",
      "Epoch: 079/3000 | Batch 0002/0011 | Loss: 3170.1289\n",
      "Epoch: 079/3000 | Batch 0003/0011 | Loss: 3196.5042\n",
      "Epoch: 079/3000 | Batch 0004/0011 | Loss: 3171.9077\n",
      "Epoch: 079/3000 | Batch 0005/0011 | Loss: 3202.1663\n",
      "Epoch: 079/3000 | Batch 0006/0011 | Loss: 3230.6541\n",
      "Epoch: 079/3000 | Batch 0007/0011 | Loss: 3187.8511\n",
      "Epoch: 079/3000 | Batch 0008/0011 | Loss: 3159.9546\n",
      "Epoch: 079/3000 | Batch 0009/0011 | Loss: 3299.6511\n",
      "Epoch: 079/3000 | Batch 0010/0011 | Loss: 3039.6516\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 080/3000 | Batch 0000/0011 | Loss: 3179.0435\n",
      "Epoch: 080/3000 | Batch 0001/0011 | Loss: 3146.5454\n",
      "Epoch: 080/3000 | Batch 0002/0011 | Loss: 3066.9719\n",
      "Epoch: 080/3000 | Batch 0003/0011 | Loss: 3275.5491\n",
      "Epoch: 080/3000 | Batch 0004/0011 | Loss: 3209.1436\n",
      "Epoch: 080/3000 | Batch 0005/0011 | Loss: 3239.0732\n",
      "Epoch: 080/3000 | Batch 0006/0011 | Loss: 3192.6885\n",
      "Epoch: 080/3000 | Batch 0007/0011 | Loss: 3239.8352\n",
      "Epoch: 080/3000 | Batch 0008/0011 | Loss: 3197.7625\n",
      "Epoch: 080/3000 | Batch 0009/0011 | Loss: 3191.2444\n",
      "Epoch: 080/3000 | Batch 0010/0011 | Loss: 3078.8491\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 081/3000 | Batch 0000/0011 | Loss: 3145.5503\n",
      "Epoch: 081/3000 | Batch 0001/0011 | Loss: 3206.6985\n",
      "Epoch: 081/3000 | Batch 0002/0011 | Loss: 3168.7007\n",
      "Epoch: 081/3000 | Batch 0003/0011 | Loss: 3267.6836\n",
      "Epoch: 081/3000 | Batch 0004/0011 | Loss: 3182.9138\n",
      "Epoch: 081/3000 | Batch 0005/0011 | Loss: 3171.5344\n",
      "Epoch: 081/3000 | Batch 0006/0011 | Loss: 3194.7039\n",
      "Epoch: 081/3000 | Batch 0007/0011 | Loss: 3192.0486\n",
      "Epoch: 081/3000 | Batch 0008/0011 | Loss: 3206.1155\n",
      "Epoch: 081/3000 | Batch 0009/0011 | Loss: 3170.0430\n",
      "Epoch: 081/3000 | Batch 0010/0011 | Loss: 3209.2507\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 082/3000 | Batch 0000/0011 | Loss: 3232.5930\n",
      "Epoch: 082/3000 | Batch 0001/0011 | Loss: 3200.0542\n",
      "Epoch: 082/3000 | Batch 0002/0011 | Loss: 3236.2419\n",
      "Epoch: 082/3000 | Batch 0003/0011 | Loss: 3248.0686\n",
      "Epoch: 082/3000 | Batch 0004/0011 | Loss: 3275.8596\n",
      "Epoch: 082/3000 | Batch 0005/0011 | Loss: 3151.8167\n",
      "Epoch: 082/3000 | Batch 0006/0011 | Loss: 3122.7668\n",
      "Epoch: 082/3000 | Batch 0007/0011 | Loss: 3165.1199\n",
      "Epoch: 082/3000 | Batch 0008/0011 | Loss: 3157.3699\n",
      "Epoch: 082/3000 | Batch 0009/0011 | Loss: 3129.8315\n",
      "Epoch: 082/3000 | Batch 0010/0011 | Loss: 3269.4443\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 083/3000 | Batch 0000/0011 | Loss: 3267.6787\n",
      "Epoch: 083/3000 | Batch 0001/0011 | Loss: 3121.3938\n",
      "Epoch: 083/3000 | Batch 0002/0011 | Loss: 3277.0286\n",
      "Epoch: 083/3000 | Batch 0003/0011 | Loss: 3227.0073\n",
      "Epoch: 083/3000 | Batch 0004/0011 | Loss: 3194.5105\n",
      "Epoch: 083/3000 | Batch 0005/0011 | Loss: 3132.6086\n",
      "Epoch: 083/3000 | Batch 0006/0011 | Loss: 3221.9526\n",
      "Epoch: 083/3000 | Batch 0007/0011 | Loss: 3201.9934\n",
      "Epoch: 083/3000 | Batch 0008/0011 | Loss: 3172.9592\n",
      "Epoch: 083/3000 | Batch 0009/0011 | Loss: 3185.9341\n",
      "Epoch: 083/3000 | Batch 0010/0011 | Loss: 3064.8147\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 084/3000 | Batch 0000/0011 | Loss: 3111.6741\n",
      "Epoch: 084/3000 | Batch 0001/0011 | Loss: 3238.4194\n",
      "Epoch: 084/3000 | Batch 0002/0011 | Loss: 3141.1411\n",
      "Epoch: 084/3000 | Batch 0003/0011 | Loss: 3257.4062\n",
      "Epoch: 084/3000 | Batch 0004/0011 | Loss: 3174.2390\n",
      "Epoch: 084/3000 | Batch 0005/0011 | Loss: 3205.7549\n",
      "Epoch: 084/3000 | Batch 0006/0011 | Loss: 3202.0791\n",
      "Epoch: 084/3000 | Batch 0007/0011 | Loss: 3223.4604\n",
      "Epoch: 084/3000 | Batch 0008/0011 | Loss: 3196.9182\n",
      "Epoch: 084/3000 | Batch 0009/0011 | Loss: 3120.2437\n",
      "Epoch: 084/3000 | Batch 0010/0011 | Loss: 3598.1985\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 085/3000 | Batch 0000/0011 | Loss: 3183.5173\n",
      "Epoch: 085/3000 | Batch 0001/0011 | Loss: 3221.6455\n",
      "Epoch: 085/3000 | Batch 0002/0011 | Loss: 3185.6433\n",
      "Epoch: 085/3000 | Batch 0003/0011 | Loss: 3194.7554\n",
      "Epoch: 085/3000 | Batch 0004/0011 | Loss: 3209.8784\n",
      "Epoch: 085/3000 | Batch 0005/0011 | Loss: 3112.1877\n",
      "Epoch: 085/3000 | Batch 0006/0011 | Loss: 3205.1123\n",
      "Epoch: 085/3000 | Batch 0007/0011 | Loss: 3222.0439\n",
      "Epoch: 085/3000 | Batch 0008/0011 | Loss: 3198.9138\n",
      "Epoch: 085/3000 | Batch 0009/0011 | Loss: 3174.9460\n",
      "Epoch: 085/3000 | Batch 0010/0011 | Loss: 3314.0620\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 086/3000 | Batch 0000/0011 | Loss: 3161.3230\n",
      "Epoch: 086/3000 | Batch 0001/0011 | Loss: 3316.4868\n",
      "Epoch: 086/3000 | Batch 0002/0011 | Loss: 3158.2463\n",
      "Epoch: 086/3000 | Batch 0003/0011 | Loss: 3174.2834\n",
      "Epoch: 086/3000 | Batch 0004/0011 | Loss: 3139.8105\n",
      "Epoch: 086/3000 | Batch 0005/0011 | Loss: 3183.0767\n",
      "Epoch: 086/3000 | Batch 0006/0011 | Loss: 3153.0684\n",
      "Epoch: 086/3000 | Batch 0007/0011 | Loss: 3193.4258\n",
      "Epoch: 086/3000 | Batch 0008/0011 | Loss: 3267.6077\n",
      "Epoch: 086/3000 | Batch 0009/0011 | Loss: 3231.7297\n",
      "Epoch: 086/3000 | Batch 0010/0011 | Loss: 3130.0234\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 087/3000 | Batch 0000/0011 | Loss: 3249.0564\n",
      "Epoch: 087/3000 | Batch 0001/0011 | Loss: 3197.2676\n",
      "Epoch: 087/3000 | Batch 0002/0011 | Loss: 3137.6094\n",
      "Epoch: 087/3000 | Batch 0003/0011 | Loss: 3236.8042\n",
      "Epoch: 087/3000 | Batch 0004/0011 | Loss: 3273.6167\n",
      "Epoch: 087/3000 | Batch 0005/0011 | Loss: 3228.3547\n",
      "Epoch: 087/3000 | Batch 0006/0011 | Loss: 3140.4358\n",
      "Epoch: 087/3000 | Batch 0007/0011 | Loss: 3157.1060\n",
      "Epoch: 087/3000 | Batch 0008/0011 | Loss: 3187.2791\n",
      "Epoch: 087/3000 | Batch 0009/0011 | Loss: 3236.2996\n",
      "Epoch: 087/3000 | Batch 0010/0011 | Loss: 3209.8564\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 088/3000 | Batch 0000/0011 | Loss: 3136.4143\n",
      "Epoch: 088/3000 | Batch 0001/0011 | Loss: 3237.1458\n",
      "Epoch: 088/3000 | Batch 0002/0011 | Loss: 3247.3306\n",
      "Epoch: 088/3000 | Batch 0003/0011 | Loss: 3149.4004\n",
      "Epoch: 088/3000 | Batch 0004/0011 | Loss: 3171.7700\n",
      "Epoch: 088/3000 | Batch 0005/0011 | Loss: 3149.3843\n",
      "Epoch: 088/3000 | Batch 0006/0011 | Loss: 3221.7805\n",
      "Epoch: 088/3000 | Batch 0007/0011 | Loss: 3180.7334\n",
      "Epoch: 088/3000 | Batch 0008/0011 | Loss: 3314.5303\n",
      "Epoch: 088/3000 | Batch 0009/0011 | Loss: 3212.5784\n",
      "Epoch: 088/3000 | Batch 0010/0011 | Loss: 3206.6543\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 089/3000 | Batch 0000/0011 | Loss: 3197.5901\n",
      "Epoch: 089/3000 | Batch 0001/0011 | Loss: 3151.3481\n",
      "Epoch: 089/3000 | Batch 0002/0011 | Loss: 3223.3506\n",
      "Epoch: 089/3000 | Batch 0003/0011 | Loss: 3233.7974\n",
      "Epoch: 089/3000 | Batch 0004/0011 | Loss: 3211.4351\n",
      "Epoch: 089/3000 | Batch 0005/0011 | Loss: 3177.8928\n",
      "Epoch: 089/3000 | Batch 0006/0011 | Loss: 3200.5720\n",
      "Epoch: 089/3000 | Batch 0007/0011 | Loss: 3220.3545\n",
      "Epoch: 089/3000 | Batch 0008/0011 | Loss: 3127.4827\n",
      "Epoch: 089/3000 | Batch 0009/0011 | Loss: 3169.0623\n",
      "Epoch: 089/3000 | Batch 0010/0011 | Loss: 3240.7991\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 090/3000 | Batch 0000/0011 | Loss: 3125.6604\n",
      "Epoch: 090/3000 | Batch 0001/0011 | Loss: 3174.1655\n",
      "Epoch: 090/3000 | Batch 0002/0011 | Loss: 3192.0967\n",
      "Epoch: 090/3000 | Batch 0003/0011 | Loss: 3137.8359\n",
      "Epoch: 090/3000 | Batch 0004/0011 | Loss: 3156.7573\n",
      "Epoch: 090/3000 | Batch 0005/0011 | Loss: 3201.0984\n",
      "Epoch: 090/3000 | Batch 0006/0011 | Loss: 3238.5183\n",
      "Epoch: 090/3000 | Batch 0007/0011 | Loss: 3342.5742\n",
      "Epoch: 090/3000 | Batch 0008/0011 | Loss: 3154.9624\n",
      "Epoch: 090/3000 | Batch 0009/0011 | Loss: 3187.1401\n",
      "Epoch: 090/3000 | Batch 0010/0011 | Loss: 3172.6807\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 091/3000 | Batch 0000/0011 | Loss: 3210.6199\n",
      "Epoch: 091/3000 | Batch 0001/0011 | Loss: 3216.5710\n",
      "Epoch: 091/3000 | Batch 0002/0011 | Loss: 3199.7812\n",
      "Epoch: 091/3000 | Batch 0003/0011 | Loss: 3122.2036\n",
      "Epoch: 091/3000 | Batch 0004/0011 | Loss: 3171.1104\n",
      "Epoch: 091/3000 | Batch 0005/0011 | Loss: 3235.9165\n",
      "Epoch: 091/3000 | Batch 0006/0011 | Loss: 3237.7439\n",
      "Epoch: 091/3000 | Batch 0007/0011 | Loss: 3192.2905\n",
      "Epoch: 091/3000 | Batch 0008/0011 | Loss: 3157.8909\n",
      "Epoch: 091/3000 | Batch 0009/0011 | Loss: 3173.4275\n",
      "Epoch: 091/3000 | Batch 0010/0011 | Loss: 2973.1169\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 092/3000 | Batch 0000/0011 | Loss: 3178.3350\n",
      "Epoch: 092/3000 | Batch 0001/0011 | Loss: 3146.3828\n",
      "Epoch: 092/3000 | Batch 0002/0011 | Loss: 3208.3572\n",
      "Epoch: 092/3000 | Batch 0003/0011 | Loss: 3239.0222\n",
      "Epoch: 092/3000 | Batch 0004/0011 | Loss: 3167.7214\n",
      "Epoch: 092/3000 | Batch 0005/0011 | Loss: 3254.8337\n",
      "Epoch: 092/3000 | Batch 0006/0011 | Loss: 3150.2507\n",
      "Epoch: 092/3000 | Batch 0007/0011 | Loss: 3144.8342\n",
      "Epoch: 092/3000 | Batch 0008/0011 | Loss: 3144.4409\n",
      "Epoch: 092/3000 | Batch 0009/0011 | Loss: 3228.4514\n",
      "Epoch: 092/3000 | Batch 0010/0011 | Loss: 3174.3604\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 093/3000 | Batch 0000/0011 | Loss: 3204.8918\n",
      "Epoch: 093/3000 | Batch 0001/0011 | Loss: 3179.9570\n",
      "Epoch: 093/3000 | Batch 0002/0011 | Loss: 3234.8271\n",
      "Epoch: 093/3000 | Batch 0003/0011 | Loss: 3177.4568\n",
      "Epoch: 093/3000 | Batch 0004/0011 | Loss: 3262.2786\n",
      "Epoch: 093/3000 | Batch 0005/0011 | Loss: 3178.8396\n",
      "Epoch: 093/3000 | Batch 0006/0011 | Loss: 3159.9355\n",
      "Epoch: 093/3000 | Batch 0007/0011 | Loss: 3125.5574\n",
      "Epoch: 093/3000 | Batch 0008/0011 | Loss: 3174.6428\n",
      "Epoch: 093/3000 | Batch 0009/0011 | Loss: 3144.3501\n",
      "Epoch: 093/3000 | Batch 0010/0011 | Loss: 3209.7341\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 094/3000 | Batch 0000/0011 | Loss: 3167.8286\n",
      "Epoch: 094/3000 | Batch 0001/0011 | Loss: 3183.7727\n",
      "Epoch: 094/3000 | Batch 0002/0011 | Loss: 3160.1367\n",
      "Epoch: 094/3000 | Batch 0003/0011 | Loss: 3238.3306\n",
      "Epoch: 094/3000 | Batch 0004/0011 | Loss: 3126.5364\n",
      "Epoch: 094/3000 | Batch 0005/0011 | Loss: 3255.1956\n",
      "Epoch: 094/3000 | Batch 0006/0011 | Loss: 3199.8374\n",
      "Epoch: 094/3000 | Batch 0007/0011 | Loss: 3167.9966\n",
      "Epoch: 094/3000 | Batch 0008/0011 | Loss: 3175.4062\n",
      "Epoch: 094/3000 | Batch 0009/0011 | Loss: 3161.8474\n",
      "Epoch: 094/3000 | Batch 0010/0011 | Loss: 3105.5686\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 095/3000 | Batch 0000/0011 | Loss: 3253.1536\n",
      "Epoch: 095/3000 | Batch 0001/0011 | Loss: 3115.5669\n",
      "Epoch: 095/3000 | Batch 0002/0011 | Loss: 3204.6143\n",
      "Epoch: 095/3000 | Batch 0003/0011 | Loss: 3153.5513\n",
      "Epoch: 095/3000 | Batch 0004/0011 | Loss: 3194.8354\n",
      "Epoch: 095/3000 | Batch 0005/0011 | Loss: 3180.6521\n",
      "Epoch: 095/3000 | Batch 0006/0011 | Loss: 3204.6777\n",
      "Epoch: 095/3000 | Batch 0007/0011 | Loss: 3175.5029\n",
      "Epoch: 095/3000 | Batch 0008/0011 | Loss: 3226.9092\n",
      "Epoch: 095/3000 | Batch 0009/0011 | Loss: 3182.3022\n",
      "Epoch: 095/3000 | Batch 0010/0011 | Loss: 3282.7837\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 096/3000 | Batch 0000/0011 | Loss: 3207.5708\n",
      "Epoch: 096/3000 | Batch 0001/0011 | Loss: 3150.6973\n",
      "Epoch: 096/3000 | Batch 0002/0011 | Loss: 3242.2222\n",
      "Epoch: 096/3000 | Batch 0003/0011 | Loss: 3169.2615\n",
      "Epoch: 096/3000 | Batch 0004/0011 | Loss: 3187.6018\n",
      "Epoch: 096/3000 | Batch 0005/0011 | Loss: 3168.4163\n",
      "Epoch: 096/3000 | Batch 0006/0011 | Loss: 3139.7400\n",
      "Epoch: 096/3000 | Batch 0007/0011 | Loss: 3194.7322\n",
      "Epoch: 096/3000 | Batch 0008/0011 | Loss: 3156.9551\n",
      "Epoch: 096/3000 | Batch 0009/0011 | Loss: 3341.2471\n",
      "Epoch: 096/3000 | Batch 0010/0011 | Loss: 3311.1077\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 097/3000 | Batch 0000/0011 | Loss: 3197.5940\n",
      "Epoch: 097/3000 | Batch 0001/0011 | Loss: 3178.8848\n",
      "Epoch: 097/3000 | Batch 0002/0011 | Loss: 3188.0012\n",
      "Epoch: 097/3000 | Batch 0003/0011 | Loss: 3233.9910\n",
      "Epoch: 097/3000 | Batch 0004/0011 | Loss: 3109.8005\n",
      "Epoch: 097/3000 | Batch 0005/0011 | Loss: 3178.8054\n",
      "Epoch: 097/3000 | Batch 0006/0011 | Loss: 3237.5093\n",
      "Epoch: 097/3000 | Batch 0007/0011 | Loss: 3185.1516\n",
      "Epoch: 097/3000 | Batch 0008/0011 | Loss: 3194.8130\n",
      "Epoch: 097/3000 | Batch 0009/0011 | Loss: 3180.4829\n",
      "Epoch: 097/3000 | Batch 0010/0011 | Loss: 2958.5981\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 098/3000 | Batch 0000/0011 | Loss: 3318.2917\n",
      "Epoch: 098/3000 | Batch 0001/0011 | Loss: 3219.6797\n",
      "Epoch: 098/3000 | Batch 0002/0011 | Loss: 3198.3552\n",
      "Epoch: 098/3000 | Batch 0003/0011 | Loss: 3202.8381\n",
      "Epoch: 098/3000 | Batch 0004/0011 | Loss: 3113.3540\n",
      "Epoch: 098/3000 | Batch 0005/0011 | Loss: 3119.1841\n",
      "Epoch: 098/3000 | Batch 0006/0011 | Loss: 3154.7759\n",
      "Epoch: 098/3000 | Batch 0007/0011 | Loss: 3127.0569\n",
      "Epoch: 098/3000 | Batch 0008/0011 | Loss: 3165.6638\n",
      "Epoch: 098/3000 | Batch 0009/0011 | Loss: 3232.2939\n",
      "Epoch: 098/3000 | Batch 0010/0011 | Loss: 3352.6101\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 099/3000 | Batch 0000/0011 | Loss: 3164.3198\n",
      "Epoch: 099/3000 | Batch 0001/0011 | Loss: 3242.6389\n",
      "Epoch: 099/3000 | Batch 0002/0011 | Loss: 3176.3481\n",
      "Epoch: 099/3000 | Batch 0003/0011 | Loss: 3146.9136\n",
      "Epoch: 099/3000 | Batch 0004/0011 | Loss: 3215.7871\n",
      "Epoch: 099/3000 | Batch 0005/0011 | Loss: 3173.9812\n",
      "Epoch: 099/3000 | Batch 0006/0011 | Loss: 3162.3572\n",
      "Epoch: 099/3000 | Batch 0007/0011 | Loss: 3252.9353\n",
      "Epoch: 099/3000 | Batch 0008/0011 | Loss: 3175.2017\n",
      "Epoch: 099/3000 | Batch 0009/0011 | Loss: 3173.6406\n",
      "Epoch: 099/3000 | Batch 0010/0011 | Loss: 3258.1938\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 100/3000 | Batch 0000/0011 | Loss: 3263.8184\n",
      "Epoch: 100/3000 | Batch 0001/0011 | Loss: 3221.8154\n",
      "Epoch: 100/3000 | Batch 0002/0011 | Loss: 3196.9849\n",
      "Epoch: 100/3000 | Batch 0003/0011 | Loss: 3194.9363\n",
      "Epoch: 100/3000 | Batch 0004/0011 | Loss: 3151.8892\n",
      "Epoch: 100/3000 | Batch 0005/0011 | Loss: 3160.6189\n",
      "Epoch: 100/3000 | Batch 0006/0011 | Loss: 3218.1521\n",
      "Epoch: 100/3000 | Batch 0007/0011 | Loss: 3244.9758\n",
      "Epoch: 100/3000 | Batch 0008/0011 | Loss: 3168.1008\n",
      "Epoch: 100/3000 | Batch 0009/0011 | Loss: 3128.8391\n",
      "Epoch: 100/3000 | Batch 0010/0011 | Loss: 3191.1118\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 101/3000 | Batch 0000/0011 | Loss: 3249.8044\n",
      "Epoch: 101/3000 | Batch 0001/0011 | Loss: 3190.8887\n",
      "Epoch: 101/3000 | Batch 0002/0011 | Loss: 3143.7637\n",
      "Epoch: 101/3000 | Batch 0003/0011 | Loss: 3281.9146\n",
      "Epoch: 101/3000 | Batch 0004/0011 | Loss: 3206.3301\n",
      "Epoch: 101/3000 | Batch 0005/0011 | Loss: 3083.6841\n",
      "Epoch: 101/3000 | Batch 0006/0011 | Loss: 3171.0369\n",
      "Epoch: 101/3000 | Batch 0007/0011 | Loss: 3183.6079\n",
      "Epoch: 101/3000 | Batch 0008/0011 | Loss: 3165.5618\n",
      "Epoch: 101/3000 | Batch 0009/0011 | Loss: 3176.7676\n",
      "Epoch: 101/3000 | Batch 0010/0011 | Loss: 3194.8350\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 102/3000 | Batch 0000/0011 | Loss: 3233.9565\n",
      "Epoch: 102/3000 | Batch 0001/0011 | Loss: 3172.3989\n",
      "Epoch: 102/3000 | Batch 0002/0011 | Loss: 3163.7468\n",
      "Epoch: 102/3000 | Batch 0003/0011 | Loss: 3104.9995\n",
      "Epoch: 102/3000 | Batch 0004/0011 | Loss: 3217.3633\n",
      "Epoch: 102/3000 | Batch 0005/0011 | Loss: 3216.8411\n",
      "Epoch: 102/3000 | Batch 0006/0011 | Loss: 3196.3191\n",
      "Epoch: 102/3000 | Batch 0007/0011 | Loss: 3188.0635\n",
      "Epoch: 102/3000 | Batch 0008/0011 | Loss: 3205.7820\n",
      "Epoch: 102/3000 | Batch 0009/0011 | Loss: 3183.2139\n",
      "Epoch: 102/3000 | Batch 0010/0011 | Loss: 3119.2668\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 103/3000 | Batch 0000/0011 | Loss: 3254.4133\n",
      "Epoch: 103/3000 | Batch 0001/0011 | Loss: 3310.4324\n",
      "Epoch: 103/3000 | Batch 0002/0011 | Loss: 3143.4458\n",
      "Epoch: 103/3000 | Batch 0003/0011 | Loss: 3177.4961\n",
      "Epoch: 103/3000 | Batch 0004/0011 | Loss: 3226.4514\n",
      "Epoch: 103/3000 | Batch 0005/0011 | Loss: 3185.1335\n",
      "Epoch: 103/3000 | Batch 0006/0011 | Loss: 3122.6926\n",
      "Epoch: 103/3000 | Batch 0007/0011 | Loss: 3202.5742\n",
      "Epoch: 103/3000 | Batch 0008/0011 | Loss: 3130.2148\n",
      "Epoch: 103/3000 | Batch 0009/0011 | Loss: 3196.2512\n",
      "Epoch: 103/3000 | Batch 0010/0011 | Loss: 3070.0508\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 104/3000 | Batch 0000/0011 | Loss: 3192.7432\n",
      "Epoch: 104/3000 | Batch 0001/0011 | Loss: 3148.1567\n",
      "Epoch: 104/3000 | Batch 0002/0011 | Loss: 3186.7336\n",
      "Epoch: 104/3000 | Batch 0003/0011 | Loss: 3109.1560\n",
      "Epoch: 104/3000 | Batch 0004/0011 | Loss: 3225.6111\n",
      "Epoch: 104/3000 | Batch 0005/0011 | Loss: 3248.5110\n",
      "Epoch: 104/3000 | Batch 0006/0011 | Loss: 3158.6299\n",
      "Epoch: 104/3000 | Batch 0007/0011 | Loss: 3240.7888\n",
      "Epoch: 104/3000 | Batch 0008/0011 | Loss: 3174.0344\n",
      "Epoch: 104/3000 | Batch 0009/0011 | Loss: 3167.3860\n",
      "Epoch: 104/3000 | Batch 0010/0011 | Loss: 3360.1323\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 105/3000 | Batch 0000/0011 | Loss: 3147.8850\n",
      "Epoch: 105/3000 | Batch 0001/0011 | Loss: 3186.0217\n",
      "Epoch: 105/3000 | Batch 0002/0011 | Loss: 3276.1938\n",
      "Epoch: 105/3000 | Batch 0003/0011 | Loss: 3171.3628\n",
      "Epoch: 105/3000 | Batch 0004/0011 | Loss: 3203.9641\n",
      "Epoch: 105/3000 | Batch 0005/0011 | Loss: 3130.5823\n",
      "Epoch: 105/3000 | Batch 0006/0011 | Loss: 3158.3870\n",
      "Epoch: 105/3000 | Batch 0007/0011 | Loss: 3163.1106\n",
      "Epoch: 105/3000 | Batch 0008/0011 | Loss: 3162.6499\n",
      "Epoch: 105/3000 | Batch 0009/0011 | Loss: 3237.7874\n",
      "Epoch: 105/3000 | Batch 0010/0011 | Loss: 3184.8608\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 106/3000 | Batch 0000/0011 | Loss: 3155.1370\n",
      "Epoch: 106/3000 | Batch 0001/0011 | Loss: 3172.0493\n",
      "Epoch: 106/3000 | Batch 0002/0011 | Loss: 3195.6895\n",
      "Epoch: 106/3000 | Batch 0003/0011 | Loss: 3170.9353\n",
      "Epoch: 106/3000 | Batch 0004/0011 | Loss: 3155.9771\n",
      "Epoch: 106/3000 | Batch 0005/0011 | Loss: 3234.1624\n",
      "Epoch: 106/3000 | Batch 0006/0011 | Loss: 3210.1045\n",
      "Epoch: 106/3000 | Batch 0007/0011 | Loss: 3232.1467\n",
      "Epoch: 106/3000 | Batch 0008/0011 | Loss: 3128.9517\n",
      "Epoch: 106/3000 | Batch 0009/0011 | Loss: 3206.7373\n",
      "Epoch: 106/3000 | Batch 0010/0011 | Loss: 3235.4143\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 107/3000 | Batch 0000/0011 | Loss: 3166.1169\n",
      "Epoch: 107/3000 | Batch 0001/0011 | Loss: 3179.5212\n",
      "Epoch: 107/3000 | Batch 0002/0011 | Loss: 3182.7163\n",
      "Epoch: 107/3000 | Batch 0003/0011 | Loss: 3185.5852\n",
      "Epoch: 107/3000 | Batch 0004/0011 | Loss: 3247.1677\n",
      "Epoch: 107/3000 | Batch 0005/0011 | Loss: 3136.1050\n",
      "Epoch: 107/3000 | Batch 0006/0011 | Loss: 3262.6702\n",
      "Epoch: 107/3000 | Batch 0007/0011 | Loss: 3151.0168\n",
      "Epoch: 107/3000 | Batch 0008/0011 | Loss: 3147.7363\n",
      "Epoch: 107/3000 | Batch 0009/0011 | Loss: 3189.9060\n",
      "Epoch: 107/3000 | Batch 0010/0011 | Loss: 3243.6741\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 108/3000 | Batch 0000/0011 | Loss: 3195.3999\n",
      "Epoch: 108/3000 | Batch 0001/0011 | Loss: 3140.6411\n",
      "Epoch: 108/3000 | Batch 0002/0011 | Loss: 3281.2598\n",
      "Epoch: 108/3000 | Batch 0003/0011 | Loss: 3251.9138\n",
      "Epoch: 108/3000 | Batch 0004/0011 | Loss: 3206.8904\n",
      "Epoch: 108/3000 | Batch 0005/0011 | Loss: 3158.9299\n",
      "Epoch: 108/3000 | Batch 0006/0011 | Loss: 3163.8977\n",
      "Epoch: 108/3000 | Batch 0007/0011 | Loss: 3197.3174\n",
      "Epoch: 108/3000 | Batch 0008/0011 | Loss: 3111.9282\n",
      "Epoch: 108/3000 | Batch 0009/0011 | Loss: 3166.3201\n",
      "Epoch: 108/3000 | Batch 0010/0011 | Loss: 3199.2612\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 109/3000 | Batch 0000/0011 | Loss: 3179.8152\n",
      "Epoch: 109/3000 | Batch 0001/0011 | Loss: 3154.2654\n",
      "Epoch: 109/3000 | Batch 0002/0011 | Loss: 3173.1272\n",
      "Epoch: 109/3000 | Batch 0003/0011 | Loss: 3172.7517\n",
      "Epoch: 109/3000 | Batch 0004/0011 | Loss: 3244.6638\n",
      "Epoch: 109/3000 | Batch 0005/0011 | Loss: 3249.1064\n",
      "Epoch: 109/3000 | Batch 0006/0011 | Loss: 3209.5652\n",
      "Epoch: 109/3000 | Batch 0007/0011 | Loss: 3193.2295\n",
      "Epoch: 109/3000 | Batch 0008/0011 | Loss: 3131.4595\n",
      "Epoch: 109/3000 | Batch 0009/0011 | Loss: 3172.4954\n",
      "Epoch: 109/3000 | Batch 0010/0011 | Loss: 3053.2007\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 110/3000 | Batch 0000/0011 | Loss: 3255.7939\n",
      "Epoch: 110/3000 | Batch 0001/0011 | Loss: 3216.4983\n",
      "Epoch: 110/3000 | Batch 0002/0011 | Loss: 3192.1807\n",
      "Epoch: 110/3000 | Batch 0003/0011 | Loss: 3137.4187\n",
      "Epoch: 110/3000 | Batch 0004/0011 | Loss: 3168.8186\n",
      "Epoch: 110/3000 | Batch 0005/0011 | Loss: 3089.6978\n",
      "Epoch: 110/3000 | Batch 0006/0011 | Loss: 3164.6208\n",
      "Epoch: 110/3000 | Batch 0007/0011 | Loss: 3239.5164\n",
      "Epoch: 110/3000 | Batch 0008/0011 | Loss: 3233.2585\n",
      "Epoch: 110/3000 | Batch 0009/0011 | Loss: 3136.6191\n",
      "Epoch: 110/3000 | Batch 0010/0011 | Loss: 3040.5212\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 111/3000 | Batch 0000/0011 | Loss: 3162.6812\n",
      "Epoch: 111/3000 | Batch 0001/0011 | Loss: 3268.1545\n",
      "Epoch: 111/3000 | Batch 0002/0011 | Loss: 3211.2683\n",
      "Epoch: 111/3000 | Batch 0003/0011 | Loss: 3182.3379\n",
      "Epoch: 111/3000 | Batch 0004/0011 | Loss: 3197.0229\n",
      "Epoch: 111/3000 | Batch 0005/0011 | Loss: 3144.6299\n",
      "Epoch: 111/3000 | Batch 0006/0011 | Loss: 3158.7908\n",
      "Epoch: 111/3000 | Batch 0007/0011 | Loss: 3188.0610\n",
      "Epoch: 111/3000 | Batch 0008/0011 | Loss: 3130.7644\n",
      "Epoch: 111/3000 | Batch 0009/0011 | Loss: 3170.0669\n",
      "Epoch: 111/3000 | Batch 0010/0011 | Loss: 3239.7598\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 112/3000 | Batch 0000/0011 | Loss: 3148.6199\n",
      "Epoch: 112/3000 | Batch 0001/0011 | Loss: 3144.4688\n",
      "Epoch: 112/3000 | Batch 0002/0011 | Loss: 3176.3767\n",
      "Epoch: 112/3000 | Batch 0003/0011 | Loss: 3128.2961\n",
      "Epoch: 112/3000 | Batch 0004/0011 | Loss: 3156.3953\n",
      "Epoch: 112/3000 | Batch 0005/0011 | Loss: 3157.8196\n",
      "Epoch: 112/3000 | Batch 0006/0011 | Loss: 3213.7954\n",
      "Epoch: 112/3000 | Batch 0007/0011 | Loss: 3203.1431\n",
      "Epoch: 112/3000 | Batch 0008/0011 | Loss: 3295.1296\n",
      "Epoch: 112/3000 | Batch 0009/0011 | Loss: 3196.7607\n",
      "Epoch: 112/3000 | Batch 0010/0011 | Loss: 3153.4119\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 113/3000 | Batch 0000/0011 | Loss: 3243.0466\n",
      "Epoch: 113/3000 | Batch 0001/0011 | Loss: 3183.4951\n",
      "Epoch: 113/3000 | Batch 0002/0011 | Loss: 3151.1042\n",
      "Epoch: 113/3000 | Batch 0003/0011 | Loss: 3241.6670\n",
      "Epoch: 113/3000 | Batch 0004/0011 | Loss: 3223.8469\n",
      "Epoch: 113/3000 | Batch 0005/0011 | Loss: 3172.4700\n",
      "Epoch: 113/3000 | Batch 0006/0011 | Loss: 3149.3037\n",
      "Epoch: 113/3000 | Batch 0007/0011 | Loss: 3170.7271\n",
      "Epoch: 113/3000 | Batch 0008/0011 | Loss: 3149.8560\n",
      "Epoch: 113/3000 | Batch 0009/0011 | Loss: 3095.7834\n",
      "Epoch: 113/3000 | Batch 0010/0011 | Loss: 3195.3008\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 114/3000 | Batch 0000/0011 | Loss: 3141.4648\n",
      "Epoch: 114/3000 | Batch 0001/0011 | Loss: 3253.3350\n",
      "Epoch: 114/3000 | Batch 0002/0011 | Loss: 3141.4275\n",
      "Epoch: 114/3000 | Batch 0003/0011 | Loss: 3272.4399\n",
      "Epoch: 114/3000 | Batch 0004/0011 | Loss: 3108.9297\n",
      "Epoch: 114/3000 | Batch 0005/0011 | Loss: 3124.3005\n",
      "Epoch: 114/3000 | Batch 0006/0011 | Loss: 3276.8010\n",
      "Epoch: 114/3000 | Batch 0007/0011 | Loss: 3107.2532\n",
      "Epoch: 114/3000 | Batch 0008/0011 | Loss: 3219.2085\n",
      "Epoch: 114/3000 | Batch 0009/0011 | Loss: 3122.4329\n",
      "Epoch: 114/3000 | Batch 0010/0011 | Loss: 3071.6995\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 115/3000 | Batch 0000/0011 | Loss: 3159.4690\n",
      "Epoch: 115/3000 | Batch 0001/0011 | Loss: 3150.6729\n",
      "Epoch: 115/3000 | Batch 0002/0011 | Loss: 3112.7766\n",
      "Epoch: 115/3000 | Batch 0003/0011 | Loss: 3239.5430\n",
      "Epoch: 115/3000 | Batch 0004/0011 | Loss: 3192.4294\n",
      "Epoch: 115/3000 | Batch 0005/0011 | Loss: 3202.9749\n",
      "Epoch: 115/3000 | Batch 0006/0011 | Loss: 3210.6235\n",
      "Epoch: 115/3000 | Batch 0007/0011 | Loss: 3275.1523\n",
      "Epoch: 115/3000 | Batch 0008/0011 | Loss: 3204.5654\n",
      "Epoch: 115/3000 | Batch 0009/0011 | Loss: 3107.0283\n",
      "Epoch: 115/3000 | Batch 0010/0011 | Loss: 3156.8057\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 116/3000 | Batch 0000/0011 | Loss: 3125.1860\n",
      "Epoch: 116/3000 | Batch 0001/0011 | Loss: 3151.0930\n",
      "Epoch: 116/3000 | Batch 0002/0011 | Loss: 3200.0186\n",
      "Epoch: 116/3000 | Batch 0003/0011 | Loss: 3143.2812\n",
      "Epoch: 116/3000 | Batch 0004/0011 | Loss: 3216.6426\n",
      "Epoch: 116/3000 | Batch 0005/0011 | Loss: 3259.6516\n",
      "Epoch: 116/3000 | Batch 0006/0011 | Loss: 3152.1453\n",
      "Epoch: 116/3000 | Batch 0007/0011 | Loss: 3090.3196\n",
      "Epoch: 116/3000 | Batch 0008/0011 | Loss: 3327.1084\n",
      "Epoch: 116/3000 | Batch 0009/0011 | Loss: 3143.8025\n",
      "Epoch: 116/3000 | Batch 0010/0011 | Loss: 3228.3745\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 117/3000 | Batch 0000/0011 | Loss: 3139.5164\n",
      "Epoch: 117/3000 | Batch 0001/0011 | Loss: 3202.3374\n",
      "Epoch: 117/3000 | Batch 0002/0011 | Loss: 3138.2170\n",
      "Epoch: 117/3000 | Batch 0003/0011 | Loss: 3153.4341\n",
      "Epoch: 117/3000 | Batch 0004/0011 | Loss: 3297.6001\n",
      "Epoch: 117/3000 | Batch 0005/0011 | Loss: 3294.8074\n",
      "Epoch: 117/3000 | Batch 0006/0011 | Loss: 3162.7510\n",
      "Epoch: 117/3000 | Batch 0007/0011 | Loss: 3165.4048\n",
      "Epoch: 117/3000 | Batch 0008/0011 | Loss: 3156.5454\n",
      "Epoch: 117/3000 | Batch 0009/0011 | Loss: 3148.6111\n",
      "Epoch: 117/3000 | Batch 0010/0011 | Loss: 3245.5908\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 118/3000 | Batch 0000/0011 | Loss: 3075.8540\n",
      "Epoch: 118/3000 | Batch 0001/0011 | Loss: 3234.3931\n",
      "Epoch: 118/3000 | Batch 0002/0011 | Loss: 3120.1885\n",
      "Epoch: 118/3000 | Batch 0003/0011 | Loss: 3171.4790\n",
      "Epoch: 118/3000 | Batch 0004/0011 | Loss: 3170.0002\n",
      "Epoch: 118/3000 | Batch 0005/0011 | Loss: 3153.1631\n",
      "Epoch: 118/3000 | Batch 0006/0011 | Loss: 3218.2007\n",
      "Epoch: 118/3000 | Batch 0007/0011 | Loss: 3183.2605\n",
      "Epoch: 118/3000 | Batch 0008/0011 | Loss: 3212.9612\n",
      "Epoch: 118/3000 | Batch 0009/0011 | Loss: 3139.1731\n",
      "Epoch: 118/3000 | Batch 0010/0011 | Loss: 3609.0276\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 119/3000 | Batch 0000/0011 | Loss: 3136.7603\n",
      "Epoch: 119/3000 | Batch 0001/0011 | Loss: 3207.9731\n",
      "Epoch: 119/3000 | Batch 0002/0011 | Loss: 3110.8960\n",
      "Epoch: 119/3000 | Batch 0003/0011 | Loss: 3223.1248\n",
      "Epoch: 119/3000 | Batch 0004/0011 | Loss: 3169.9795\n",
      "Epoch: 119/3000 | Batch 0005/0011 | Loss: 3191.2285\n",
      "Epoch: 119/3000 | Batch 0006/0011 | Loss: 3127.2507\n",
      "Epoch: 119/3000 | Batch 0007/0011 | Loss: 3131.3213\n",
      "Epoch: 119/3000 | Batch 0008/0011 | Loss: 3212.3523\n",
      "Epoch: 119/3000 | Batch 0009/0011 | Loss: 3238.2515\n",
      "Epoch: 119/3000 | Batch 0010/0011 | Loss: 3299.2783\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 120/3000 | Batch 0000/0011 | Loss: 3193.7261\n",
      "Epoch: 120/3000 | Batch 0001/0011 | Loss: 3194.3875\n",
      "Epoch: 120/3000 | Batch 0002/0011 | Loss: 3134.3281\n",
      "Epoch: 120/3000 | Batch 0003/0011 | Loss: 3216.6270\n",
      "Epoch: 120/3000 | Batch 0004/0011 | Loss: 3209.6772\n",
      "Epoch: 120/3000 | Batch 0005/0011 | Loss: 3116.1697\n",
      "Epoch: 120/3000 | Batch 0006/0011 | Loss: 3143.0498\n",
      "Epoch: 120/3000 | Batch 0007/0011 | Loss: 3170.0215\n",
      "Epoch: 120/3000 | Batch 0008/0011 | Loss: 3133.0039\n",
      "Epoch: 120/3000 | Batch 0009/0011 | Loss: 3247.2361\n",
      "Epoch: 120/3000 | Batch 0010/0011 | Loss: 3242.5278\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 121/3000 | Batch 0000/0011 | Loss: 3205.2336\n",
      "Epoch: 121/3000 | Batch 0001/0011 | Loss: 3173.5532\n",
      "Epoch: 121/3000 | Batch 0002/0011 | Loss: 3221.3984\n",
      "Epoch: 121/3000 | Batch 0003/0011 | Loss: 3172.3940\n",
      "Epoch: 121/3000 | Batch 0004/0011 | Loss: 3174.4246\n",
      "Epoch: 121/3000 | Batch 0005/0011 | Loss: 3131.9504\n",
      "Epoch: 121/3000 | Batch 0006/0011 | Loss: 3164.7852\n",
      "Epoch: 121/3000 | Batch 0007/0011 | Loss: 3200.8311\n",
      "Epoch: 121/3000 | Batch 0008/0011 | Loss: 3152.1064\n",
      "Epoch: 121/3000 | Batch 0009/0011 | Loss: 3185.4421\n",
      "Epoch: 121/3000 | Batch 0010/0011 | Loss: 3044.4988\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 122/3000 | Batch 0000/0011 | Loss: 3156.9165\n",
      "Epoch: 122/3000 | Batch 0001/0011 | Loss: 3147.3162\n",
      "Epoch: 122/3000 | Batch 0002/0011 | Loss: 3142.5732\n",
      "Epoch: 122/3000 | Batch 0003/0011 | Loss: 3203.6416\n",
      "Epoch: 122/3000 | Batch 0004/0011 | Loss: 3205.9163\n",
      "Epoch: 122/3000 | Batch 0005/0011 | Loss: 3104.1858\n",
      "Epoch: 122/3000 | Batch 0006/0011 | Loss: 3231.0000\n",
      "Epoch: 122/3000 | Batch 0007/0011 | Loss: 3213.5461\n",
      "Epoch: 122/3000 | Batch 0008/0011 | Loss: 3192.3079\n",
      "Epoch: 122/3000 | Batch 0009/0011 | Loss: 3184.2917\n",
      "Epoch: 122/3000 | Batch 0010/0011 | Loss: 3211.3445\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 123/3000 | Batch 0000/0011 | Loss: 3222.3870\n",
      "Epoch: 123/3000 | Batch 0001/0011 | Loss: 3138.8333\n",
      "Epoch: 123/3000 | Batch 0002/0011 | Loss: 3195.1130\n",
      "Epoch: 123/3000 | Batch 0003/0011 | Loss: 3179.8840\n",
      "Epoch: 123/3000 | Batch 0004/0011 | Loss: 3193.4004\n",
      "Epoch: 123/3000 | Batch 0005/0011 | Loss: 3186.9463\n",
      "Epoch: 123/3000 | Batch 0006/0011 | Loss: 3114.3179\n",
      "Epoch: 123/3000 | Batch 0007/0011 | Loss: 3216.2612\n",
      "Epoch: 123/3000 | Batch 0008/0011 | Loss: 3209.6826\n",
      "Epoch: 123/3000 | Batch 0009/0011 | Loss: 3137.3345\n",
      "Epoch: 123/3000 | Batch 0010/0011 | Loss: 2980.1560\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 124/3000 | Batch 0000/0011 | Loss: 3188.7847\n",
      "Epoch: 124/3000 | Batch 0001/0011 | Loss: 3226.8174\n",
      "Epoch: 124/3000 | Batch 0002/0011 | Loss: 3113.1196\n",
      "Epoch: 124/3000 | Batch 0003/0011 | Loss: 3212.1816\n",
      "Epoch: 124/3000 | Batch 0004/0011 | Loss: 3230.0759\n",
      "Epoch: 124/3000 | Batch 0005/0011 | Loss: 3187.2480\n",
      "Epoch: 124/3000 | Batch 0006/0011 | Loss: 3144.7161\n",
      "Epoch: 124/3000 | Batch 0007/0011 | Loss: 3157.7017\n",
      "Epoch: 124/3000 | Batch 0008/0011 | Loss: 3186.2681\n",
      "Epoch: 124/3000 | Batch 0009/0011 | Loss: 3160.8440\n",
      "Epoch: 124/3000 | Batch 0010/0011 | Loss: 3151.5288\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 125/3000 | Batch 0000/0011 | Loss: 3134.8940\n",
      "Epoch: 125/3000 | Batch 0001/0011 | Loss: 3161.6653\n",
      "Epoch: 125/3000 | Batch 0002/0011 | Loss: 3184.6880\n",
      "Epoch: 125/3000 | Batch 0003/0011 | Loss: 3178.1106\n",
      "Epoch: 125/3000 | Batch 0004/0011 | Loss: 3166.2927\n",
      "Epoch: 125/3000 | Batch 0005/0011 | Loss: 3206.2175\n",
      "Epoch: 125/3000 | Batch 0006/0011 | Loss: 3165.3599\n",
      "Epoch: 125/3000 | Batch 0007/0011 | Loss: 3207.9932\n",
      "Epoch: 125/3000 | Batch 0008/0011 | Loss: 3174.3872\n",
      "Epoch: 125/3000 | Batch 0009/0011 | Loss: 3226.3088\n",
      "Epoch: 125/3000 | Batch 0010/0011 | Loss: 3051.4241\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 126/3000 | Batch 0000/0011 | Loss: 3096.5654\n",
      "Epoch: 126/3000 | Batch 0001/0011 | Loss: 3168.9956\n",
      "Epoch: 126/3000 | Batch 0002/0011 | Loss: 3136.4146\n",
      "Epoch: 126/3000 | Batch 0003/0011 | Loss: 3144.1875\n",
      "Epoch: 126/3000 | Batch 0004/0011 | Loss: 3186.2878\n",
      "Epoch: 126/3000 | Batch 0005/0011 | Loss: 3234.0647\n",
      "Epoch: 126/3000 | Batch 0006/0011 | Loss: 3219.1978\n",
      "Epoch: 126/3000 | Batch 0007/0011 | Loss: 3203.7729\n",
      "Epoch: 126/3000 | Batch 0008/0011 | Loss: 3167.9250\n",
      "Epoch: 126/3000 | Batch 0009/0011 | Loss: 3302.2585\n",
      "Epoch: 126/3000 | Batch 0010/0011 | Loss: 3147.6416\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 127/3000 | Batch 0000/0011 | Loss: 3123.8481\n",
      "Epoch: 127/3000 | Batch 0001/0011 | Loss: 3180.5532\n",
      "Epoch: 127/3000 | Batch 0002/0011 | Loss: 3207.2075\n",
      "Epoch: 127/3000 | Batch 0003/0011 | Loss: 3227.4302\n",
      "Epoch: 127/3000 | Batch 0004/0011 | Loss: 3238.3660\n",
      "Epoch: 127/3000 | Batch 0005/0011 | Loss: 3129.6609\n",
      "Epoch: 127/3000 | Batch 0006/0011 | Loss: 3203.5205\n",
      "Epoch: 127/3000 | Batch 0007/0011 | Loss: 3141.0886\n",
      "Epoch: 127/3000 | Batch 0008/0011 | Loss: 3173.0256\n",
      "Epoch: 127/3000 | Batch 0009/0011 | Loss: 3144.2866\n",
      "Epoch: 127/3000 | Batch 0010/0011 | Loss: 3009.7026\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 128/3000 | Batch 0000/0011 | Loss: 3202.9919\n",
      "Epoch: 128/3000 | Batch 0001/0011 | Loss: 3192.4609\n",
      "Epoch: 128/3000 | Batch 0002/0011 | Loss: 3184.2219\n",
      "Epoch: 128/3000 | Batch 0003/0011 | Loss: 3163.3899\n",
      "Epoch: 128/3000 | Batch 0004/0011 | Loss: 3138.6191\n",
      "Epoch: 128/3000 | Batch 0005/0011 | Loss: 3195.0151\n",
      "Epoch: 128/3000 | Batch 0006/0011 | Loss: 3165.4595\n",
      "Epoch: 128/3000 | Batch 0007/0011 | Loss: 3148.8672\n",
      "Epoch: 128/3000 | Batch 0008/0011 | Loss: 3209.7969\n",
      "Epoch: 128/3000 | Batch 0009/0011 | Loss: 3169.1904\n",
      "Epoch: 128/3000 | Batch 0010/0011 | Loss: 3199.1013\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 129/3000 | Batch 0000/0011 | Loss: 3135.2341\n",
      "Epoch: 129/3000 | Batch 0001/0011 | Loss: 3180.1621\n",
      "Epoch: 129/3000 | Batch 0002/0011 | Loss: 3189.4788\n",
      "Epoch: 129/3000 | Batch 0003/0011 | Loss: 3269.7415\n",
      "Epoch: 129/3000 | Batch 0004/0011 | Loss: 3148.5347\n",
      "Epoch: 129/3000 | Batch 0005/0011 | Loss: 3157.0984\n",
      "Epoch: 129/3000 | Batch 0006/0011 | Loss: 3181.0723\n",
      "Epoch: 129/3000 | Batch 0007/0011 | Loss: 3175.6140\n",
      "Epoch: 129/3000 | Batch 0008/0011 | Loss: 3139.3279\n",
      "Epoch: 129/3000 | Batch 0009/0011 | Loss: 3249.1399\n",
      "Epoch: 129/3000 | Batch 0010/0011 | Loss: 2989.2981\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 130/3000 | Batch 0000/0011 | Loss: 3190.1545\n",
      "Epoch: 130/3000 | Batch 0001/0011 | Loss: 3217.8625\n",
      "Epoch: 130/3000 | Batch 0002/0011 | Loss: 3255.4233\n",
      "Epoch: 130/3000 | Batch 0003/0011 | Loss: 3116.3503\n",
      "Epoch: 130/3000 | Batch 0004/0011 | Loss: 3136.7231\n",
      "Epoch: 130/3000 | Batch 0005/0011 | Loss: 3175.2620\n",
      "Epoch: 130/3000 | Batch 0006/0011 | Loss: 3158.9631\n",
      "Epoch: 130/3000 | Batch 0007/0011 | Loss: 3129.8860\n",
      "Epoch: 130/3000 | Batch 0008/0011 | Loss: 3244.5037\n",
      "Epoch: 130/3000 | Batch 0009/0011 | Loss: 3103.9028\n",
      "Epoch: 130/3000 | Batch 0010/0011 | Loss: 3118.7625\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 131/3000 | Batch 0000/0011 | Loss: 3205.5791\n",
      "Epoch: 131/3000 | Batch 0001/0011 | Loss: 3211.7107\n",
      "Epoch: 131/3000 | Batch 0002/0011 | Loss: 3164.0596\n",
      "Epoch: 131/3000 | Batch 0003/0011 | Loss: 3203.4839\n",
      "Epoch: 131/3000 | Batch 0004/0011 | Loss: 3156.7185\n",
      "Epoch: 131/3000 | Batch 0005/0011 | Loss: 3139.0508\n",
      "Epoch: 131/3000 | Batch 0006/0011 | Loss: 3268.2126\n",
      "Epoch: 131/3000 | Batch 0007/0011 | Loss: 3104.2747\n",
      "Epoch: 131/3000 | Batch 0008/0011 | Loss: 3185.2954\n",
      "Epoch: 131/3000 | Batch 0009/0011 | Loss: 3179.3342\n",
      "Epoch: 131/3000 | Batch 0010/0011 | Loss: 3108.5525\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 132/3000 | Batch 0000/0011 | Loss: 3163.5410\n",
      "Epoch: 132/3000 | Batch 0001/0011 | Loss: 3223.1077\n",
      "Epoch: 132/3000 | Batch 0002/0011 | Loss: 3186.7295\n",
      "Epoch: 132/3000 | Batch 0003/0011 | Loss: 3185.1985\n",
      "Epoch: 132/3000 | Batch 0004/0011 | Loss: 3217.9500\n",
      "Epoch: 132/3000 | Batch 0005/0011 | Loss: 3177.2449\n",
      "Epoch: 132/3000 | Batch 0006/0011 | Loss: 3122.4463\n",
      "Epoch: 132/3000 | Batch 0007/0011 | Loss: 3195.5925\n",
      "Epoch: 132/3000 | Batch 0008/0011 | Loss: 3135.7229\n",
      "Epoch: 132/3000 | Batch 0009/0011 | Loss: 3212.3103\n",
      "Epoch: 132/3000 | Batch 0010/0011 | Loss: 3203.5825\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 133/3000 | Batch 0000/0011 | Loss: 3172.5454\n",
      "Epoch: 133/3000 | Batch 0001/0011 | Loss: 3211.2358\n",
      "Epoch: 133/3000 | Batch 0002/0011 | Loss: 3157.9319\n",
      "Epoch: 133/3000 | Batch 0003/0011 | Loss: 3139.2109\n",
      "Epoch: 133/3000 | Batch 0004/0011 | Loss: 3155.5330\n",
      "Epoch: 133/3000 | Batch 0005/0011 | Loss: 3131.7808\n",
      "Epoch: 133/3000 | Batch 0006/0011 | Loss: 3163.7153\n",
      "Epoch: 133/3000 | Batch 0007/0011 | Loss: 3166.8667\n",
      "Epoch: 133/3000 | Batch 0008/0011 | Loss: 3313.7461\n",
      "Epoch: 133/3000 | Batch 0009/0011 | Loss: 3201.3599\n",
      "Epoch: 133/3000 | Batch 0010/0011 | Loss: 3082.4695\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 134/3000 | Batch 0000/0011 | Loss: 3240.3818\n",
      "Epoch: 134/3000 | Batch 0001/0011 | Loss: 3159.8115\n",
      "Epoch: 134/3000 | Batch 0002/0011 | Loss: 3180.7515\n",
      "Epoch: 134/3000 | Batch 0003/0011 | Loss: 3195.2058\n",
      "Epoch: 134/3000 | Batch 0004/0011 | Loss: 3128.9814\n",
      "Epoch: 134/3000 | Batch 0005/0011 | Loss: 3223.6575\n",
      "Epoch: 134/3000 | Batch 0006/0011 | Loss: 3177.2805\n",
      "Epoch: 134/3000 | Batch 0007/0011 | Loss: 3155.9358\n",
      "Epoch: 134/3000 | Batch 0008/0011 | Loss: 3205.3210\n",
      "Epoch: 134/3000 | Batch 0009/0011 | Loss: 3134.8252\n",
      "Epoch: 134/3000 | Batch 0010/0011 | Loss: 3000.4534\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 135/3000 | Batch 0000/0011 | Loss: 3143.4595\n",
      "Epoch: 135/3000 | Batch 0001/0011 | Loss: 3220.0723\n",
      "Epoch: 135/3000 | Batch 0002/0011 | Loss: 3269.1628\n",
      "Epoch: 135/3000 | Batch 0003/0011 | Loss: 3167.2419\n",
      "Epoch: 135/3000 | Batch 0004/0011 | Loss: 3193.8069\n",
      "Epoch: 135/3000 | Batch 0005/0011 | Loss: 3152.3296\n",
      "Epoch: 135/3000 | Batch 0006/0011 | Loss: 3156.7942\n",
      "Epoch: 135/3000 | Batch 0007/0011 | Loss: 3183.5525\n",
      "Epoch: 135/3000 | Batch 0008/0011 | Loss: 3123.7524\n",
      "Epoch: 135/3000 | Batch 0009/0011 | Loss: 3191.3958\n",
      "Epoch: 135/3000 | Batch 0010/0011 | Loss: 3164.3845\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 136/3000 | Batch 0000/0011 | Loss: 3222.4199\n",
      "Epoch: 136/3000 | Batch 0001/0011 | Loss: 3236.1841\n",
      "Epoch: 136/3000 | Batch 0002/0011 | Loss: 3163.7202\n",
      "Epoch: 136/3000 | Batch 0003/0011 | Loss: 3184.9785\n",
      "Epoch: 136/3000 | Batch 0004/0011 | Loss: 3154.0417\n",
      "Epoch: 136/3000 | Batch 0005/0011 | Loss: 3220.3955\n",
      "Epoch: 136/3000 | Batch 0006/0011 | Loss: 3189.8501\n",
      "Epoch: 136/3000 | Batch 0007/0011 | Loss: 3174.5098\n",
      "Epoch: 136/3000 | Batch 0008/0011 | Loss: 3094.4448\n",
      "Epoch: 136/3000 | Batch 0009/0011 | Loss: 3124.4180\n",
      "Epoch: 136/3000 | Batch 0010/0011 | Loss: 3167.5757\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 137/3000 | Batch 0000/0011 | Loss: 3243.9834\n",
      "Epoch: 137/3000 | Batch 0001/0011 | Loss: 3243.8867\n",
      "Epoch: 137/3000 | Batch 0002/0011 | Loss: 3153.5435\n",
      "Epoch: 137/3000 | Batch 0003/0011 | Loss: 3206.3481\n",
      "Epoch: 137/3000 | Batch 0004/0011 | Loss: 3150.4331\n",
      "Epoch: 137/3000 | Batch 0005/0011 | Loss: 3171.7795\n",
      "Epoch: 137/3000 | Batch 0006/0011 | Loss: 3155.2646\n",
      "Epoch: 137/3000 | Batch 0007/0011 | Loss: 3134.1111\n",
      "Epoch: 137/3000 | Batch 0008/0011 | Loss: 3138.2693\n",
      "Epoch: 137/3000 | Batch 0009/0011 | Loss: 3139.1943\n",
      "Epoch: 137/3000 | Batch 0010/0011 | Loss: 3477.2363\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 138/3000 | Batch 0000/0011 | Loss: 3196.5564\n",
      "Epoch: 138/3000 | Batch 0001/0011 | Loss: 3150.2747\n",
      "Epoch: 138/3000 | Batch 0002/0011 | Loss: 3117.6963\n",
      "Epoch: 138/3000 | Batch 0003/0011 | Loss: 3166.2175\n",
      "Epoch: 138/3000 | Batch 0004/0011 | Loss: 3214.2910\n",
      "Epoch: 138/3000 | Batch 0005/0011 | Loss: 3188.2029\n",
      "Epoch: 138/3000 | Batch 0006/0011 | Loss: 3213.1438\n",
      "Epoch: 138/3000 | Batch 0007/0011 | Loss: 3095.0576\n",
      "Epoch: 138/3000 | Batch 0008/0011 | Loss: 3211.0950\n",
      "Epoch: 138/3000 | Batch 0009/0011 | Loss: 3183.5037\n",
      "Epoch: 138/3000 | Batch 0010/0011 | Loss: 3216.2769\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 139/3000 | Batch 0000/0011 | Loss: 3157.1887\n",
      "Epoch: 139/3000 | Batch 0001/0011 | Loss: 3174.1252\n",
      "Epoch: 139/3000 | Batch 0002/0011 | Loss: 3191.3567\n",
      "Epoch: 139/3000 | Batch 0003/0011 | Loss: 3145.0684\n",
      "Epoch: 139/3000 | Batch 0004/0011 | Loss: 3151.2263\n",
      "Epoch: 139/3000 | Batch 0005/0011 | Loss: 3227.9751\n",
      "Epoch: 139/3000 | Batch 0006/0011 | Loss: 3188.4109\n",
      "Epoch: 139/3000 | Batch 0007/0011 | Loss: 3197.7280\n",
      "Epoch: 139/3000 | Batch 0008/0011 | Loss: 3170.4399\n",
      "Epoch: 139/3000 | Batch 0009/0011 | Loss: 3155.6438\n",
      "Epoch: 139/3000 | Batch 0010/0011 | Loss: 3004.2725\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 140/3000 | Batch 0000/0011 | Loss: 3153.6172\n",
      "Epoch: 140/3000 | Batch 0001/0011 | Loss: 3210.7471\n",
      "Epoch: 140/3000 | Batch 0002/0011 | Loss: 3142.3779\n",
      "Epoch: 140/3000 | Batch 0003/0011 | Loss: 3201.6917\n",
      "Epoch: 140/3000 | Batch 0004/0011 | Loss: 3203.0393\n",
      "Epoch: 140/3000 | Batch 0005/0011 | Loss: 3144.2673\n",
      "Epoch: 140/3000 | Batch 0006/0011 | Loss: 3219.2732\n",
      "Epoch: 140/3000 | Batch 0007/0011 | Loss: 3180.6184\n",
      "Epoch: 140/3000 | Batch 0008/0011 | Loss: 3222.0188\n",
      "Epoch: 140/3000 | Batch 0009/0011 | Loss: 3137.9482\n",
      "Epoch: 140/3000 | Batch 0010/0011 | Loss: 3108.8245\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 141/3000 | Batch 0000/0011 | Loss: 3143.1252\n",
      "Epoch: 141/3000 | Batch 0001/0011 | Loss: 3172.0935\n",
      "Epoch: 141/3000 | Batch 0002/0011 | Loss: 3150.2886\n",
      "Epoch: 141/3000 | Batch 0003/0011 | Loss: 3203.5217\n",
      "Epoch: 141/3000 | Batch 0004/0011 | Loss: 3247.1091\n",
      "Epoch: 141/3000 | Batch 0005/0011 | Loss: 3227.5413\n",
      "Epoch: 141/3000 | Batch 0006/0011 | Loss: 3232.0081\n",
      "Epoch: 141/3000 | Batch 0007/0011 | Loss: 3157.6719\n",
      "Epoch: 141/3000 | Batch 0008/0011 | Loss: 3151.6245\n",
      "Epoch: 141/3000 | Batch 0009/0011 | Loss: 3082.8879\n",
      "Epoch: 141/3000 | Batch 0010/0011 | Loss: 3201.7910\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 142/3000 | Batch 0000/0011 | Loss: 3186.7600\n",
      "Epoch: 142/3000 | Batch 0001/0011 | Loss: 3081.4436\n",
      "Epoch: 142/3000 | Batch 0002/0011 | Loss: 3143.9238\n",
      "Epoch: 142/3000 | Batch 0003/0011 | Loss: 3240.8877\n",
      "Epoch: 142/3000 | Batch 0004/0011 | Loss: 3207.2224\n",
      "Epoch: 142/3000 | Batch 0005/0011 | Loss: 3159.4297\n",
      "Epoch: 142/3000 | Batch 0006/0011 | Loss: 3209.7031\n",
      "Epoch: 142/3000 | Batch 0007/0011 | Loss: 3214.6401\n",
      "Epoch: 142/3000 | Batch 0008/0011 | Loss: 3160.2134\n",
      "Epoch: 142/3000 | Batch 0009/0011 | Loss: 3176.7961\n",
      "Epoch: 142/3000 | Batch 0010/0011 | Loss: 3213.9114\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 143/3000 | Batch 0000/0011 | Loss: 3205.9336\n",
      "Epoch: 143/3000 | Batch 0001/0011 | Loss: 3149.1335\n",
      "Epoch: 143/3000 | Batch 0002/0011 | Loss: 3169.5725\n",
      "Epoch: 143/3000 | Batch 0003/0011 | Loss: 3149.1030\n",
      "Epoch: 143/3000 | Batch 0004/0011 | Loss: 3251.4634\n",
      "Epoch: 143/3000 | Batch 0005/0011 | Loss: 3239.4214\n",
      "Epoch: 143/3000 | Batch 0006/0011 | Loss: 3120.5068\n",
      "Epoch: 143/3000 | Batch 0007/0011 | Loss: 3134.8926\n",
      "Epoch: 143/3000 | Batch 0008/0011 | Loss: 3162.3545\n",
      "Epoch: 143/3000 | Batch 0009/0011 | Loss: 3196.9573\n",
      "Epoch: 143/3000 | Batch 0010/0011 | Loss: 3070.9072\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 144/3000 | Batch 0000/0011 | Loss: 3223.2231\n",
      "Epoch: 144/3000 | Batch 0001/0011 | Loss: 3094.4973\n",
      "Epoch: 144/3000 | Batch 0002/0011 | Loss: 3156.3411\n",
      "Epoch: 144/3000 | Batch 0003/0011 | Loss: 3180.9338\n",
      "Epoch: 144/3000 | Batch 0004/0011 | Loss: 3131.4277\n",
      "Epoch: 144/3000 | Batch 0005/0011 | Loss: 3125.5076\n",
      "Epoch: 144/3000 | Batch 0006/0011 | Loss: 3225.1858\n",
      "Epoch: 144/3000 | Batch 0007/0011 | Loss: 3111.7283\n",
      "Epoch: 144/3000 | Batch 0008/0011 | Loss: 3213.2693\n",
      "Epoch: 144/3000 | Batch 0009/0011 | Loss: 3262.9514\n",
      "Epoch: 144/3000 | Batch 0010/0011 | Loss: 3290.8000\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 145/3000 | Batch 0000/0011 | Loss: 3189.2864\n",
      "Epoch: 145/3000 | Batch 0001/0011 | Loss: 3197.4055\n",
      "Epoch: 145/3000 | Batch 0002/0011 | Loss: 3157.7898\n",
      "Epoch: 145/3000 | Batch 0003/0011 | Loss: 3114.2278\n",
      "Epoch: 145/3000 | Batch 0004/0011 | Loss: 3167.1262\n",
      "Epoch: 145/3000 | Batch 0005/0011 | Loss: 3247.6409\n",
      "Epoch: 145/3000 | Batch 0006/0011 | Loss: 3194.3171\n",
      "Epoch: 145/3000 | Batch 0007/0011 | Loss: 3172.4766\n",
      "Epoch: 145/3000 | Batch 0008/0011 | Loss: 3211.4585\n",
      "Epoch: 145/3000 | Batch 0009/0011 | Loss: 3119.6360\n",
      "Epoch: 145/3000 | Batch 0010/0011 | Loss: 3159.5015\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 146/3000 | Batch 0000/0011 | Loss: 3161.6257\n",
      "Epoch: 146/3000 | Batch 0001/0011 | Loss: 3244.4316\n",
      "Epoch: 146/3000 | Batch 0002/0011 | Loss: 3116.4011\n",
      "Epoch: 146/3000 | Batch 0003/0011 | Loss: 3152.6797\n",
      "Epoch: 146/3000 | Batch 0004/0011 | Loss: 3177.3389\n",
      "Epoch: 146/3000 | Batch 0005/0011 | Loss: 3189.3267\n",
      "Epoch: 146/3000 | Batch 0006/0011 | Loss: 3106.8345\n",
      "Epoch: 146/3000 | Batch 0007/0011 | Loss: 3149.3076\n",
      "Epoch: 146/3000 | Batch 0008/0011 | Loss: 3291.9202\n",
      "Epoch: 146/3000 | Batch 0009/0011 | Loss: 3174.9368\n",
      "Epoch: 146/3000 | Batch 0010/0011 | Loss: 3081.5737\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 147/3000 | Batch 0000/0011 | Loss: 3090.1863\n",
      "Epoch: 147/3000 | Batch 0001/0011 | Loss: 3131.0471\n",
      "Epoch: 147/3000 | Batch 0002/0011 | Loss: 3215.9185\n",
      "Epoch: 147/3000 | Batch 0003/0011 | Loss: 3149.0161\n",
      "Epoch: 147/3000 | Batch 0004/0011 | Loss: 3115.7307\n",
      "Epoch: 147/3000 | Batch 0005/0011 | Loss: 3224.0920\n",
      "Epoch: 147/3000 | Batch 0006/0011 | Loss: 3111.3943\n",
      "Epoch: 147/3000 | Batch 0007/0011 | Loss: 3194.1152\n",
      "Epoch: 147/3000 | Batch 0008/0011 | Loss: 3289.0747\n",
      "Epoch: 147/3000 | Batch 0009/0011 | Loss: 3146.0122\n",
      "Epoch: 147/3000 | Batch 0010/0011 | Loss: 3434.8523\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 148/3000 | Batch 0000/0011 | Loss: 3121.4180\n",
      "Epoch: 148/3000 | Batch 0001/0011 | Loss: 3198.2759\n",
      "Epoch: 148/3000 | Batch 0002/0011 | Loss: 3154.1724\n",
      "Epoch: 148/3000 | Batch 0003/0011 | Loss: 3222.8574\n",
      "Epoch: 148/3000 | Batch 0004/0011 | Loss: 3138.6411\n",
      "Epoch: 148/3000 | Batch 0005/0011 | Loss: 3188.0701\n",
      "Epoch: 148/3000 | Batch 0006/0011 | Loss: 3191.2219\n",
      "Epoch: 148/3000 | Batch 0007/0011 | Loss: 3180.4270\n",
      "Epoch: 148/3000 | Batch 0008/0011 | Loss: 3174.8279\n",
      "Epoch: 148/3000 | Batch 0009/0011 | Loss: 3195.1870\n",
      "Epoch: 148/3000 | Batch 0010/0011 | Loss: 3412.2314\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 149/3000 | Batch 0000/0011 | Loss: 3228.6201\n",
      "Epoch: 149/3000 | Batch 0001/0011 | Loss: 3127.3413\n",
      "Epoch: 149/3000 | Batch 0002/0011 | Loss: 3180.4170\n",
      "Epoch: 149/3000 | Batch 0003/0011 | Loss: 3150.6685\n",
      "Epoch: 149/3000 | Batch 0004/0011 | Loss: 3220.4639\n",
      "Epoch: 149/3000 | Batch 0005/0011 | Loss: 3198.5239\n",
      "Epoch: 149/3000 | Batch 0006/0011 | Loss: 3113.0698\n",
      "Epoch: 149/3000 | Batch 0007/0011 | Loss: 3219.9302\n",
      "Epoch: 149/3000 | Batch 0008/0011 | Loss: 3189.1133\n",
      "Epoch: 149/3000 | Batch 0009/0011 | Loss: 3122.6628\n",
      "Epoch: 149/3000 | Batch 0010/0011 | Loss: 3262.7478\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 150/3000 | Batch 0000/0011 | Loss: 3193.7773\n",
      "Epoch: 150/3000 | Batch 0001/0011 | Loss: 3154.8904\n",
      "Epoch: 150/3000 | Batch 0002/0011 | Loss: 3135.5945\n",
      "Epoch: 150/3000 | Batch 0003/0011 | Loss: 3188.9314\n",
      "Epoch: 150/3000 | Batch 0004/0011 | Loss: 3141.4375\n",
      "Epoch: 150/3000 | Batch 0005/0011 | Loss: 3236.8562\n",
      "Epoch: 150/3000 | Batch 0006/0011 | Loss: 3187.9211\n",
      "Epoch: 150/3000 | Batch 0007/0011 | Loss: 3225.7874\n",
      "Epoch: 150/3000 | Batch 0008/0011 | Loss: 3174.1003\n",
      "Epoch: 150/3000 | Batch 0009/0011 | Loss: 3114.6189\n",
      "Epoch: 150/3000 | Batch 0010/0011 | Loss: 3292.8313\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 151/3000 | Batch 0000/0011 | Loss: 3198.7375\n",
      "Epoch: 151/3000 | Batch 0001/0011 | Loss: 3210.5322\n",
      "Epoch: 151/3000 | Batch 0002/0011 | Loss: 3189.2278\n",
      "Epoch: 151/3000 | Batch 0003/0011 | Loss: 3167.9231\n",
      "Epoch: 151/3000 | Batch 0004/0011 | Loss: 3142.6079\n",
      "Epoch: 151/3000 | Batch 0005/0011 | Loss: 3222.1206\n",
      "Epoch: 151/3000 | Batch 0006/0011 | Loss: 3129.5842\n",
      "Epoch: 151/3000 | Batch 0007/0011 | Loss: 3177.5662\n",
      "Epoch: 151/3000 | Batch 0008/0011 | Loss: 3184.2256\n",
      "Epoch: 151/3000 | Batch 0009/0011 | Loss: 3120.9133\n",
      "Epoch: 151/3000 | Batch 0010/0011 | Loss: 3098.6780\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 152/3000 | Batch 0000/0011 | Loss: 3204.4192\n",
      "Epoch: 152/3000 | Batch 0001/0011 | Loss: 3183.7776\n",
      "Epoch: 152/3000 | Batch 0002/0011 | Loss: 3225.5085\n",
      "Epoch: 152/3000 | Batch 0003/0011 | Loss: 3162.2979\n",
      "Epoch: 152/3000 | Batch 0004/0011 | Loss: 3228.9944\n",
      "Epoch: 152/3000 | Batch 0005/0011 | Loss: 3162.3899\n",
      "Epoch: 152/3000 | Batch 0006/0011 | Loss: 3109.9636\n",
      "Epoch: 152/3000 | Batch 0007/0011 | Loss: 3143.9075\n",
      "Epoch: 152/3000 | Batch 0008/0011 | Loss: 3203.1707\n",
      "Epoch: 152/3000 | Batch 0009/0011 | Loss: 3084.9766\n",
      "Epoch: 152/3000 | Batch 0010/0011 | Loss: 3050.3459\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 153/3000 | Batch 0000/0011 | Loss: 3106.8643\n",
      "Epoch: 153/3000 | Batch 0001/0011 | Loss: 3104.6675\n",
      "Epoch: 153/3000 | Batch 0002/0011 | Loss: 3178.3960\n",
      "Epoch: 153/3000 | Batch 0003/0011 | Loss: 3210.7932\n",
      "Epoch: 153/3000 | Batch 0004/0011 | Loss: 3198.3245\n",
      "Epoch: 153/3000 | Batch 0005/0011 | Loss: 3217.5146\n",
      "Epoch: 153/3000 | Batch 0006/0011 | Loss: 3147.9128\n",
      "Epoch: 153/3000 | Batch 0007/0011 | Loss: 3188.5342\n",
      "Epoch: 153/3000 | Batch 0008/0011 | Loss: 3117.5137\n",
      "Epoch: 153/3000 | Batch 0009/0011 | Loss: 3237.5430\n",
      "Epoch: 153/3000 | Batch 0010/0011 | Loss: 3304.2927\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 154/3000 | Batch 0000/0011 | Loss: 3161.0659\n",
      "Epoch: 154/3000 | Batch 0001/0011 | Loss: 3139.4851\n",
      "Epoch: 154/3000 | Batch 0002/0011 | Loss: 3204.1167\n",
      "Epoch: 154/3000 | Batch 0003/0011 | Loss: 3222.1628\n",
      "Epoch: 154/3000 | Batch 0004/0011 | Loss: 3141.8726\n",
      "Epoch: 154/3000 | Batch 0005/0011 | Loss: 3144.2927\n",
      "Epoch: 154/3000 | Batch 0006/0011 | Loss: 3137.7715\n",
      "Epoch: 154/3000 | Batch 0007/0011 | Loss: 3127.9875\n",
      "Epoch: 154/3000 | Batch 0008/0011 | Loss: 3170.6694\n",
      "Epoch: 154/3000 | Batch 0009/0011 | Loss: 3200.6233\n",
      "Epoch: 154/3000 | Batch 0010/0011 | Loss: 3543.1462\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 155/3000 | Batch 0000/0011 | Loss: 3213.2390\n",
      "Epoch: 155/3000 | Batch 0001/0011 | Loss: 3131.8416\n",
      "Epoch: 155/3000 | Batch 0002/0011 | Loss: 3114.7561\n",
      "Epoch: 155/3000 | Batch 0003/0011 | Loss: 3221.0864\n",
      "Epoch: 155/3000 | Batch 0004/0011 | Loss: 3156.5913\n",
      "Epoch: 155/3000 | Batch 0005/0011 | Loss: 3246.8352\n",
      "Epoch: 155/3000 | Batch 0006/0011 | Loss: 3187.1382\n",
      "Epoch: 155/3000 | Batch 0007/0011 | Loss: 3116.6919\n",
      "Epoch: 155/3000 | Batch 0008/0011 | Loss: 3160.2480\n",
      "Epoch: 155/3000 | Batch 0009/0011 | Loss: 3162.7488\n",
      "Epoch: 155/3000 | Batch 0010/0011 | Loss: 3182.2107\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 156/3000 | Batch 0000/0011 | Loss: 3173.4280\n",
      "Epoch: 156/3000 | Batch 0001/0011 | Loss: 3154.9509\n",
      "Epoch: 156/3000 | Batch 0002/0011 | Loss: 3156.0891\n",
      "Epoch: 156/3000 | Batch 0003/0011 | Loss: 3131.7793\n",
      "Epoch: 156/3000 | Batch 0004/0011 | Loss: 3265.9531\n",
      "Epoch: 156/3000 | Batch 0005/0011 | Loss: 3152.2241\n",
      "Epoch: 156/3000 | Batch 0006/0011 | Loss: 3208.9517\n",
      "Epoch: 156/3000 | Batch 0007/0011 | Loss: 3158.4460\n",
      "Epoch: 156/3000 | Batch 0008/0011 | Loss: 3118.1777\n",
      "Epoch: 156/3000 | Batch 0009/0011 | Loss: 3192.8413\n",
      "Epoch: 156/3000 | Batch 0010/0011 | Loss: 3343.7625\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 157/3000 | Batch 0000/0011 | Loss: 3139.5225\n",
      "Epoch: 157/3000 | Batch 0001/0011 | Loss: 3230.8457\n",
      "Epoch: 157/3000 | Batch 0002/0011 | Loss: 3181.1641\n",
      "Epoch: 157/3000 | Batch 0003/0011 | Loss: 3254.5215\n",
      "Epoch: 157/3000 | Batch 0004/0011 | Loss: 3130.9502\n",
      "Epoch: 157/3000 | Batch 0005/0011 | Loss: 3141.8809\n",
      "Epoch: 157/3000 | Batch 0006/0011 | Loss: 3122.2915\n",
      "Epoch: 157/3000 | Batch 0007/0011 | Loss: 3192.3997\n",
      "Epoch: 157/3000 | Batch 0008/0011 | Loss: 3187.3997\n",
      "Epoch: 157/3000 | Batch 0009/0011 | Loss: 3138.8020\n",
      "Epoch: 157/3000 | Batch 0010/0011 | Loss: 3277.9836\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 158/3000 | Batch 0000/0011 | Loss: 3216.9895\n",
      "Epoch: 158/3000 | Batch 0001/0011 | Loss: 3127.6992\n",
      "Epoch: 158/3000 | Batch 0002/0011 | Loss: 3159.2791\n",
      "Epoch: 158/3000 | Batch 0003/0011 | Loss: 3141.1528\n",
      "Epoch: 158/3000 | Batch 0004/0011 | Loss: 3123.1482\n",
      "Epoch: 158/3000 | Batch 0005/0011 | Loss: 3229.1670\n",
      "Epoch: 158/3000 | Batch 0006/0011 | Loss: 3126.2495\n",
      "Epoch: 158/3000 | Batch 0007/0011 | Loss: 3163.3987\n",
      "Epoch: 158/3000 | Batch 0008/0011 | Loss: 3233.7161\n",
      "Epoch: 158/3000 | Batch 0009/0011 | Loss: 3239.2822\n",
      "Epoch: 158/3000 | Batch 0010/0011 | Loss: 2970.4929\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 159/3000 | Batch 0000/0011 | Loss: 3167.8740\n",
      "Epoch: 159/3000 | Batch 0001/0011 | Loss: 3228.1736\n",
      "Epoch: 159/3000 | Batch 0002/0011 | Loss: 3153.6438\n",
      "Epoch: 159/3000 | Batch 0003/0011 | Loss: 3140.8213\n",
      "Epoch: 159/3000 | Batch 0004/0011 | Loss: 3156.7759\n",
      "Epoch: 159/3000 | Batch 0005/0011 | Loss: 3178.0557\n",
      "Epoch: 159/3000 | Batch 0006/0011 | Loss: 3138.7483\n",
      "Epoch: 159/3000 | Batch 0007/0011 | Loss: 3157.7915\n",
      "Epoch: 159/3000 | Batch 0008/0011 | Loss: 3210.6184\n",
      "Epoch: 159/3000 | Batch 0009/0011 | Loss: 3112.7778\n",
      "Epoch: 159/3000 | Batch 0010/0011 | Loss: 3870.1116\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 160/3000 | Batch 0000/0011 | Loss: 3153.9436\n",
      "Epoch: 160/3000 | Batch 0001/0011 | Loss: 3133.4199\n",
      "Epoch: 160/3000 | Batch 0002/0011 | Loss: 3216.6965\n",
      "Epoch: 160/3000 | Batch 0003/0011 | Loss: 3179.6814\n",
      "Epoch: 160/3000 | Batch 0004/0011 | Loss: 3233.9460\n",
      "Epoch: 160/3000 | Batch 0005/0011 | Loss: 3213.5068\n",
      "Epoch: 160/3000 | Batch 0006/0011 | Loss: 3182.7498\n",
      "Epoch: 160/3000 | Batch 0007/0011 | Loss: 3150.7908\n",
      "Epoch: 160/3000 | Batch 0008/0011 | Loss: 3090.0471\n",
      "Epoch: 160/3000 | Batch 0009/0011 | Loss: 3259.4070\n",
      "Epoch: 160/3000 | Batch 0010/0011 | Loss: 3030.3359\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 161/3000 | Batch 0000/0011 | Loss: 3115.9277\n",
      "Epoch: 161/3000 | Batch 0001/0011 | Loss: 3244.7502\n",
      "Epoch: 161/3000 | Batch 0002/0011 | Loss: 3130.4180\n",
      "Epoch: 161/3000 | Batch 0003/0011 | Loss: 3126.9395\n",
      "Epoch: 161/3000 | Batch 0004/0011 | Loss: 3142.3193\n",
      "Epoch: 161/3000 | Batch 0005/0011 | Loss: 3145.2620\n",
      "Epoch: 161/3000 | Batch 0006/0011 | Loss: 3205.8333\n",
      "Epoch: 161/3000 | Batch 0007/0011 | Loss: 3122.8987\n",
      "Epoch: 161/3000 | Batch 0008/0011 | Loss: 3282.5073\n",
      "Epoch: 161/3000 | Batch 0009/0011 | Loss: 3160.0461\n",
      "Epoch: 161/3000 | Batch 0010/0011 | Loss: 3467.1790\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 162/3000 | Batch 0000/0011 | Loss: 3093.5151\n",
      "Epoch: 162/3000 | Batch 0001/0011 | Loss: 3224.0786\n",
      "Epoch: 162/3000 | Batch 0002/0011 | Loss: 3183.5637\n",
      "Epoch: 162/3000 | Batch 0003/0011 | Loss: 3173.1460\n",
      "Epoch: 162/3000 | Batch 0004/0011 | Loss: 3178.7571\n",
      "Epoch: 162/3000 | Batch 0005/0011 | Loss: 3261.8010\n",
      "Epoch: 162/3000 | Batch 0006/0011 | Loss: 3183.4851\n",
      "Epoch: 162/3000 | Batch 0007/0011 | Loss: 3124.9246\n",
      "Epoch: 162/3000 | Batch 0008/0011 | Loss: 3160.9727\n",
      "Epoch: 162/3000 | Batch 0009/0011 | Loss: 3159.7288\n",
      "Epoch: 162/3000 | Batch 0010/0011 | Loss: 3153.1414\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 163/3000 | Batch 0000/0011 | Loss: 3160.1248\n",
      "Epoch: 163/3000 | Batch 0001/0011 | Loss: 3175.3206\n",
      "Epoch: 163/3000 | Batch 0002/0011 | Loss: 3187.0916\n",
      "Epoch: 163/3000 | Batch 0003/0011 | Loss: 3148.8472\n",
      "Epoch: 163/3000 | Batch 0004/0011 | Loss: 3188.0273\n",
      "Epoch: 163/3000 | Batch 0005/0011 | Loss: 3197.1311\n",
      "Epoch: 163/3000 | Batch 0006/0011 | Loss: 3146.5647\n",
      "Epoch: 163/3000 | Batch 0007/0011 | Loss: 3188.7607\n",
      "Epoch: 163/3000 | Batch 0008/0011 | Loss: 3169.7786\n",
      "Epoch: 163/3000 | Batch 0009/0011 | Loss: 3122.4553\n",
      "Epoch: 163/3000 | Batch 0010/0011 | Loss: 3122.3521\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 164/3000 | Batch 0000/0011 | Loss: 3185.9060\n",
      "Epoch: 164/3000 | Batch 0001/0011 | Loss: 3156.8442\n",
      "Epoch: 164/3000 | Batch 0002/0011 | Loss: 3248.4790\n",
      "Epoch: 164/3000 | Batch 0003/0011 | Loss: 3227.5618\n",
      "Epoch: 164/3000 | Batch 0004/0011 | Loss: 3070.8696\n",
      "Epoch: 164/3000 | Batch 0005/0011 | Loss: 3196.9600\n",
      "Epoch: 164/3000 | Batch 0006/0011 | Loss: 3108.7793\n",
      "Epoch: 164/3000 | Batch 0007/0011 | Loss: 3089.9153\n",
      "Epoch: 164/3000 | Batch 0008/0011 | Loss: 3172.0562\n",
      "Epoch: 164/3000 | Batch 0009/0011 | Loss: 3166.0491\n",
      "Epoch: 164/3000 | Batch 0010/0011 | Loss: 3685.9343\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 165/3000 | Batch 0000/0011 | Loss: 3165.5415\n",
      "Epoch: 165/3000 | Batch 0001/0011 | Loss: 3182.3809\n",
      "Epoch: 165/3000 | Batch 0002/0011 | Loss: 3203.4880\n",
      "Epoch: 165/3000 | Batch 0003/0011 | Loss: 3242.4998\n",
      "Epoch: 165/3000 | Batch 0004/0011 | Loss: 3209.9170\n",
      "Epoch: 165/3000 | Batch 0005/0011 | Loss: 3175.2476\n",
      "Epoch: 165/3000 | Batch 0006/0011 | Loss: 3087.5051\n",
      "Epoch: 165/3000 | Batch 0007/0011 | Loss: 3149.9500\n",
      "Epoch: 165/3000 | Batch 0008/0011 | Loss: 3156.0588\n",
      "Epoch: 165/3000 | Batch 0009/0011 | Loss: 3202.5110\n",
      "Epoch: 165/3000 | Batch 0010/0011 | Loss: 3060.5913\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 166/3000 | Batch 0000/0011 | Loss: 3180.6448\n",
      "Epoch: 166/3000 | Batch 0001/0011 | Loss: 3222.7961\n",
      "Epoch: 166/3000 | Batch 0002/0011 | Loss: 3161.2483\n",
      "Epoch: 166/3000 | Batch 0003/0011 | Loss: 3157.5371\n",
      "Epoch: 166/3000 | Batch 0004/0011 | Loss: 3137.7529\n",
      "Epoch: 166/3000 | Batch 0005/0011 | Loss: 3232.3054\n",
      "Epoch: 166/3000 | Batch 0006/0011 | Loss: 3139.9607\n",
      "Epoch: 166/3000 | Batch 0007/0011 | Loss: 3143.6848\n",
      "Epoch: 166/3000 | Batch 0008/0011 | Loss: 3114.5422\n",
      "Epoch: 166/3000 | Batch 0009/0011 | Loss: 3223.8235\n",
      "Epoch: 166/3000 | Batch 0010/0011 | Loss: 3034.7271\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 167/3000 | Batch 0000/0011 | Loss: 3126.8071\n",
      "Epoch: 167/3000 | Batch 0001/0011 | Loss: 3256.3755\n",
      "Epoch: 167/3000 | Batch 0002/0011 | Loss: 3112.4207\n",
      "Epoch: 167/3000 | Batch 0003/0011 | Loss: 3170.2012\n",
      "Epoch: 167/3000 | Batch 0004/0011 | Loss: 3186.6450\n",
      "Epoch: 167/3000 | Batch 0005/0011 | Loss: 3163.0784\n",
      "Epoch: 167/3000 | Batch 0006/0011 | Loss: 3168.6992\n",
      "Epoch: 167/3000 | Batch 0007/0011 | Loss: 3171.5210\n",
      "Epoch: 167/3000 | Batch 0008/0011 | Loss: 3209.4753\n",
      "Epoch: 167/3000 | Batch 0009/0011 | Loss: 3142.7993\n",
      "Epoch: 167/3000 | Batch 0010/0011 | Loss: 3205.6736\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 168/3000 | Batch 0000/0011 | Loss: 3189.1160\n",
      "Epoch: 168/3000 | Batch 0001/0011 | Loss: 3260.6213\n",
      "Epoch: 168/3000 | Batch 0002/0011 | Loss: 3121.2166\n",
      "Epoch: 168/3000 | Batch 0003/0011 | Loss: 3133.6350\n",
      "Epoch: 168/3000 | Batch 0004/0011 | Loss: 3114.2644\n",
      "Epoch: 168/3000 | Batch 0005/0011 | Loss: 3164.2312\n",
      "Epoch: 168/3000 | Batch 0006/0011 | Loss: 3084.4170\n",
      "Epoch: 168/3000 | Batch 0007/0011 | Loss: 3250.0261\n",
      "Epoch: 168/3000 | Batch 0008/0011 | Loss: 3174.4290\n",
      "Epoch: 168/3000 | Batch 0009/0011 | Loss: 3201.8789\n",
      "Epoch: 168/3000 | Batch 0010/0011 | Loss: 3067.6145\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 169/3000 | Batch 0000/0011 | Loss: 3197.1218\n",
      "Epoch: 169/3000 | Batch 0001/0011 | Loss: 3185.3162\n",
      "Epoch: 169/3000 | Batch 0002/0011 | Loss: 3157.3936\n",
      "Epoch: 169/3000 | Batch 0003/0011 | Loss: 3128.0984\n",
      "Epoch: 169/3000 | Batch 0004/0011 | Loss: 3126.2747\n",
      "Epoch: 169/3000 | Batch 0005/0011 | Loss: 3166.1377\n",
      "Epoch: 169/3000 | Batch 0006/0011 | Loss: 3233.7979\n",
      "Epoch: 169/3000 | Batch 0007/0011 | Loss: 3211.5059\n",
      "Epoch: 169/3000 | Batch 0008/0011 | Loss: 3147.3582\n",
      "Epoch: 169/3000 | Batch 0009/0011 | Loss: 3163.2197\n",
      "Epoch: 169/3000 | Batch 0010/0011 | Loss: 3157.1687\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 170/3000 | Batch 0000/0011 | Loss: 3301.2129\n",
      "Epoch: 170/3000 | Batch 0001/0011 | Loss: 3160.7222\n",
      "Epoch: 170/3000 | Batch 0002/0011 | Loss: 3156.7566\n",
      "Epoch: 170/3000 | Batch 0003/0011 | Loss: 3125.4700\n",
      "Epoch: 170/3000 | Batch 0004/0011 | Loss: 3126.5637\n",
      "Epoch: 170/3000 | Batch 0005/0011 | Loss: 3143.8667\n",
      "Epoch: 170/3000 | Batch 0006/0011 | Loss: 3132.6018\n",
      "Epoch: 170/3000 | Batch 0007/0011 | Loss: 3238.6602\n",
      "Epoch: 170/3000 | Batch 0008/0011 | Loss: 3184.1448\n",
      "Epoch: 170/3000 | Batch 0009/0011 | Loss: 3136.9187\n",
      "Epoch: 170/3000 | Batch 0010/0011 | Loss: 3143.5579\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 171/3000 | Batch 0000/0011 | Loss: 3190.5659\n",
      "Epoch: 171/3000 | Batch 0001/0011 | Loss: 3129.5095\n",
      "Epoch: 171/3000 | Batch 0002/0011 | Loss: 3260.4114\n",
      "Epoch: 171/3000 | Batch 0003/0011 | Loss: 3187.1638\n",
      "Epoch: 171/3000 | Batch 0004/0011 | Loss: 3209.8582\n",
      "Epoch: 171/3000 | Batch 0005/0011 | Loss: 3068.6426\n",
      "Epoch: 171/3000 | Batch 0006/0011 | Loss: 3151.8176\n",
      "Epoch: 171/3000 | Batch 0007/0011 | Loss: 3134.2322\n",
      "Epoch: 171/3000 | Batch 0008/0011 | Loss: 3181.6533\n",
      "Epoch: 171/3000 | Batch 0009/0011 | Loss: 3146.2209\n",
      "Epoch: 171/3000 | Batch 0010/0011 | Loss: 3126.5532\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 172/3000 | Batch 0000/0011 | Loss: 3261.0078\n",
      "Epoch: 172/3000 | Batch 0001/0011 | Loss: 3176.0486\n",
      "Epoch: 172/3000 | Batch 0002/0011 | Loss: 3158.0393\n",
      "Epoch: 172/3000 | Batch 0003/0011 | Loss: 3133.3345\n",
      "Epoch: 172/3000 | Batch 0004/0011 | Loss: 3269.7388\n",
      "Epoch: 172/3000 | Batch 0005/0011 | Loss: 3119.3821\n",
      "Epoch: 172/3000 | Batch 0006/0011 | Loss: 3114.6155\n",
      "Epoch: 172/3000 | Batch 0007/0011 | Loss: 3163.1973\n",
      "Epoch: 172/3000 | Batch 0008/0011 | Loss: 3117.6028\n",
      "Epoch: 172/3000 | Batch 0009/0011 | Loss: 3186.1238\n",
      "Epoch: 172/3000 | Batch 0010/0011 | Loss: 3340.0654\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 173/3000 | Batch 0000/0011 | Loss: 3191.9529\n",
      "Epoch: 173/3000 | Batch 0001/0011 | Loss: 3091.1245\n",
      "Epoch: 173/3000 | Batch 0002/0011 | Loss: 3107.7832\n",
      "Epoch: 173/3000 | Batch 0003/0011 | Loss: 3188.3547\n",
      "Epoch: 173/3000 | Batch 0004/0011 | Loss: 3144.2393\n",
      "Epoch: 173/3000 | Batch 0005/0011 | Loss: 3208.8938\n",
      "Epoch: 173/3000 | Batch 0006/0011 | Loss: 3246.5378\n",
      "Epoch: 173/3000 | Batch 0007/0011 | Loss: 3184.2485\n",
      "Epoch: 173/3000 | Batch 0008/0011 | Loss: 3180.8132\n",
      "Epoch: 173/3000 | Batch 0009/0011 | Loss: 3156.6965\n",
      "Epoch: 173/3000 | Batch 0010/0011 | Loss: 3101.9534\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 174/3000 | Batch 0000/0011 | Loss: 3134.8792\n",
      "Epoch: 174/3000 | Batch 0001/0011 | Loss: 3174.7598\n",
      "Epoch: 174/3000 | Batch 0002/0011 | Loss: 3177.7764\n",
      "Epoch: 174/3000 | Batch 0003/0011 | Loss: 3189.2041\n",
      "Epoch: 174/3000 | Batch 0004/0011 | Loss: 3182.8242\n",
      "Epoch: 174/3000 | Batch 0005/0011 | Loss: 3180.0591\n",
      "Epoch: 174/3000 | Batch 0006/0011 | Loss: 3248.3254\n",
      "Epoch: 174/3000 | Batch 0007/0011 | Loss: 3123.7422\n",
      "Epoch: 174/3000 | Batch 0008/0011 | Loss: 3152.2009\n",
      "Epoch: 174/3000 | Batch 0009/0011 | Loss: 3188.7722\n",
      "Epoch: 174/3000 | Batch 0010/0011 | Loss: 3338.1943\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 175/3000 | Batch 0000/0011 | Loss: 3226.4375\n",
      "Epoch: 175/3000 | Batch 0001/0011 | Loss: 3199.5312\n",
      "Epoch: 175/3000 | Batch 0002/0011 | Loss: 3158.5254\n",
      "Epoch: 175/3000 | Batch 0003/0011 | Loss: 3256.7732\n",
      "Epoch: 175/3000 | Batch 0004/0011 | Loss: 3148.5396\n",
      "Epoch: 175/3000 | Batch 0005/0011 | Loss: 3252.3850\n",
      "Epoch: 175/3000 | Batch 0006/0011 | Loss: 3148.2656\n",
      "Epoch: 175/3000 | Batch 0007/0011 | Loss: 3211.8752\n",
      "Epoch: 175/3000 | Batch 0008/0011 | Loss: 3103.3579\n",
      "Epoch: 175/3000 | Batch 0009/0011 | Loss: 3130.6858\n",
      "Epoch: 175/3000 | Batch 0010/0011 | Loss: 3168.4983\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 176/3000 | Batch 0000/0011 | Loss: 3091.8196\n",
      "Epoch: 176/3000 | Batch 0001/0011 | Loss: 3205.4666\n",
      "Epoch: 176/3000 | Batch 0002/0011 | Loss: 3223.6970\n",
      "Epoch: 176/3000 | Batch 0003/0011 | Loss: 3218.7795\n",
      "Epoch: 176/3000 | Batch 0004/0011 | Loss: 3164.4265\n",
      "Epoch: 176/3000 | Batch 0005/0011 | Loss: 3129.2986\n",
      "Epoch: 176/3000 | Batch 0006/0011 | Loss: 3145.1340\n",
      "Epoch: 176/3000 | Batch 0007/0011 | Loss: 3133.8003\n",
      "Epoch: 176/3000 | Batch 0008/0011 | Loss: 3188.2751\n",
      "Epoch: 176/3000 | Batch 0009/0011 | Loss: 3190.6880\n",
      "Epoch: 176/3000 | Batch 0010/0011 | Loss: 3059.0454\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 177/3000 | Batch 0000/0011 | Loss: 3140.7432\n",
      "Epoch: 177/3000 | Batch 0001/0011 | Loss: 3138.6763\n",
      "Epoch: 177/3000 | Batch 0002/0011 | Loss: 3184.4697\n",
      "Epoch: 177/3000 | Batch 0003/0011 | Loss: 3187.8101\n",
      "Epoch: 177/3000 | Batch 0004/0011 | Loss: 3139.3181\n",
      "Epoch: 177/3000 | Batch 0005/0011 | Loss: 3140.8599\n",
      "Epoch: 177/3000 | Batch 0006/0011 | Loss: 3215.8003\n",
      "Epoch: 177/3000 | Batch 0007/0011 | Loss: 3174.1125\n",
      "Epoch: 177/3000 | Batch 0008/0011 | Loss: 3272.9036\n",
      "Epoch: 177/3000 | Batch 0009/0011 | Loss: 3116.6807\n",
      "Epoch: 177/3000 | Batch 0010/0011 | Loss: 3121.8403\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 178/3000 | Batch 0000/0011 | Loss: 3138.0908\n",
      "Epoch: 178/3000 | Batch 0001/0011 | Loss: 3155.3767\n",
      "Epoch: 178/3000 | Batch 0002/0011 | Loss: 3187.6455\n",
      "Epoch: 178/3000 | Batch 0003/0011 | Loss: 3169.3689\n",
      "Epoch: 178/3000 | Batch 0004/0011 | Loss: 3160.8010\n",
      "Epoch: 178/3000 | Batch 0005/0011 | Loss: 3119.3281\n",
      "Epoch: 178/3000 | Batch 0006/0011 | Loss: 3130.0034\n",
      "Epoch: 178/3000 | Batch 0007/0011 | Loss: 3190.2219\n",
      "Epoch: 178/3000 | Batch 0008/0011 | Loss: 3144.4187\n",
      "Epoch: 178/3000 | Batch 0009/0011 | Loss: 3265.1680\n",
      "Epoch: 178/3000 | Batch 0010/0011 | Loss: 3308.1638\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 179/3000 | Batch 0000/0011 | Loss: 3155.3682\n",
      "Epoch: 179/3000 | Batch 0001/0011 | Loss: 3103.5901\n",
      "Epoch: 179/3000 | Batch 0002/0011 | Loss: 3124.2876\n",
      "Epoch: 179/3000 | Batch 0003/0011 | Loss: 3157.9568\n",
      "Epoch: 179/3000 | Batch 0004/0011 | Loss: 3186.7981\n",
      "Epoch: 179/3000 | Batch 0005/0011 | Loss: 3251.1418\n",
      "Epoch: 179/3000 | Batch 0006/0011 | Loss: 3181.4639\n",
      "Epoch: 179/3000 | Batch 0007/0011 | Loss: 3189.5627\n",
      "Epoch: 179/3000 | Batch 0008/0011 | Loss: 3221.9875\n",
      "Epoch: 179/3000 | Batch 0009/0011 | Loss: 3105.2900\n",
      "Epoch: 179/3000 | Batch 0010/0011 | Loss: 3217.7246\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 180/3000 | Batch 0000/0011 | Loss: 3210.1924\n",
      "Epoch: 180/3000 | Batch 0001/0011 | Loss: 3131.5225\n",
      "Epoch: 180/3000 | Batch 0002/0011 | Loss: 3176.4290\n",
      "Epoch: 180/3000 | Batch 0003/0011 | Loss: 3198.0845\n",
      "Epoch: 180/3000 | Batch 0004/0011 | Loss: 3126.3608\n",
      "Epoch: 180/3000 | Batch 0005/0011 | Loss: 3172.5522\n",
      "Epoch: 180/3000 | Batch 0006/0011 | Loss: 3132.6001\n",
      "Epoch: 180/3000 | Batch 0007/0011 | Loss: 3184.0525\n",
      "Epoch: 180/3000 | Batch 0008/0011 | Loss: 3176.3989\n",
      "Epoch: 180/3000 | Batch 0009/0011 | Loss: 3174.9856\n",
      "Epoch: 180/3000 | Batch 0010/0011 | Loss: 3193.6104\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 181/3000 | Batch 0000/0011 | Loss: 3219.2793\n",
      "Epoch: 181/3000 | Batch 0001/0011 | Loss: 3175.7168\n",
      "Epoch: 181/3000 | Batch 0002/0011 | Loss: 3197.2861\n",
      "Epoch: 181/3000 | Batch 0003/0011 | Loss: 3140.9143\n",
      "Epoch: 181/3000 | Batch 0004/0011 | Loss: 3210.1279\n",
      "Epoch: 181/3000 | Batch 0005/0011 | Loss: 3149.6553\n",
      "Epoch: 181/3000 | Batch 0006/0011 | Loss: 3172.7498\n",
      "Epoch: 181/3000 | Batch 0007/0011 | Loss: 3115.4551\n",
      "Epoch: 181/3000 | Batch 0008/0011 | Loss: 3115.5308\n",
      "Epoch: 181/3000 | Batch 0009/0011 | Loss: 3239.4084\n",
      "Epoch: 181/3000 | Batch 0010/0011 | Loss: 3165.1155\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 182/3000 | Batch 0000/0011 | Loss: 3208.0564\n",
      "Epoch: 182/3000 | Batch 0001/0011 | Loss: 3212.0486\n",
      "Epoch: 182/3000 | Batch 0002/0011 | Loss: 3185.0342\n",
      "Epoch: 182/3000 | Batch 0003/0011 | Loss: 3140.6138\n",
      "Epoch: 182/3000 | Batch 0004/0011 | Loss: 3111.6582\n",
      "Epoch: 182/3000 | Batch 0005/0011 | Loss: 3210.8723\n",
      "Epoch: 182/3000 | Batch 0006/0011 | Loss: 3215.3059\n",
      "Epoch: 182/3000 | Batch 0007/0011 | Loss: 3056.0557\n",
      "Epoch: 182/3000 | Batch 0008/0011 | Loss: 3194.0007\n",
      "Epoch: 182/3000 | Batch 0009/0011 | Loss: 3093.1011\n",
      "Epoch: 182/3000 | Batch 0010/0011 | Loss: 3520.4761\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 183/3000 | Batch 0000/0011 | Loss: 3107.3313\n",
      "Epoch: 183/3000 | Batch 0001/0011 | Loss: 3169.8281\n",
      "Epoch: 183/3000 | Batch 0002/0011 | Loss: 3188.0386\n",
      "Epoch: 183/3000 | Batch 0003/0011 | Loss: 3193.7058\n",
      "Epoch: 183/3000 | Batch 0004/0011 | Loss: 3175.8318\n",
      "Epoch: 183/3000 | Batch 0005/0011 | Loss: 3156.0601\n",
      "Epoch: 183/3000 | Batch 0006/0011 | Loss: 3149.3274\n",
      "Epoch: 183/3000 | Batch 0007/0011 | Loss: 3210.5505\n",
      "Epoch: 183/3000 | Batch 0008/0011 | Loss: 3193.3862\n",
      "Epoch: 183/3000 | Batch 0009/0011 | Loss: 3160.8142\n",
      "Epoch: 183/3000 | Batch 0010/0011 | Loss: 3036.1064\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 184/3000 | Batch 0000/0011 | Loss: 3159.3604\n",
      "Epoch: 184/3000 | Batch 0001/0011 | Loss: 3119.4187\n",
      "Epoch: 184/3000 | Batch 0002/0011 | Loss: 3155.3572\n",
      "Epoch: 184/3000 | Batch 0003/0011 | Loss: 3158.1440\n",
      "Epoch: 184/3000 | Batch 0004/0011 | Loss: 3155.9761\n",
      "Epoch: 184/3000 | Batch 0005/0011 | Loss: 3160.3191\n",
      "Epoch: 184/3000 | Batch 0006/0011 | Loss: 3180.2275\n",
      "Epoch: 184/3000 | Batch 0007/0011 | Loss: 3093.3540\n",
      "Epoch: 184/3000 | Batch 0008/0011 | Loss: 3257.0200\n",
      "Epoch: 184/3000 | Batch 0009/0011 | Loss: 3203.3987\n",
      "Epoch: 184/3000 | Batch 0010/0011 | Loss: 3414.4614\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 185/3000 | Batch 0000/0011 | Loss: 3146.8318\n",
      "Epoch: 185/3000 | Batch 0001/0011 | Loss: 3156.4512\n",
      "Epoch: 185/3000 | Batch 0002/0011 | Loss: 3139.5898\n",
      "Epoch: 185/3000 | Batch 0003/0011 | Loss: 3136.0432\n",
      "Epoch: 185/3000 | Batch 0004/0011 | Loss: 3169.1851\n",
      "Epoch: 185/3000 | Batch 0005/0011 | Loss: 3122.2109\n",
      "Epoch: 185/3000 | Batch 0006/0011 | Loss: 3212.9026\n",
      "Epoch: 185/3000 | Batch 0007/0011 | Loss: 3259.8374\n",
      "Epoch: 185/3000 | Batch 0008/0011 | Loss: 3153.5032\n",
      "Epoch: 185/3000 | Batch 0009/0011 | Loss: 3196.4419\n",
      "Epoch: 185/3000 | Batch 0010/0011 | Loss: 3373.1895\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 186/3000 | Batch 0000/0011 | Loss: 3223.6677\n",
      "Epoch: 186/3000 | Batch 0001/0011 | Loss: 3107.7358\n",
      "Epoch: 186/3000 | Batch 0002/0011 | Loss: 3150.1545\n",
      "Epoch: 186/3000 | Batch 0003/0011 | Loss: 3133.1213\n",
      "Epoch: 186/3000 | Batch 0004/0011 | Loss: 3182.3962\n",
      "Epoch: 186/3000 | Batch 0005/0011 | Loss: 3245.6680\n",
      "Epoch: 186/3000 | Batch 0006/0011 | Loss: 3160.2642\n",
      "Epoch: 186/3000 | Batch 0007/0011 | Loss: 3204.9290\n",
      "Epoch: 186/3000 | Batch 0008/0011 | Loss: 3174.5667\n",
      "Epoch: 186/3000 | Batch 0009/0011 | Loss: 3210.9583\n",
      "Epoch: 186/3000 | Batch 0010/0011 | Loss: 3066.6824\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 187/3000 | Batch 0000/0011 | Loss: 3138.8564\n",
      "Epoch: 187/3000 | Batch 0001/0011 | Loss: 3163.8235\n",
      "Epoch: 187/3000 | Batch 0002/0011 | Loss: 3124.7083\n",
      "Epoch: 187/3000 | Batch 0003/0011 | Loss: 3171.7822\n",
      "Epoch: 187/3000 | Batch 0004/0011 | Loss: 3208.4453\n",
      "Epoch: 187/3000 | Batch 0005/0011 | Loss: 3178.1064\n",
      "Epoch: 187/3000 | Batch 0006/0011 | Loss: 3245.3606\n",
      "Epoch: 187/3000 | Batch 0007/0011 | Loss: 3076.7988\n",
      "Epoch: 187/3000 | Batch 0008/0011 | Loss: 3166.1882\n",
      "Epoch: 187/3000 | Batch 0009/0011 | Loss: 3169.5994\n",
      "Epoch: 187/3000 | Batch 0010/0011 | Loss: 3293.3206\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 188/3000 | Batch 0000/0011 | Loss: 3128.2810\n",
      "Epoch: 188/3000 | Batch 0001/0011 | Loss: 3153.4065\n",
      "Epoch: 188/3000 | Batch 0002/0011 | Loss: 3164.1895\n",
      "Epoch: 188/3000 | Batch 0003/0011 | Loss: 3270.7903\n",
      "Epoch: 188/3000 | Batch 0004/0011 | Loss: 3218.6313\n",
      "Epoch: 188/3000 | Batch 0005/0011 | Loss: 3131.6638\n",
      "Epoch: 188/3000 | Batch 0006/0011 | Loss: 3117.6514\n",
      "Epoch: 188/3000 | Batch 0007/0011 | Loss: 3176.4929\n",
      "Epoch: 188/3000 | Batch 0008/0011 | Loss: 3271.7063\n",
      "Epoch: 188/3000 | Batch 0009/0011 | Loss: 3146.0918\n",
      "Epoch: 188/3000 | Batch 0010/0011 | Loss: 3031.7729\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 189/3000 | Batch 0000/0011 | Loss: 3141.1367\n",
      "Epoch: 189/3000 | Batch 0001/0011 | Loss: 3162.6772\n",
      "Epoch: 189/3000 | Batch 0002/0011 | Loss: 3218.2881\n",
      "Epoch: 189/3000 | Batch 0003/0011 | Loss: 3142.7053\n",
      "Epoch: 189/3000 | Batch 0004/0011 | Loss: 3181.8518\n",
      "Epoch: 189/3000 | Batch 0005/0011 | Loss: 3143.0315\n",
      "Epoch: 189/3000 | Batch 0006/0011 | Loss: 3180.6562\n",
      "Epoch: 189/3000 | Batch 0007/0011 | Loss: 3108.9045\n",
      "Epoch: 189/3000 | Batch 0008/0011 | Loss: 3141.7847\n",
      "Epoch: 189/3000 | Batch 0009/0011 | Loss: 3252.2263\n",
      "Epoch: 189/3000 | Batch 0010/0011 | Loss: 3307.2754\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 190/3000 | Batch 0000/0011 | Loss: 3172.2261\n",
      "Epoch: 190/3000 | Batch 0001/0011 | Loss: 3163.8481\n",
      "Epoch: 190/3000 | Batch 0002/0011 | Loss: 3152.4680\n",
      "Epoch: 190/3000 | Batch 0003/0011 | Loss: 3124.6270\n",
      "Epoch: 190/3000 | Batch 0004/0011 | Loss: 3188.0874\n",
      "Epoch: 190/3000 | Batch 0005/0011 | Loss: 3231.3965\n",
      "Epoch: 190/3000 | Batch 0006/0011 | Loss: 3177.1807\n",
      "Epoch: 190/3000 | Batch 0007/0011 | Loss: 3157.1111\n",
      "Epoch: 190/3000 | Batch 0008/0011 | Loss: 3160.3218\n",
      "Epoch: 190/3000 | Batch 0009/0011 | Loss: 3200.2380\n",
      "Epoch: 190/3000 | Batch 0010/0011 | Loss: 3186.7837\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 191/3000 | Batch 0000/0011 | Loss: 3182.0684\n",
      "Epoch: 191/3000 | Batch 0001/0011 | Loss: 3147.5017\n",
      "Epoch: 191/3000 | Batch 0002/0011 | Loss: 3099.6865\n",
      "Epoch: 191/3000 | Batch 0003/0011 | Loss: 3132.4458\n",
      "Epoch: 191/3000 | Batch 0004/0011 | Loss: 3161.7632\n",
      "Epoch: 191/3000 | Batch 0005/0011 | Loss: 3202.9224\n",
      "Epoch: 191/3000 | Batch 0006/0011 | Loss: 3144.3188\n",
      "Epoch: 191/3000 | Batch 0007/0011 | Loss: 3327.8796\n",
      "Epoch: 191/3000 | Batch 0008/0011 | Loss: 3109.0735\n",
      "Epoch: 191/3000 | Batch 0009/0011 | Loss: 3178.7551\n",
      "Epoch: 191/3000 | Batch 0010/0011 | Loss: 3404.6301\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 192/3000 | Batch 0000/0011 | Loss: 3203.5942\n",
      "Epoch: 192/3000 | Batch 0001/0011 | Loss: 3205.2822\n",
      "Epoch: 192/3000 | Batch 0002/0011 | Loss: 3146.1292\n",
      "Epoch: 192/3000 | Batch 0003/0011 | Loss: 3117.5583\n",
      "Epoch: 192/3000 | Batch 0004/0011 | Loss: 3089.3726\n",
      "Epoch: 192/3000 | Batch 0005/0011 | Loss: 3170.7856\n",
      "Epoch: 192/3000 | Batch 0006/0011 | Loss: 3184.3538\n",
      "Epoch: 192/3000 | Batch 0007/0011 | Loss: 3166.9697\n",
      "Epoch: 192/3000 | Batch 0008/0011 | Loss: 3219.5706\n",
      "Epoch: 192/3000 | Batch 0009/0011 | Loss: 3139.3867\n",
      "Epoch: 192/3000 | Batch 0010/0011 | Loss: 3160.0220\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 193/3000 | Batch 0000/0011 | Loss: 3148.6660\n",
      "Epoch: 193/3000 | Batch 0001/0011 | Loss: 3153.9578\n",
      "Epoch: 193/3000 | Batch 0002/0011 | Loss: 3150.9939\n",
      "Epoch: 193/3000 | Batch 0003/0011 | Loss: 3224.7578\n",
      "Epoch: 193/3000 | Batch 0004/0011 | Loss: 3119.0134\n",
      "Epoch: 193/3000 | Batch 0005/0011 | Loss: 3221.2708\n",
      "Epoch: 193/3000 | Batch 0006/0011 | Loss: 3203.6860\n",
      "Epoch: 193/3000 | Batch 0007/0011 | Loss: 3074.3386\n",
      "Epoch: 193/3000 | Batch 0008/0011 | Loss: 3141.5942\n",
      "Epoch: 193/3000 | Batch 0009/0011 | Loss: 3214.0420\n",
      "Epoch: 193/3000 | Batch 0010/0011 | Loss: 3236.1860\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 194/3000 | Batch 0000/0011 | Loss: 3176.8628\n",
      "Epoch: 194/3000 | Batch 0001/0011 | Loss: 3154.0962\n",
      "Epoch: 194/3000 | Batch 0002/0011 | Loss: 3157.3184\n",
      "Epoch: 194/3000 | Batch 0003/0011 | Loss: 3136.7136\n",
      "Epoch: 194/3000 | Batch 0004/0011 | Loss: 3259.6121\n",
      "Epoch: 194/3000 | Batch 0005/0011 | Loss: 3089.7112\n",
      "Epoch: 194/3000 | Batch 0006/0011 | Loss: 3248.9844\n",
      "Epoch: 194/3000 | Batch 0007/0011 | Loss: 3162.9377\n",
      "Epoch: 194/3000 | Batch 0008/0011 | Loss: 3125.6194\n",
      "Epoch: 194/3000 | Batch 0009/0011 | Loss: 3154.4658\n",
      "Epoch: 194/3000 | Batch 0010/0011 | Loss: 3211.9988\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 195/3000 | Batch 0000/0011 | Loss: 3224.5039\n",
      "Epoch: 195/3000 | Batch 0001/0011 | Loss: 3130.1372\n",
      "Epoch: 195/3000 | Batch 0002/0011 | Loss: 3126.3879\n",
      "Epoch: 195/3000 | Batch 0003/0011 | Loss: 3188.4314\n",
      "Epoch: 195/3000 | Batch 0004/0011 | Loss: 3166.3179\n",
      "Epoch: 195/3000 | Batch 0005/0011 | Loss: 3150.3464\n",
      "Epoch: 195/3000 | Batch 0006/0011 | Loss: 3140.5291\n",
      "Epoch: 195/3000 | Batch 0007/0011 | Loss: 3277.3181\n",
      "Epoch: 195/3000 | Batch 0008/0011 | Loss: 3155.2625\n",
      "Epoch: 195/3000 | Batch 0009/0011 | Loss: 3184.9021\n",
      "Epoch: 195/3000 | Batch 0010/0011 | Loss: 3050.3782\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 196/3000 | Batch 0000/0011 | Loss: 3112.1731\n",
      "Epoch: 196/3000 | Batch 0001/0011 | Loss: 3134.1990\n",
      "Epoch: 196/3000 | Batch 0002/0011 | Loss: 3171.4031\n",
      "Epoch: 196/3000 | Batch 0003/0011 | Loss: 3168.5310\n",
      "Epoch: 196/3000 | Batch 0004/0011 | Loss: 3187.3540\n",
      "Epoch: 196/3000 | Batch 0005/0011 | Loss: 3292.6650\n",
      "Epoch: 196/3000 | Batch 0006/0011 | Loss: 3183.8584\n",
      "Epoch: 196/3000 | Batch 0007/0011 | Loss: 3147.2166\n",
      "Epoch: 196/3000 | Batch 0008/0011 | Loss: 3183.9373\n",
      "Epoch: 196/3000 | Batch 0009/0011 | Loss: 3139.4397\n",
      "Epoch: 196/3000 | Batch 0010/0011 | Loss: 3082.8823\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 197/3000 | Batch 0000/0011 | Loss: 3159.1836\n",
      "Epoch: 197/3000 | Batch 0001/0011 | Loss: 3251.3682\n",
      "Epoch: 197/3000 | Batch 0002/0011 | Loss: 3203.6860\n",
      "Epoch: 197/3000 | Batch 0003/0011 | Loss: 3109.8308\n",
      "Epoch: 197/3000 | Batch 0004/0011 | Loss: 3143.1924\n",
      "Epoch: 197/3000 | Batch 0005/0011 | Loss: 3173.5679\n",
      "Epoch: 197/3000 | Batch 0006/0011 | Loss: 3116.9810\n",
      "Epoch: 197/3000 | Batch 0007/0011 | Loss: 3092.0562\n",
      "Epoch: 197/3000 | Batch 0008/0011 | Loss: 3168.2893\n",
      "Epoch: 197/3000 | Batch 0009/0011 | Loss: 3238.5818\n",
      "Epoch: 197/3000 | Batch 0010/0011 | Loss: 3210.2327\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 198/3000 | Batch 0000/0011 | Loss: 3162.9087\n",
      "Epoch: 198/3000 | Batch 0001/0011 | Loss: 3154.0269\n",
      "Epoch: 198/3000 | Batch 0002/0011 | Loss: 3131.3354\n",
      "Epoch: 198/3000 | Batch 0003/0011 | Loss: 3164.3218\n",
      "Epoch: 198/3000 | Batch 0004/0011 | Loss: 3138.0984\n",
      "Epoch: 198/3000 | Batch 0005/0011 | Loss: 3267.7874\n",
      "Epoch: 198/3000 | Batch 0006/0011 | Loss: 3211.2751\n",
      "Epoch: 198/3000 | Batch 0007/0011 | Loss: 3123.8765\n",
      "Epoch: 198/3000 | Batch 0008/0011 | Loss: 3217.0901\n",
      "Epoch: 198/3000 | Batch 0009/0011 | Loss: 3073.0825\n",
      "Epoch: 198/3000 | Batch 0010/0011 | Loss: 2996.2559\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 199/3000 | Batch 0000/0011 | Loss: 3180.9141\n",
      "Epoch: 199/3000 | Batch 0001/0011 | Loss: 3184.0955\n",
      "Epoch: 199/3000 | Batch 0002/0011 | Loss: 3116.8472\n",
      "Epoch: 199/3000 | Batch 0003/0011 | Loss: 3071.4968\n",
      "Epoch: 199/3000 | Batch 0004/0011 | Loss: 3143.4587\n",
      "Epoch: 199/3000 | Batch 0005/0011 | Loss: 3232.3281\n",
      "Epoch: 199/3000 | Batch 0006/0011 | Loss: 3168.5994\n",
      "Epoch: 199/3000 | Batch 0007/0011 | Loss: 3081.6355\n",
      "Epoch: 199/3000 | Batch 0008/0011 | Loss: 3204.6387\n",
      "Epoch: 199/3000 | Batch 0009/0011 | Loss: 3182.4958\n",
      "Epoch: 199/3000 | Batch 0010/0011 | Loss: 3473.9028\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 200/3000 | Batch 0000/0011 | Loss: 3164.0300\n",
      "Epoch: 200/3000 | Batch 0001/0011 | Loss: 3128.2476\n",
      "Epoch: 200/3000 | Batch 0002/0011 | Loss: 3219.6587\n",
      "Epoch: 200/3000 | Batch 0003/0011 | Loss: 3149.7312\n",
      "Epoch: 200/3000 | Batch 0004/0011 | Loss: 3188.2498\n",
      "Epoch: 200/3000 | Batch 0005/0011 | Loss: 3215.7490\n",
      "Epoch: 200/3000 | Batch 0006/0011 | Loss: 3167.3696\n",
      "Epoch: 200/3000 | Batch 0007/0011 | Loss: 3190.7595\n",
      "Epoch: 200/3000 | Batch 0008/0011 | Loss: 3136.1606\n",
      "Epoch: 200/3000 | Batch 0009/0011 | Loss: 3178.9758\n",
      "Epoch: 200/3000 | Batch 0010/0011 | Loss: 3099.8625\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 201/3000 | Batch 0000/0011 | Loss: 3153.5815\n",
      "Epoch: 201/3000 | Batch 0001/0011 | Loss: 3157.6104\n",
      "Epoch: 201/3000 | Batch 0002/0011 | Loss: 3217.1023\n",
      "Epoch: 201/3000 | Batch 0003/0011 | Loss: 3200.9397\n",
      "Epoch: 201/3000 | Batch 0004/0011 | Loss: 3168.2336\n",
      "Epoch: 201/3000 | Batch 0005/0011 | Loss: 3141.7502\n",
      "Epoch: 201/3000 | Batch 0006/0011 | Loss: 3195.3823\n",
      "Epoch: 201/3000 | Batch 0007/0011 | Loss: 3255.2563\n",
      "Epoch: 201/3000 | Batch 0008/0011 | Loss: 3141.8508\n",
      "Epoch: 201/3000 | Batch 0009/0011 | Loss: 3154.8213\n",
      "Epoch: 201/3000 | Batch 0010/0011 | Loss: 3198.1248\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 202/3000 | Batch 0000/0011 | Loss: 3094.7166\n",
      "Epoch: 202/3000 | Batch 0001/0011 | Loss: 3105.9758\n",
      "Epoch: 202/3000 | Batch 0002/0011 | Loss: 3177.6638\n",
      "Epoch: 202/3000 | Batch 0003/0011 | Loss: 3192.6797\n",
      "Epoch: 202/3000 | Batch 0004/0011 | Loss: 3212.4619\n",
      "Epoch: 202/3000 | Batch 0005/0011 | Loss: 3136.6655\n",
      "Epoch: 202/3000 | Batch 0006/0011 | Loss: 3291.2944\n",
      "Epoch: 202/3000 | Batch 0007/0011 | Loss: 3091.8284\n",
      "Epoch: 202/3000 | Batch 0008/0011 | Loss: 3249.1343\n",
      "Epoch: 202/3000 | Batch 0009/0011 | Loss: 3109.0713\n",
      "Epoch: 202/3000 | Batch 0010/0011 | Loss: 3203.8799\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 203/3000 | Batch 0000/0011 | Loss: 3116.0806\n",
      "Epoch: 203/3000 | Batch 0001/0011 | Loss: 3139.5366\n",
      "Epoch: 203/3000 | Batch 0002/0011 | Loss: 3141.5732\n",
      "Epoch: 203/3000 | Batch 0003/0011 | Loss: 3140.5598\n",
      "Epoch: 203/3000 | Batch 0004/0011 | Loss: 3207.2478\n",
      "Epoch: 203/3000 | Batch 0005/0011 | Loss: 3190.0771\n",
      "Epoch: 203/3000 | Batch 0006/0011 | Loss: 3220.9431\n",
      "Epoch: 203/3000 | Batch 0007/0011 | Loss: 3166.7434\n",
      "Epoch: 203/3000 | Batch 0008/0011 | Loss: 3126.7761\n",
      "Epoch: 203/3000 | Batch 0009/0011 | Loss: 3228.2102\n",
      "Epoch: 203/3000 | Batch 0010/0011 | Loss: 3155.0493\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 204/3000 | Batch 0000/0011 | Loss: 3124.2539\n",
      "Epoch: 204/3000 | Batch 0001/0011 | Loss: 3226.6445\n",
      "Epoch: 204/3000 | Batch 0002/0011 | Loss: 3233.8933\n",
      "Epoch: 204/3000 | Batch 0003/0011 | Loss: 3111.2798\n",
      "Epoch: 204/3000 | Batch 0004/0011 | Loss: 3163.1042\n",
      "Epoch: 204/3000 | Batch 0005/0011 | Loss: 3208.1721\n",
      "Epoch: 204/3000 | Batch 0006/0011 | Loss: 3188.4023\n",
      "Epoch: 204/3000 | Batch 0007/0011 | Loss: 3128.3340\n",
      "Epoch: 204/3000 | Batch 0008/0011 | Loss: 3229.0493\n",
      "Epoch: 204/3000 | Batch 0009/0011 | Loss: 3170.6597\n",
      "Epoch: 204/3000 | Batch 0010/0011 | Loss: 3087.5210\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 205/3000 | Batch 0000/0011 | Loss: 3185.6958\n",
      "Epoch: 205/3000 | Batch 0001/0011 | Loss: 3103.8364\n",
      "Epoch: 205/3000 | Batch 0002/0011 | Loss: 3143.4336\n",
      "Epoch: 205/3000 | Batch 0003/0011 | Loss: 3207.0105\n",
      "Epoch: 205/3000 | Batch 0004/0011 | Loss: 3109.2371\n",
      "Epoch: 205/3000 | Batch 0005/0011 | Loss: 3136.3472\n",
      "Epoch: 205/3000 | Batch 0006/0011 | Loss: 3197.1670\n",
      "Epoch: 205/3000 | Batch 0007/0011 | Loss: 3152.4902\n",
      "Epoch: 205/3000 | Batch 0008/0011 | Loss: 3178.1218\n",
      "Epoch: 205/3000 | Batch 0009/0011 | Loss: 3275.0999\n",
      "Epoch: 205/3000 | Batch 0010/0011 | Loss: 3076.8792\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 206/3000 | Batch 0000/0011 | Loss: 3182.9834\n",
      "Epoch: 206/3000 | Batch 0001/0011 | Loss: 3197.5745\n",
      "Epoch: 206/3000 | Batch 0002/0011 | Loss: 3196.0261\n",
      "Epoch: 206/3000 | Batch 0003/0011 | Loss: 3195.9907\n",
      "Epoch: 206/3000 | Batch 0004/0011 | Loss: 3091.3489\n",
      "Epoch: 206/3000 | Batch 0005/0011 | Loss: 3154.7817\n",
      "Epoch: 206/3000 | Batch 0006/0011 | Loss: 3153.9670\n",
      "Epoch: 206/3000 | Batch 0007/0011 | Loss: 3218.9514\n",
      "Epoch: 206/3000 | Batch 0008/0011 | Loss: 3149.4824\n",
      "Epoch: 206/3000 | Batch 0009/0011 | Loss: 3165.7261\n",
      "Epoch: 206/3000 | Batch 0010/0011 | Loss: 3265.9263\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 207/3000 | Batch 0000/0011 | Loss: 3120.8521\n",
      "Epoch: 207/3000 | Batch 0001/0011 | Loss: 3221.3223\n",
      "Epoch: 207/3000 | Batch 0002/0011 | Loss: 3189.3428\n",
      "Epoch: 207/3000 | Batch 0003/0011 | Loss: 3096.4019\n",
      "Epoch: 207/3000 | Batch 0004/0011 | Loss: 3158.7520\n",
      "Epoch: 207/3000 | Batch 0005/0011 | Loss: 3106.5127\n",
      "Epoch: 207/3000 | Batch 0006/0011 | Loss: 3171.5750\n",
      "Epoch: 207/3000 | Batch 0007/0011 | Loss: 3215.0647\n",
      "Epoch: 207/3000 | Batch 0008/0011 | Loss: 3173.6514\n",
      "Epoch: 207/3000 | Batch 0009/0011 | Loss: 3190.1677\n",
      "Epoch: 207/3000 | Batch 0010/0011 | Loss: 3279.2744\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 208/3000 | Batch 0000/0011 | Loss: 3179.6514\n",
      "Epoch: 208/3000 | Batch 0001/0011 | Loss: 3193.7688\n",
      "Epoch: 208/3000 | Batch 0002/0011 | Loss: 3212.0305\n",
      "Epoch: 208/3000 | Batch 0003/0011 | Loss: 3158.8313\n",
      "Epoch: 208/3000 | Batch 0004/0011 | Loss: 3089.0464\n",
      "Epoch: 208/3000 | Batch 0005/0011 | Loss: 3206.1582\n",
      "Epoch: 208/3000 | Batch 0006/0011 | Loss: 3126.2754\n",
      "Epoch: 208/3000 | Batch 0007/0011 | Loss: 3250.6069\n",
      "Epoch: 208/3000 | Batch 0008/0011 | Loss: 3139.3008\n",
      "Epoch: 208/3000 | Batch 0009/0011 | Loss: 3107.1587\n",
      "Epoch: 208/3000 | Batch 0010/0011 | Loss: 3052.1702\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 209/3000 | Batch 0000/0011 | Loss: 3136.1660\n",
      "Epoch: 209/3000 | Batch 0001/0011 | Loss: 3152.9558\n",
      "Epoch: 209/3000 | Batch 0002/0011 | Loss: 3179.2341\n",
      "Epoch: 209/3000 | Batch 0003/0011 | Loss: 3097.2458\n",
      "Epoch: 209/3000 | Batch 0004/0011 | Loss: 3175.6536\n",
      "Epoch: 209/3000 | Batch 0005/0011 | Loss: 3143.9199\n",
      "Epoch: 209/3000 | Batch 0006/0011 | Loss: 3187.0845\n",
      "Epoch: 209/3000 | Batch 0007/0011 | Loss: 3198.2874\n",
      "Epoch: 209/3000 | Batch 0008/0011 | Loss: 3209.9866\n",
      "Epoch: 209/3000 | Batch 0009/0011 | Loss: 3229.0676\n",
      "Epoch: 209/3000 | Batch 0010/0011 | Loss: 3084.7053\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 210/3000 | Batch 0000/0011 | Loss: 3110.3979\n",
      "Epoch: 210/3000 | Batch 0001/0011 | Loss: 3144.9890\n",
      "Epoch: 210/3000 | Batch 0002/0011 | Loss: 3191.9004\n",
      "Epoch: 210/3000 | Batch 0003/0011 | Loss: 3171.3943\n",
      "Epoch: 210/3000 | Batch 0004/0011 | Loss: 3146.8550\n",
      "Epoch: 210/3000 | Batch 0005/0011 | Loss: 3125.2412\n",
      "Epoch: 210/3000 | Batch 0006/0011 | Loss: 3173.3542\n",
      "Epoch: 210/3000 | Batch 0007/0011 | Loss: 3267.4138\n",
      "Epoch: 210/3000 | Batch 0008/0011 | Loss: 3166.6021\n",
      "Epoch: 210/3000 | Batch 0009/0011 | Loss: 3176.1313\n",
      "Epoch: 210/3000 | Batch 0010/0011 | Loss: 3261.8289\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 211/3000 | Batch 0000/0011 | Loss: 3191.6230\n",
      "Epoch: 211/3000 | Batch 0001/0011 | Loss: 3112.1802\n",
      "Epoch: 211/3000 | Batch 0002/0011 | Loss: 3186.9209\n",
      "Epoch: 211/3000 | Batch 0003/0011 | Loss: 3152.3423\n",
      "Epoch: 211/3000 | Batch 0004/0011 | Loss: 3155.6274\n",
      "Epoch: 211/3000 | Batch 0005/0011 | Loss: 3143.3384\n",
      "Epoch: 211/3000 | Batch 0006/0011 | Loss: 3163.7341\n",
      "Epoch: 211/3000 | Batch 0007/0011 | Loss: 3224.6753\n",
      "Epoch: 211/3000 | Batch 0008/0011 | Loss: 3140.7356\n",
      "Epoch: 211/3000 | Batch 0009/0011 | Loss: 3219.1440\n",
      "Epoch: 211/3000 | Batch 0010/0011 | Loss: 3252.6333\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 212/3000 | Batch 0000/0011 | Loss: 3150.4514\n",
      "Epoch: 212/3000 | Batch 0001/0011 | Loss: 3133.1055\n",
      "Epoch: 212/3000 | Batch 0002/0011 | Loss: 3189.0872\n",
      "Epoch: 212/3000 | Batch 0003/0011 | Loss: 3151.1250\n",
      "Epoch: 212/3000 | Batch 0004/0011 | Loss: 3186.7603\n",
      "Epoch: 212/3000 | Batch 0005/0011 | Loss: 3100.1262\n",
      "Epoch: 212/3000 | Batch 0006/0011 | Loss: 3176.5164\n",
      "Epoch: 212/3000 | Batch 0007/0011 | Loss: 3219.6287\n",
      "Epoch: 212/3000 | Batch 0008/0011 | Loss: 3168.7085\n",
      "Epoch: 212/3000 | Batch 0009/0011 | Loss: 3152.8721\n",
      "Epoch: 212/3000 | Batch 0010/0011 | Loss: 3296.7720\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 213/3000 | Batch 0000/0011 | Loss: 3108.6296\n",
      "Epoch: 213/3000 | Batch 0001/0011 | Loss: 3155.3933\n",
      "Epoch: 213/3000 | Batch 0002/0011 | Loss: 3157.8711\n",
      "Epoch: 213/3000 | Batch 0003/0011 | Loss: 3170.2905\n",
      "Epoch: 213/3000 | Batch 0004/0011 | Loss: 3191.0815\n",
      "Epoch: 213/3000 | Batch 0005/0011 | Loss: 3111.1174\n",
      "Epoch: 213/3000 | Batch 0006/0011 | Loss: 3196.2041\n",
      "Epoch: 213/3000 | Batch 0007/0011 | Loss: 3171.1709\n",
      "Epoch: 213/3000 | Batch 0008/0011 | Loss: 3228.5012\n",
      "Epoch: 213/3000 | Batch 0009/0011 | Loss: 3208.7666\n",
      "Epoch: 213/3000 | Batch 0010/0011 | Loss: 3213.4380\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 214/3000 | Batch 0000/0011 | Loss: 3153.6812\n",
      "Epoch: 214/3000 | Batch 0001/0011 | Loss: 3203.9346\n",
      "Epoch: 214/3000 | Batch 0002/0011 | Loss: 3144.0457\n",
      "Epoch: 214/3000 | Batch 0003/0011 | Loss: 3169.5488\n",
      "Epoch: 214/3000 | Batch 0004/0011 | Loss: 3176.1414\n",
      "Epoch: 214/3000 | Batch 0005/0011 | Loss: 3157.1614\n",
      "Epoch: 214/3000 | Batch 0006/0011 | Loss: 3166.1528\n",
      "Epoch: 214/3000 | Batch 0007/0011 | Loss: 3203.3542\n",
      "Epoch: 214/3000 | Batch 0008/0011 | Loss: 3137.6367\n",
      "Epoch: 214/3000 | Batch 0009/0011 | Loss: 3156.0928\n",
      "Epoch: 214/3000 | Batch 0010/0011 | Loss: 2915.1885\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 215/3000 | Batch 0000/0011 | Loss: 3195.0913\n",
      "Epoch: 215/3000 | Batch 0001/0011 | Loss: 3121.4048\n",
      "Epoch: 215/3000 | Batch 0002/0011 | Loss: 3299.2937\n",
      "Epoch: 215/3000 | Batch 0003/0011 | Loss: 3114.9417\n",
      "Epoch: 215/3000 | Batch 0004/0011 | Loss: 3169.5623\n",
      "Epoch: 215/3000 | Batch 0005/0011 | Loss: 3142.0723\n",
      "Epoch: 215/3000 | Batch 0006/0011 | Loss: 3176.5430\n",
      "Epoch: 215/3000 | Batch 0007/0011 | Loss: 3152.3777\n",
      "Epoch: 215/3000 | Batch 0008/0011 | Loss: 3142.5859\n",
      "Epoch: 215/3000 | Batch 0009/0011 | Loss: 3091.3259\n",
      "Epoch: 215/3000 | Batch 0010/0011 | Loss: 3071.1802\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 216/3000 | Batch 0000/0011 | Loss: 3196.2410\n",
      "Epoch: 216/3000 | Batch 0001/0011 | Loss: 3163.7861\n",
      "Epoch: 216/3000 | Batch 0002/0011 | Loss: 3163.3691\n",
      "Epoch: 216/3000 | Batch 0003/0011 | Loss: 3256.6960\n",
      "Epoch: 216/3000 | Batch 0004/0011 | Loss: 3177.4985\n",
      "Epoch: 216/3000 | Batch 0005/0011 | Loss: 3120.0762\n",
      "Epoch: 216/3000 | Batch 0006/0011 | Loss: 3103.4854\n",
      "Epoch: 216/3000 | Batch 0007/0011 | Loss: 3206.7903\n",
      "Epoch: 216/3000 | Batch 0008/0011 | Loss: 3095.1965\n",
      "Epoch: 216/3000 | Batch 0009/0011 | Loss: 3131.9124\n",
      "Epoch: 216/3000 | Batch 0010/0011 | Loss: 3204.3020\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 217/3000 | Batch 0000/0011 | Loss: 3160.5386\n",
      "Epoch: 217/3000 | Batch 0001/0011 | Loss: 3174.1833\n",
      "Epoch: 217/3000 | Batch 0002/0011 | Loss: 3137.2258\n",
      "Epoch: 217/3000 | Batch 0003/0011 | Loss: 3079.5137\n",
      "Epoch: 217/3000 | Batch 0004/0011 | Loss: 3185.7559\n",
      "Epoch: 217/3000 | Batch 0005/0011 | Loss: 3218.8376\n",
      "Epoch: 217/3000 | Batch 0006/0011 | Loss: 3208.0115\n",
      "Epoch: 217/3000 | Batch 0007/0011 | Loss: 3137.4167\n",
      "Epoch: 217/3000 | Batch 0008/0011 | Loss: 3160.5615\n",
      "Epoch: 217/3000 | Batch 0009/0011 | Loss: 3158.9370\n",
      "Epoch: 217/3000 | Batch 0010/0011 | Loss: 3029.5220\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 218/3000 | Batch 0000/0011 | Loss: 3155.1797\n",
      "Epoch: 218/3000 | Batch 0001/0011 | Loss: 3168.7229\n",
      "Epoch: 218/3000 | Batch 0002/0011 | Loss: 3136.5303\n",
      "Epoch: 218/3000 | Batch 0003/0011 | Loss: 3161.9663\n",
      "Epoch: 218/3000 | Batch 0004/0011 | Loss: 3145.4265\n",
      "Epoch: 218/3000 | Batch 0005/0011 | Loss: 3139.4883\n",
      "Epoch: 218/3000 | Batch 0006/0011 | Loss: 3225.3914\n",
      "Epoch: 218/3000 | Batch 0007/0011 | Loss: 3202.6216\n",
      "Epoch: 218/3000 | Batch 0008/0011 | Loss: 3184.4392\n",
      "Epoch: 218/3000 | Batch 0009/0011 | Loss: 3140.0251\n",
      "Epoch: 218/3000 | Batch 0010/0011 | Loss: 3045.8242\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 219/3000 | Batch 0000/0011 | Loss: 3150.9407\n",
      "Epoch: 219/3000 | Batch 0001/0011 | Loss: 3200.3821\n",
      "Epoch: 219/3000 | Batch 0002/0011 | Loss: 3141.2439\n",
      "Epoch: 219/3000 | Batch 0003/0011 | Loss: 3114.0930\n",
      "Epoch: 219/3000 | Batch 0004/0011 | Loss: 3156.6938\n",
      "Epoch: 219/3000 | Batch 0005/0011 | Loss: 3081.6313\n",
      "Epoch: 219/3000 | Batch 0006/0011 | Loss: 3133.6714\n",
      "Epoch: 219/3000 | Batch 0007/0011 | Loss: 3237.7207\n",
      "Epoch: 219/3000 | Batch 0008/0011 | Loss: 3291.5034\n",
      "Epoch: 219/3000 | Batch 0009/0011 | Loss: 3097.0793\n",
      "Epoch: 219/3000 | Batch 0010/0011 | Loss: 3156.3774\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 220/3000 | Batch 0000/0011 | Loss: 3278.6775\n",
      "Epoch: 220/3000 | Batch 0001/0011 | Loss: 3111.8457\n",
      "Epoch: 220/3000 | Batch 0002/0011 | Loss: 3118.5203\n",
      "Epoch: 220/3000 | Batch 0003/0011 | Loss: 3162.3254\n",
      "Epoch: 220/3000 | Batch 0004/0011 | Loss: 3167.0779\n",
      "Epoch: 220/3000 | Batch 0005/0011 | Loss: 3191.7710\n",
      "Epoch: 220/3000 | Batch 0006/0011 | Loss: 3130.2188\n",
      "Epoch: 220/3000 | Batch 0007/0011 | Loss: 3164.5012\n",
      "Epoch: 220/3000 | Batch 0008/0011 | Loss: 3152.8979\n",
      "Epoch: 220/3000 | Batch 0009/0011 | Loss: 3114.4482\n",
      "Epoch: 220/3000 | Batch 0010/0011 | Loss: 3290.4385\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 221/3000 | Batch 0000/0011 | Loss: 3105.5803\n",
      "Epoch: 221/3000 | Batch 0001/0011 | Loss: 3230.4658\n",
      "Epoch: 221/3000 | Batch 0002/0011 | Loss: 3174.5503\n",
      "Epoch: 221/3000 | Batch 0003/0011 | Loss: 3149.2764\n",
      "Epoch: 221/3000 | Batch 0004/0011 | Loss: 3165.0791\n",
      "Epoch: 221/3000 | Batch 0005/0011 | Loss: 3161.5920\n",
      "Epoch: 221/3000 | Batch 0006/0011 | Loss: 3121.2991\n",
      "Epoch: 221/3000 | Batch 0007/0011 | Loss: 3192.0530\n",
      "Epoch: 221/3000 | Batch 0008/0011 | Loss: 3261.5681\n",
      "Epoch: 221/3000 | Batch 0009/0011 | Loss: 3106.6968\n",
      "Epoch: 221/3000 | Batch 0010/0011 | Loss: 2988.2886\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 222/3000 | Batch 0000/0011 | Loss: 3224.7190\n",
      "Epoch: 222/3000 | Batch 0001/0011 | Loss: 3185.5603\n",
      "Epoch: 222/3000 | Batch 0002/0011 | Loss: 3112.0754\n",
      "Epoch: 222/3000 | Batch 0003/0011 | Loss: 3138.2429\n",
      "Epoch: 222/3000 | Batch 0004/0011 | Loss: 3214.0271\n",
      "Epoch: 222/3000 | Batch 0005/0011 | Loss: 3168.3865\n",
      "Epoch: 222/3000 | Batch 0006/0011 | Loss: 3083.3860\n",
      "Epoch: 222/3000 | Batch 0007/0011 | Loss: 3168.9421\n",
      "Epoch: 222/3000 | Batch 0008/0011 | Loss: 3131.0432\n",
      "Epoch: 222/3000 | Batch 0009/0011 | Loss: 3176.9653\n",
      "Epoch: 222/3000 | Batch 0010/0011 | Loss: 3152.0552\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 223/3000 | Batch 0000/0011 | Loss: 3125.2368\n",
      "Epoch: 223/3000 | Batch 0001/0011 | Loss: 3187.8882\n",
      "Epoch: 223/3000 | Batch 0002/0011 | Loss: 3132.3960\n",
      "Epoch: 223/3000 | Batch 0003/0011 | Loss: 3128.6426\n",
      "Epoch: 223/3000 | Batch 0004/0011 | Loss: 3129.8545\n",
      "Epoch: 223/3000 | Batch 0005/0011 | Loss: 3187.8396\n",
      "Epoch: 223/3000 | Batch 0006/0011 | Loss: 3175.0142\n",
      "Epoch: 223/3000 | Batch 0007/0011 | Loss: 3228.7202\n",
      "Epoch: 223/3000 | Batch 0008/0011 | Loss: 3116.1169\n",
      "Epoch: 223/3000 | Batch 0009/0011 | Loss: 3224.3779\n",
      "Epoch: 223/3000 | Batch 0010/0011 | Loss: 2983.6650\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 224/3000 | Batch 0000/0011 | Loss: 3219.7510\n",
      "Epoch: 224/3000 | Batch 0001/0011 | Loss: 3152.0276\n",
      "Epoch: 224/3000 | Batch 0002/0011 | Loss: 3165.9614\n",
      "Epoch: 224/3000 | Batch 0003/0011 | Loss: 3091.7397\n",
      "Epoch: 224/3000 | Batch 0004/0011 | Loss: 3120.0771\n",
      "Epoch: 224/3000 | Batch 0005/0011 | Loss: 3192.1924\n",
      "Epoch: 224/3000 | Batch 0006/0011 | Loss: 3176.7078\n",
      "Epoch: 224/3000 | Batch 0007/0011 | Loss: 3227.2080\n",
      "Epoch: 224/3000 | Batch 0008/0011 | Loss: 3162.4114\n",
      "Epoch: 224/3000 | Batch 0009/0011 | Loss: 3082.2454\n",
      "Epoch: 224/3000 | Batch 0010/0011 | Loss: 3329.3706\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 225/3000 | Batch 0000/0011 | Loss: 3226.5339\n",
      "Epoch: 225/3000 | Batch 0001/0011 | Loss: 3138.1445\n",
      "Epoch: 225/3000 | Batch 0002/0011 | Loss: 3096.8953\n",
      "Epoch: 225/3000 | Batch 0003/0011 | Loss: 3188.4910\n",
      "Epoch: 225/3000 | Batch 0004/0011 | Loss: 3108.8379\n",
      "Epoch: 225/3000 | Batch 0005/0011 | Loss: 3211.2856\n",
      "Epoch: 225/3000 | Batch 0006/0011 | Loss: 3196.1899\n",
      "Epoch: 225/3000 | Batch 0007/0011 | Loss: 3179.3508\n",
      "Epoch: 225/3000 | Batch 0008/0011 | Loss: 3160.8259\n",
      "Epoch: 225/3000 | Batch 0009/0011 | Loss: 3148.4355\n",
      "Epoch: 225/3000 | Batch 0010/0011 | Loss: 3045.2937\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 226/3000 | Batch 0000/0011 | Loss: 3248.4639\n",
      "Epoch: 226/3000 | Batch 0001/0011 | Loss: 3236.9167\n",
      "Epoch: 226/3000 | Batch 0002/0011 | Loss: 3158.2886\n",
      "Epoch: 226/3000 | Batch 0003/0011 | Loss: 3101.5593\n",
      "Epoch: 226/3000 | Batch 0004/0011 | Loss: 3054.3184\n",
      "Epoch: 226/3000 | Batch 0005/0011 | Loss: 3201.1567\n",
      "Epoch: 226/3000 | Batch 0006/0011 | Loss: 3157.7478\n",
      "Epoch: 226/3000 | Batch 0007/0011 | Loss: 3196.5984\n",
      "Epoch: 226/3000 | Batch 0008/0011 | Loss: 3096.7097\n",
      "Epoch: 226/3000 | Batch 0009/0011 | Loss: 3209.7075\n",
      "Epoch: 226/3000 | Batch 0010/0011 | Loss: 3274.7803\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 227/3000 | Batch 0000/0011 | Loss: 3183.0618\n",
      "Epoch: 227/3000 | Batch 0001/0011 | Loss: 3233.4963\n",
      "Epoch: 227/3000 | Batch 0002/0011 | Loss: 3144.5017\n",
      "Epoch: 227/3000 | Batch 0003/0011 | Loss: 3111.8250\n",
      "Epoch: 227/3000 | Batch 0004/0011 | Loss: 3141.2908\n",
      "Epoch: 227/3000 | Batch 0005/0011 | Loss: 3257.1274\n",
      "Epoch: 227/3000 | Batch 0006/0011 | Loss: 3200.9629\n",
      "Epoch: 227/3000 | Batch 0007/0011 | Loss: 3098.8499\n",
      "Epoch: 227/3000 | Batch 0008/0011 | Loss: 3086.2354\n",
      "Epoch: 227/3000 | Batch 0009/0011 | Loss: 3189.1895\n",
      "Epoch: 227/3000 | Batch 0010/0011 | Loss: 3248.2795\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 228/3000 | Batch 0000/0011 | Loss: 3112.4004\n",
      "Epoch: 228/3000 | Batch 0001/0011 | Loss: 3231.5046\n",
      "Epoch: 228/3000 | Batch 0002/0011 | Loss: 3154.2756\n",
      "Epoch: 228/3000 | Batch 0003/0011 | Loss: 3232.0315\n",
      "Epoch: 228/3000 | Batch 0004/0011 | Loss: 3194.3638\n",
      "Epoch: 228/3000 | Batch 0005/0011 | Loss: 3173.2664\n",
      "Epoch: 228/3000 | Batch 0006/0011 | Loss: 3167.0723\n",
      "Epoch: 228/3000 | Batch 0007/0011 | Loss: 3151.8552\n",
      "Epoch: 228/3000 | Batch 0008/0011 | Loss: 3175.7083\n",
      "Epoch: 228/3000 | Batch 0009/0011 | Loss: 3094.7341\n",
      "Epoch: 228/3000 | Batch 0010/0011 | Loss: 3228.8357\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 229/3000 | Batch 0000/0011 | Loss: 3176.3909\n",
      "Epoch: 229/3000 | Batch 0001/0011 | Loss: 3145.9558\n",
      "Epoch: 229/3000 | Batch 0002/0011 | Loss: 3167.2673\n",
      "Epoch: 229/3000 | Batch 0003/0011 | Loss: 3120.0752\n",
      "Epoch: 229/3000 | Batch 0004/0011 | Loss: 3148.0920\n",
      "Epoch: 229/3000 | Batch 0005/0011 | Loss: 3179.5564\n",
      "Epoch: 229/3000 | Batch 0006/0011 | Loss: 3221.3560\n",
      "Epoch: 229/3000 | Batch 0007/0011 | Loss: 3113.5405\n",
      "Epoch: 229/3000 | Batch 0008/0011 | Loss: 3195.6582\n",
      "Epoch: 229/3000 | Batch 0009/0011 | Loss: 3133.0461\n",
      "Epoch: 229/3000 | Batch 0010/0011 | Loss: 3192.4866\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 230/3000 | Batch 0000/0011 | Loss: 3096.2473\n",
      "Epoch: 230/3000 | Batch 0001/0011 | Loss: 3103.1013\n",
      "Epoch: 230/3000 | Batch 0002/0011 | Loss: 3260.0522\n",
      "Epoch: 230/3000 | Batch 0003/0011 | Loss: 3135.1775\n",
      "Epoch: 230/3000 | Batch 0004/0011 | Loss: 3293.1338\n",
      "Epoch: 230/3000 | Batch 0005/0011 | Loss: 3133.1509\n",
      "Epoch: 230/3000 | Batch 0006/0011 | Loss: 3167.5505\n",
      "Epoch: 230/3000 | Batch 0007/0011 | Loss: 3139.4250\n",
      "Epoch: 230/3000 | Batch 0008/0011 | Loss: 3179.6567\n",
      "Epoch: 230/3000 | Batch 0009/0011 | Loss: 3119.7654\n",
      "Epoch: 230/3000 | Batch 0010/0011 | Loss: 3187.2769\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 231/3000 | Batch 0000/0011 | Loss: 3137.6113\n",
      "Epoch: 231/3000 | Batch 0001/0011 | Loss: 3167.0676\n",
      "Epoch: 231/3000 | Batch 0002/0011 | Loss: 3141.5295\n",
      "Epoch: 231/3000 | Batch 0003/0011 | Loss: 3162.6560\n",
      "Epoch: 231/3000 | Batch 0004/0011 | Loss: 3198.6682\n",
      "Epoch: 231/3000 | Batch 0005/0011 | Loss: 3180.1909\n",
      "Epoch: 231/3000 | Batch 0006/0011 | Loss: 3125.3608\n",
      "Epoch: 231/3000 | Batch 0007/0011 | Loss: 3079.7607\n",
      "Epoch: 231/3000 | Batch 0008/0011 | Loss: 3166.7566\n",
      "Epoch: 231/3000 | Batch 0009/0011 | Loss: 3225.9082\n",
      "Epoch: 231/3000 | Batch 0010/0011 | Loss: 3245.0012\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 232/3000 | Batch 0000/0011 | Loss: 3135.5557\n",
      "Epoch: 232/3000 | Batch 0001/0011 | Loss: 3190.2190\n",
      "Epoch: 232/3000 | Batch 0002/0011 | Loss: 3133.5281\n",
      "Epoch: 232/3000 | Batch 0003/0011 | Loss: 3098.6870\n",
      "Epoch: 232/3000 | Batch 0004/0011 | Loss: 3160.5901\n",
      "Epoch: 232/3000 | Batch 0005/0011 | Loss: 3172.1904\n",
      "Epoch: 232/3000 | Batch 0006/0011 | Loss: 3185.7156\n",
      "Epoch: 232/3000 | Batch 0007/0011 | Loss: 3199.4399\n",
      "Epoch: 232/3000 | Batch 0008/0011 | Loss: 3115.3257\n",
      "Epoch: 232/3000 | Batch 0009/0011 | Loss: 3184.0366\n",
      "Epoch: 232/3000 | Batch 0010/0011 | Loss: 3285.7244\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 233/3000 | Batch 0000/0011 | Loss: 3160.9507\n",
      "Epoch: 233/3000 | Batch 0001/0011 | Loss: 3119.5134\n",
      "Epoch: 233/3000 | Batch 0002/0011 | Loss: 3192.1448\n",
      "Epoch: 233/3000 | Batch 0003/0011 | Loss: 3174.7568\n",
      "Epoch: 233/3000 | Batch 0004/0011 | Loss: 3158.2683\n",
      "Epoch: 233/3000 | Batch 0005/0011 | Loss: 3144.6321\n",
      "Epoch: 233/3000 | Batch 0006/0011 | Loss: 3223.3005\n",
      "Epoch: 233/3000 | Batch 0007/0011 | Loss: 3126.0095\n",
      "Epoch: 233/3000 | Batch 0008/0011 | Loss: 3136.8855\n",
      "Epoch: 233/3000 | Batch 0009/0011 | Loss: 3164.7964\n",
      "Epoch: 233/3000 | Batch 0010/0011 | Loss: 3072.9460\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 234/3000 | Batch 0000/0011 | Loss: 3127.5693\n",
      "Epoch: 234/3000 | Batch 0001/0011 | Loss: 3153.3887\n",
      "Epoch: 234/3000 | Batch 0002/0011 | Loss: 3130.9207\n",
      "Epoch: 234/3000 | Batch 0003/0011 | Loss: 3149.5764\n",
      "Epoch: 234/3000 | Batch 0004/0011 | Loss: 3220.1885\n",
      "Epoch: 234/3000 | Batch 0005/0011 | Loss: 3181.8728\n",
      "Epoch: 234/3000 | Batch 0006/0011 | Loss: 3273.7014\n",
      "Epoch: 234/3000 | Batch 0007/0011 | Loss: 3180.7258\n",
      "Epoch: 234/3000 | Batch 0008/0011 | Loss: 3126.0039\n",
      "Epoch: 234/3000 | Batch 0009/0011 | Loss: 3118.2236\n",
      "Epoch: 234/3000 | Batch 0010/0011 | Loss: 3077.2429\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 235/3000 | Batch 0000/0011 | Loss: 3183.4397\n",
      "Epoch: 235/3000 | Batch 0001/0011 | Loss: 3214.7983\n",
      "Epoch: 235/3000 | Batch 0002/0011 | Loss: 3059.5674\n",
      "Epoch: 235/3000 | Batch 0003/0011 | Loss: 3163.8196\n",
      "Epoch: 235/3000 | Batch 0004/0011 | Loss: 3164.8276\n",
      "Epoch: 235/3000 | Batch 0005/0011 | Loss: 3137.5869\n",
      "Epoch: 235/3000 | Batch 0006/0011 | Loss: 3128.1436\n",
      "Epoch: 235/3000 | Batch 0007/0011 | Loss: 3188.1721\n",
      "Epoch: 235/3000 | Batch 0008/0011 | Loss: 3105.5681\n",
      "Epoch: 235/3000 | Batch 0009/0011 | Loss: 3197.6543\n",
      "Epoch: 235/3000 | Batch 0010/0011 | Loss: 3302.6201\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 236/3000 | Batch 0000/0011 | Loss: 3089.9646\n",
      "Epoch: 236/3000 | Batch 0001/0011 | Loss: 3116.6367\n",
      "Epoch: 236/3000 | Batch 0002/0011 | Loss: 3206.2170\n",
      "Epoch: 236/3000 | Batch 0003/0011 | Loss: 3199.2075\n",
      "Epoch: 236/3000 | Batch 0004/0011 | Loss: 3166.7327\n",
      "Epoch: 236/3000 | Batch 0005/0011 | Loss: 3108.0979\n",
      "Epoch: 236/3000 | Batch 0006/0011 | Loss: 3192.5366\n",
      "Epoch: 236/3000 | Batch 0007/0011 | Loss: 3227.3462\n",
      "Epoch: 236/3000 | Batch 0008/0011 | Loss: 3179.5059\n",
      "Epoch: 236/3000 | Batch 0009/0011 | Loss: 3132.0662\n",
      "Epoch: 236/3000 | Batch 0010/0011 | Loss: 3230.6873\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 237/3000 | Batch 0000/0011 | Loss: 3128.7585\n",
      "Epoch: 237/3000 | Batch 0001/0011 | Loss: 3195.3306\n",
      "Epoch: 237/3000 | Batch 0002/0011 | Loss: 3111.1125\n",
      "Epoch: 237/3000 | Batch 0003/0011 | Loss: 3169.7820\n",
      "Epoch: 237/3000 | Batch 0004/0011 | Loss: 3169.8040\n",
      "Epoch: 237/3000 | Batch 0005/0011 | Loss: 3126.2717\n",
      "Epoch: 237/3000 | Batch 0006/0011 | Loss: 3174.3176\n",
      "Epoch: 237/3000 | Batch 0007/0011 | Loss: 3147.0754\n",
      "Epoch: 237/3000 | Batch 0008/0011 | Loss: 3152.6118\n",
      "Epoch: 237/3000 | Batch 0009/0011 | Loss: 3278.6421\n",
      "Epoch: 237/3000 | Batch 0010/0011 | Loss: 3064.0247\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 238/3000 | Batch 0000/0011 | Loss: 3222.8777\n",
      "Epoch: 238/3000 | Batch 0001/0011 | Loss: 3126.3484\n",
      "Epoch: 238/3000 | Batch 0002/0011 | Loss: 3233.8740\n",
      "Epoch: 238/3000 | Batch 0003/0011 | Loss: 3124.2009\n",
      "Epoch: 238/3000 | Batch 0004/0011 | Loss: 3155.8052\n",
      "Epoch: 238/3000 | Batch 0005/0011 | Loss: 3130.1475\n",
      "Epoch: 238/3000 | Batch 0006/0011 | Loss: 3154.1948\n",
      "Epoch: 238/3000 | Batch 0007/0011 | Loss: 3086.7417\n",
      "Epoch: 238/3000 | Batch 0008/0011 | Loss: 3171.5586\n",
      "Epoch: 238/3000 | Batch 0009/0011 | Loss: 3179.7092\n",
      "Epoch: 238/3000 | Batch 0010/0011 | Loss: 3174.7786\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 239/3000 | Batch 0000/0011 | Loss: 3127.9355\n",
      "Epoch: 239/3000 | Batch 0001/0011 | Loss: 3155.6257\n",
      "Epoch: 239/3000 | Batch 0002/0011 | Loss: 3175.9307\n",
      "Epoch: 239/3000 | Batch 0003/0011 | Loss: 3168.5483\n",
      "Epoch: 239/3000 | Batch 0004/0011 | Loss: 3138.2451\n",
      "Epoch: 239/3000 | Batch 0005/0011 | Loss: 3240.6750\n",
      "Epoch: 239/3000 | Batch 0006/0011 | Loss: 3165.1934\n",
      "Epoch: 239/3000 | Batch 0007/0011 | Loss: 3174.0291\n",
      "Epoch: 239/3000 | Batch 0008/0011 | Loss: 3066.6736\n",
      "Epoch: 239/3000 | Batch 0009/0011 | Loss: 3219.0847\n",
      "Epoch: 239/3000 | Batch 0010/0011 | Loss: 3210.4512\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 240/3000 | Batch 0000/0011 | Loss: 3147.2551\n",
      "Epoch: 240/3000 | Batch 0001/0011 | Loss: 3224.8223\n",
      "Epoch: 240/3000 | Batch 0002/0011 | Loss: 3214.4707\n",
      "Epoch: 240/3000 | Batch 0003/0011 | Loss: 3158.5032\n",
      "Epoch: 240/3000 | Batch 0004/0011 | Loss: 3140.9524\n",
      "Epoch: 240/3000 | Batch 0005/0011 | Loss: 3116.8828\n",
      "Epoch: 240/3000 | Batch 0006/0011 | Loss: 3190.9016\n",
      "Epoch: 240/3000 | Batch 0007/0011 | Loss: 3173.2046\n",
      "Epoch: 240/3000 | Batch 0008/0011 | Loss: 3134.4722\n",
      "Epoch: 240/3000 | Batch 0009/0011 | Loss: 3119.8870\n",
      "Epoch: 240/3000 | Batch 0010/0011 | Loss: 2987.8560\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 241/3000 | Batch 0000/0011 | Loss: 3183.7869\n",
      "Epoch: 241/3000 | Batch 0001/0011 | Loss: 3130.6628\n",
      "Epoch: 241/3000 | Batch 0002/0011 | Loss: 3208.8467\n",
      "Epoch: 241/3000 | Batch 0003/0011 | Loss: 3135.2083\n",
      "Epoch: 241/3000 | Batch 0004/0011 | Loss: 3206.1802\n",
      "Epoch: 241/3000 | Batch 0005/0011 | Loss: 3070.9106\n",
      "Epoch: 241/3000 | Batch 0006/0011 | Loss: 3163.0996\n",
      "Epoch: 241/3000 | Batch 0007/0011 | Loss: 3165.3970\n",
      "Epoch: 241/3000 | Batch 0008/0011 | Loss: 3115.8677\n",
      "Epoch: 241/3000 | Batch 0009/0011 | Loss: 3214.8569\n",
      "Epoch: 241/3000 | Batch 0010/0011 | Loss: 3178.1194\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 242/3000 | Batch 0000/0011 | Loss: 3171.3367\n",
      "Epoch: 242/3000 | Batch 0001/0011 | Loss: 3155.6960\n",
      "Epoch: 242/3000 | Batch 0002/0011 | Loss: 3087.7031\n",
      "Epoch: 242/3000 | Batch 0003/0011 | Loss: 3144.2278\n",
      "Epoch: 242/3000 | Batch 0004/0011 | Loss: 3145.4514\n",
      "Epoch: 242/3000 | Batch 0005/0011 | Loss: 3156.5037\n",
      "Epoch: 242/3000 | Batch 0006/0011 | Loss: 3088.4255\n",
      "Epoch: 242/3000 | Batch 0007/0011 | Loss: 3200.4001\n",
      "Epoch: 242/3000 | Batch 0008/0011 | Loss: 3212.2954\n",
      "Epoch: 242/3000 | Batch 0009/0011 | Loss: 3213.1392\n",
      "Epoch: 242/3000 | Batch 0010/0011 | Loss: 3077.4949\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 243/3000 | Batch 0000/0011 | Loss: 3161.7292\n",
      "Epoch: 243/3000 | Batch 0001/0011 | Loss: 3090.3320\n",
      "Epoch: 243/3000 | Batch 0002/0011 | Loss: 3164.5569\n",
      "Epoch: 243/3000 | Batch 0003/0011 | Loss: 3139.3518\n",
      "Epoch: 243/3000 | Batch 0004/0011 | Loss: 3246.8950\n",
      "Epoch: 243/3000 | Batch 0005/0011 | Loss: 3155.4280\n",
      "Epoch: 243/3000 | Batch 0006/0011 | Loss: 3090.1331\n",
      "Epoch: 243/3000 | Batch 0007/0011 | Loss: 3164.3513\n",
      "Epoch: 243/3000 | Batch 0008/0011 | Loss: 3182.9536\n",
      "Epoch: 243/3000 | Batch 0009/0011 | Loss: 3220.9558\n",
      "Epoch: 243/3000 | Batch 0010/0011 | Loss: 3075.0256\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 244/3000 | Batch 0000/0011 | Loss: 3213.5129\n",
      "Epoch: 244/3000 | Batch 0001/0011 | Loss: 3082.1594\n",
      "Epoch: 244/3000 | Batch 0002/0011 | Loss: 3146.0691\n",
      "Epoch: 244/3000 | Batch 0003/0011 | Loss: 3167.2195\n",
      "Epoch: 244/3000 | Batch 0004/0011 | Loss: 3115.6064\n",
      "Epoch: 244/3000 | Batch 0005/0011 | Loss: 3176.7822\n",
      "Epoch: 244/3000 | Batch 0006/0011 | Loss: 3204.9524\n",
      "Epoch: 244/3000 | Batch 0007/0011 | Loss: 3142.1394\n",
      "Epoch: 244/3000 | Batch 0008/0011 | Loss: 3162.3870\n",
      "Epoch: 244/3000 | Batch 0009/0011 | Loss: 3180.5481\n",
      "Epoch: 244/3000 | Batch 0010/0011 | Loss: 3120.0781\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 245/3000 | Batch 0000/0011 | Loss: 3151.2498\n",
      "Epoch: 245/3000 | Batch 0001/0011 | Loss: 3186.5564\n",
      "Epoch: 245/3000 | Batch 0002/0011 | Loss: 3096.9717\n",
      "Epoch: 245/3000 | Batch 0003/0011 | Loss: 3184.9692\n",
      "Epoch: 245/3000 | Batch 0004/0011 | Loss: 3187.1492\n",
      "Epoch: 245/3000 | Batch 0005/0011 | Loss: 3283.9785\n",
      "Epoch: 245/3000 | Batch 0006/0011 | Loss: 3139.1372\n",
      "Epoch: 245/3000 | Batch 0007/0011 | Loss: 3129.3767\n",
      "Epoch: 245/3000 | Batch 0008/0011 | Loss: 3209.1753\n",
      "Epoch: 245/3000 | Batch 0009/0011 | Loss: 3090.3096\n",
      "Epoch: 245/3000 | Batch 0010/0011 | Loss: 3022.9614\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 246/3000 | Batch 0000/0011 | Loss: 3235.4512\n",
      "Epoch: 246/3000 | Batch 0001/0011 | Loss: 3226.7432\n",
      "Epoch: 246/3000 | Batch 0002/0011 | Loss: 3161.5911\n",
      "Epoch: 246/3000 | Batch 0003/0011 | Loss: 3173.6277\n",
      "Epoch: 246/3000 | Batch 0004/0011 | Loss: 3090.6470\n",
      "Epoch: 246/3000 | Batch 0005/0011 | Loss: 3156.3281\n",
      "Epoch: 246/3000 | Batch 0006/0011 | Loss: 3162.9392\n",
      "Epoch: 246/3000 | Batch 0007/0011 | Loss: 3164.8708\n",
      "Epoch: 246/3000 | Batch 0008/0011 | Loss: 3123.3918\n",
      "Epoch: 246/3000 | Batch 0009/0011 | Loss: 3141.8406\n",
      "Epoch: 246/3000 | Batch 0010/0011 | Loss: 2996.9846\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 247/3000 | Batch 0000/0011 | Loss: 3200.3906\n",
      "Epoch: 247/3000 | Batch 0001/0011 | Loss: 3142.6169\n",
      "Epoch: 247/3000 | Batch 0002/0011 | Loss: 3190.6396\n",
      "Epoch: 247/3000 | Batch 0003/0011 | Loss: 3130.7847\n",
      "Epoch: 247/3000 | Batch 0004/0011 | Loss: 3120.0681\n",
      "Epoch: 247/3000 | Batch 0005/0011 | Loss: 3156.8291\n",
      "Epoch: 247/3000 | Batch 0006/0011 | Loss: 3216.8521\n",
      "Epoch: 247/3000 | Batch 0007/0011 | Loss: 3161.4822\n",
      "Epoch: 247/3000 | Batch 0008/0011 | Loss: 3127.4102\n",
      "Epoch: 247/3000 | Batch 0009/0011 | Loss: 3133.2407\n",
      "Epoch: 247/3000 | Batch 0010/0011 | Loss: 3043.6094\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 248/3000 | Batch 0000/0011 | Loss: 3132.1228\n",
      "Epoch: 248/3000 | Batch 0001/0011 | Loss: 3150.7883\n",
      "Epoch: 248/3000 | Batch 0002/0011 | Loss: 3164.0046\n",
      "Epoch: 248/3000 | Batch 0003/0011 | Loss: 3099.3494\n",
      "Epoch: 248/3000 | Batch 0004/0011 | Loss: 3206.6047\n",
      "Epoch: 248/3000 | Batch 0005/0011 | Loss: 3144.7048\n",
      "Epoch: 248/3000 | Batch 0006/0011 | Loss: 3130.4680\n",
      "Epoch: 248/3000 | Batch 0007/0011 | Loss: 3225.7502\n",
      "Epoch: 248/3000 | Batch 0008/0011 | Loss: 3130.4219\n",
      "Epoch: 248/3000 | Batch 0009/0011 | Loss: 3240.0452\n",
      "Epoch: 248/3000 | Batch 0010/0011 | Loss: 3126.7742\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 249/3000 | Batch 0000/0011 | Loss: 3213.3391\n",
      "Epoch: 249/3000 | Batch 0001/0011 | Loss: 3096.8040\n",
      "Epoch: 249/3000 | Batch 0002/0011 | Loss: 3162.2429\n",
      "Epoch: 249/3000 | Batch 0003/0011 | Loss: 3129.1233\n",
      "Epoch: 249/3000 | Batch 0004/0011 | Loss: 3149.2783\n",
      "Epoch: 249/3000 | Batch 0005/0011 | Loss: 3086.7915\n",
      "Epoch: 249/3000 | Batch 0006/0011 | Loss: 3144.5154\n",
      "Epoch: 249/3000 | Batch 0007/0011 | Loss: 3246.6116\n",
      "Epoch: 249/3000 | Batch 0008/0011 | Loss: 3142.4958\n",
      "Epoch: 249/3000 | Batch 0009/0011 | Loss: 3224.1843\n",
      "Epoch: 249/3000 | Batch 0010/0011 | Loss: 3067.0576\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 250/3000 | Batch 0000/0011 | Loss: 3167.0457\n",
      "Epoch: 250/3000 | Batch 0001/0011 | Loss: 3157.8806\n",
      "Epoch: 250/3000 | Batch 0002/0011 | Loss: 3224.3940\n",
      "Epoch: 250/3000 | Batch 0003/0011 | Loss: 3074.2517\n",
      "Epoch: 250/3000 | Batch 0004/0011 | Loss: 3100.1895\n",
      "Epoch: 250/3000 | Batch 0005/0011 | Loss: 3150.0803\n",
      "Epoch: 250/3000 | Batch 0006/0011 | Loss: 3196.1304\n",
      "Epoch: 250/3000 | Batch 0007/0011 | Loss: 3222.3477\n",
      "Epoch: 250/3000 | Batch 0008/0011 | Loss: 3128.3599\n",
      "Epoch: 250/3000 | Batch 0009/0011 | Loss: 3110.9199\n",
      "Epoch: 250/3000 | Batch 0010/0011 | Loss: 3306.2546\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 251/3000 | Batch 0000/0011 | Loss: 3198.3875\n",
      "Epoch: 251/3000 | Batch 0001/0011 | Loss: 3111.0061\n",
      "Epoch: 251/3000 | Batch 0002/0011 | Loss: 3182.8855\n",
      "Epoch: 251/3000 | Batch 0003/0011 | Loss: 3123.6301\n",
      "Epoch: 251/3000 | Batch 0004/0011 | Loss: 3191.1660\n",
      "Epoch: 251/3000 | Batch 0005/0011 | Loss: 3155.9502\n",
      "Epoch: 251/3000 | Batch 0006/0011 | Loss: 3179.6045\n",
      "Epoch: 251/3000 | Batch 0007/0011 | Loss: 3157.0105\n",
      "Epoch: 251/3000 | Batch 0008/0011 | Loss: 3128.1621\n",
      "Epoch: 251/3000 | Batch 0009/0011 | Loss: 3144.5991\n",
      "Epoch: 251/3000 | Batch 0010/0011 | Loss: 3129.6809\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 252/3000 | Batch 0000/0011 | Loss: 3148.7610\n",
      "Epoch: 252/3000 | Batch 0001/0011 | Loss: 3190.8066\n",
      "Epoch: 252/3000 | Batch 0002/0011 | Loss: 3141.5789\n",
      "Epoch: 252/3000 | Batch 0003/0011 | Loss: 3165.7041\n",
      "Epoch: 252/3000 | Batch 0004/0011 | Loss: 3137.1370\n",
      "Epoch: 252/3000 | Batch 0005/0011 | Loss: 3172.0081\n",
      "Epoch: 252/3000 | Batch 0006/0011 | Loss: 3158.4060\n",
      "Epoch: 252/3000 | Batch 0007/0011 | Loss: 3213.9382\n",
      "Epoch: 252/3000 | Batch 0008/0011 | Loss: 3176.9226\n",
      "Epoch: 252/3000 | Batch 0009/0011 | Loss: 3060.3218\n",
      "Epoch: 252/3000 | Batch 0010/0011 | Loss: 3181.1553\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 253/3000 | Batch 0000/0011 | Loss: 3212.3223\n",
      "Epoch: 253/3000 | Batch 0001/0011 | Loss: 3200.4268\n",
      "Epoch: 253/3000 | Batch 0002/0011 | Loss: 3107.9177\n",
      "Epoch: 253/3000 | Batch 0003/0011 | Loss: 3158.1875\n",
      "Epoch: 253/3000 | Batch 0004/0011 | Loss: 3071.6636\n",
      "Epoch: 253/3000 | Batch 0005/0011 | Loss: 3125.6638\n",
      "Epoch: 253/3000 | Batch 0006/0011 | Loss: 3099.9739\n",
      "Epoch: 253/3000 | Batch 0007/0011 | Loss: 3158.0171\n",
      "Epoch: 253/3000 | Batch 0008/0011 | Loss: 3200.6582\n",
      "Epoch: 253/3000 | Batch 0009/0011 | Loss: 3172.5562\n",
      "Epoch: 253/3000 | Batch 0010/0011 | Loss: 3331.8853\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 254/3000 | Batch 0000/0011 | Loss: 3082.9722\n",
      "Epoch: 254/3000 | Batch 0001/0011 | Loss: 3202.3262\n",
      "Epoch: 254/3000 | Batch 0002/0011 | Loss: 3218.6052\n",
      "Epoch: 254/3000 | Batch 0003/0011 | Loss: 3133.2019\n",
      "Epoch: 254/3000 | Batch 0004/0011 | Loss: 3152.7795\n",
      "Epoch: 254/3000 | Batch 0005/0011 | Loss: 3159.0979\n",
      "Epoch: 254/3000 | Batch 0006/0011 | Loss: 3170.3958\n",
      "Epoch: 254/3000 | Batch 0007/0011 | Loss: 3116.3972\n",
      "Epoch: 254/3000 | Batch 0008/0011 | Loss: 3156.7129\n",
      "Epoch: 254/3000 | Batch 0009/0011 | Loss: 3206.1873\n",
      "Epoch: 254/3000 | Batch 0010/0011 | Loss: 3070.8723\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 255/3000 | Batch 0000/0011 | Loss: 3135.1711\n",
      "Epoch: 255/3000 | Batch 0001/0011 | Loss: 3134.3948\n",
      "Epoch: 255/3000 | Batch 0002/0011 | Loss: 3072.4006\n",
      "Epoch: 255/3000 | Batch 0003/0011 | Loss: 3126.5791\n",
      "Epoch: 255/3000 | Batch 0004/0011 | Loss: 3158.4199\n",
      "Epoch: 255/3000 | Batch 0005/0011 | Loss: 3221.9023\n",
      "Epoch: 255/3000 | Batch 0006/0011 | Loss: 3134.5164\n",
      "Epoch: 255/3000 | Batch 0007/0011 | Loss: 3193.2173\n",
      "Epoch: 255/3000 | Batch 0008/0011 | Loss: 3269.1848\n",
      "Epoch: 255/3000 | Batch 0009/0011 | Loss: 3108.4590\n",
      "Epoch: 255/3000 | Batch 0010/0011 | Loss: 3200.3652\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 256/3000 | Batch 0000/0011 | Loss: 3087.1138\n",
      "Epoch: 256/3000 | Batch 0001/0011 | Loss: 3119.6455\n",
      "Epoch: 256/3000 | Batch 0002/0011 | Loss: 3119.7722\n",
      "Epoch: 256/3000 | Batch 0003/0011 | Loss: 3162.5569\n",
      "Epoch: 256/3000 | Batch 0004/0011 | Loss: 3204.4001\n",
      "Epoch: 256/3000 | Batch 0005/0011 | Loss: 3165.2551\n",
      "Epoch: 256/3000 | Batch 0006/0011 | Loss: 3128.7720\n",
      "Epoch: 256/3000 | Batch 0007/0011 | Loss: 3153.6870\n",
      "Epoch: 256/3000 | Batch 0008/0011 | Loss: 3237.8818\n",
      "Epoch: 256/3000 | Batch 0009/0011 | Loss: 3193.0347\n",
      "Epoch: 256/3000 | Batch 0010/0011 | Loss: 3105.3040\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 257/3000 | Batch 0000/0011 | Loss: 3125.1665\n",
      "Epoch: 257/3000 | Batch 0001/0011 | Loss: 3142.2107\n",
      "Epoch: 257/3000 | Batch 0002/0011 | Loss: 3146.9355\n",
      "Epoch: 257/3000 | Batch 0003/0011 | Loss: 3118.1460\n",
      "Epoch: 257/3000 | Batch 0004/0011 | Loss: 3189.4807\n",
      "Epoch: 257/3000 | Batch 0005/0011 | Loss: 3145.5054\n",
      "Epoch: 257/3000 | Batch 0006/0011 | Loss: 3231.3198\n",
      "Epoch: 257/3000 | Batch 0007/0011 | Loss: 3195.7400\n",
      "Epoch: 257/3000 | Batch 0008/0011 | Loss: 3128.8518\n",
      "Epoch: 257/3000 | Batch 0009/0011 | Loss: 3122.5352\n",
      "Epoch: 257/3000 | Batch 0010/0011 | Loss: 3134.0928\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 258/3000 | Batch 0000/0011 | Loss: 3273.9124\n",
      "Epoch: 258/3000 | Batch 0001/0011 | Loss: 3148.2341\n",
      "Epoch: 258/3000 | Batch 0002/0011 | Loss: 3146.3181\n",
      "Epoch: 258/3000 | Batch 0003/0011 | Loss: 3179.9597\n",
      "Epoch: 258/3000 | Batch 0004/0011 | Loss: 3124.8838\n",
      "Epoch: 258/3000 | Batch 0005/0011 | Loss: 3119.1855\n",
      "Epoch: 258/3000 | Batch 0006/0011 | Loss: 3225.4111\n",
      "Epoch: 258/3000 | Batch 0007/0011 | Loss: 3110.9009\n",
      "Epoch: 258/3000 | Batch 0008/0011 | Loss: 3159.2471\n",
      "Epoch: 258/3000 | Batch 0009/0011 | Loss: 3113.4966\n",
      "Epoch: 258/3000 | Batch 0010/0011 | Loss: 3075.4233\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 259/3000 | Batch 0000/0011 | Loss: 3238.6011\n",
      "Epoch: 259/3000 | Batch 0001/0011 | Loss: 3129.6948\n",
      "Epoch: 259/3000 | Batch 0002/0011 | Loss: 3144.5374\n",
      "Epoch: 259/3000 | Batch 0003/0011 | Loss: 3122.9800\n",
      "Epoch: 259/3000 | Batch 0004/0011 | Loss: 3214.9663\n",
      "Epoch: 259/3000 | Batch 0005/0011 | Loss: 3235.5352\n",
      "Epoch: 259/3000 | Batch 0006/0011 | Loss: 3217.8105\n",
      "Epoch: 259/3000 | Batch 0007/0011 | Loss: 3086.7051\n",
      "Epoch: 259/3000 | Batch 0008/0011 | Loss: 3105.4688\n",
      "Epoch: 259/3000 | Batch 0009/0011 | Loss: 3075.7610\n",
      "Epoch: 259/3000 | Batch 0010/0011 | Loss: 3168.4915\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 260/3000 | Batch 0000/0011 | Loss: 3098.0183\n",
      "Epoch: 260/3000 | Batch 0001/0011 | Loss: 3187.9614\n",
      "Epoch: 260/3000 | Batch 0002/0011 | Loss: 3083.2024\n",
      "Epoch: 260/3000 | Batch 0003/0011 | Loss: 3180.8877\n",
      "Epoch: 260/3000 | Batch 0004/0011 | Loss: 3214.9292\n",
      "Epoch: 260/3000 | Batch 0005/0011 | Loss: 3181.8320\n",
      "Epoch: 260/3000 | Batch 0006/0011 | Loss: 3118.9395\n",
      "Epoch: 260/3000 | Batch 0007/0011 | Loss: 3129.9434\n",
      "Epoch: 260/3000 | Batch 0008/0011 | Loss: 3193.3186\n",
      "Epoch: 260/3000 | Batch 0009/0011 | Loss: 3124.4287\n",
      "Epoch: 260/3000 | Batch 0010/0011 | Loss: 3386.5552\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 261/3000 | Batch 0000/0011 | Loss: 3091.8870\n",
      "Epoch: 261/3000 | Batch 0001/0011 | Loss: 3158.1411\n",
      "Epoch: 261/3000 | Batch 0002/0011 | Loss: 3120.4143\n",
      "Epoch: 261/3000 | Batch 0003/0011 | Loss: 3211.3220\n",
      "Epoch: 261/3000 | Batch 0004/0011 | Loss: 3165.8499\n",
      "Epoch: 261/3000 | Batch 0005/0011 | Loss: 3134.9116\n",
      "Epoch: 261/3000 | Batch 0006/0011 | Loss: 3194.1218\n",
      "Epoch: 261/3000 | Batch 0007/0011 | Loss: 3179.3950\n",
      "Epoch: 261/3000 | Batch 0008/0011 | Loss: 3131.4331\n",
      "Epoch: 261/3000 | Batch 0009/0011 | Loss: 3197.9658\n",
      "Epoch: 261/3000 | Batch 0010/0011 | Loss: 3308.8391\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 262/3000 | Batch 0000/0011 | Loss: 3110.4785\n",
      "Epoch: 262/3000 | Batch 0001/0011 | Loss: 3207.9885\n",
      "Epoch: 262/3000 | Batch 0002/0011 | Loss: 3212.0405\n",
      "Epoch: 262/3000 | Batch 0003/0011 | Loss: 3162.3706\n",
      "Epoch: 262/3000 | Batch 0004/0011 | Loss: 3129.6970\n",
      "Epoch: 262/3000 | Batch 0005/0011 | Loss: 3152.4453\n",
      "Epoch: 262/3000 | Batch 0006/0011 | Loss: 3102.7495\n",
      "Epoch: 262/3000 | Batch 0007/0011 | Loss: 3125.9631\n",
      "Epoch: 262/3000 | Batch 0008/0011 | Loss: 3162.6235\n",
      "Epoch: 262/3000 | Batch 0009/0011 | Loss: 3202.9399\n",
      "Epoch: 262/3000 | Batch 0010/0011 | Loss: 3155.3535\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 263/3000 | Batch 0000/0011 | Loss: 3120.4331\n",
      "Epoch: 263/3000 | Batch 0001/0011 | Loss: 3114.0383\n",
      "Epoch: 263/3000 | Batch 0002/0011 | Loss: 3133.4287\n",
      "Epoch: 263/3000 | Batch 0003/0011 | Loss: 3144.3787\n",
      "Epoch: 263/3000 | Batch 0004/0011 | Loss: 3219.1218\n",
      "Epoch: 263/3000 | Batch 0005/0011 | Loss: 3187.5547\n",
      "Epoch: 263/3000 | Batch 0006/0011 | Loss: 3207.5417\n",
      "Epoch: 263/3000 | Batch 0007/0011 | Loss: 3153.8906\n",
      "Epoch: 263/3000 | Batch 0008/0011 | Loss: 3234.6904\n",
      "Epoch: 263/3000 | Batch 0009/0011 | Loss: 3050.6555\n",
      "Epoch: 263/3000 | Batch 0010/0011 | Loss: 3202.6614\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 264/3000 | Batch 0000/0011 | Loss: 3153.3896\n",
      "Epoch: 264/3000 | Batch 0001/0011 | Loss: 3188.4124\n",
      "Epoch: 264/3000 | Batch 0002/0011 | Loss: 3128.7500\n",
      "Epoch: 264/3000 | Batch 0003/0011 | Loss: 3192.8562\n",
      "Epoch: 264/3000 | Batch 0004/0011 | Loss: 3142.7292\n",
      "Epoch: 264/3000 | Batch 0005/0011 | Loss: 3120.7864\n",
      "Epoch: 264/3000 | Batch 0006/0011 | Loss: 3116.6345\n",
      "Epoch: 264/3000 | Batch 0007/0011 | Loss: 3253.5095\n",
      "Epoch: 264/3000 | Batch 0008/0011 | Loss: 3154.9438\n",
      "Epoch: 264/3000 | Batch 0009/0011 | Loss: 3134.5542\n",
      "Epoch: 264/3000 | Batch 0010/0011 | Loss: 3085.4607\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 265/3000 | Batch 0000/0011 | Loss: 3203.1482\n",
      "Epoch: 265/3000 | Batch 0001/0011 | Loss: 3038.8926\n",
      "Epoch: 265/3000 | Batch 0002/0011 | Loss: 3100.0603\n",
      "Epoch: 265/3000 | Batch 0003/0011 | Loss: 3162.9126\n",
      "Epoch: 265/3000 | Batch 0004/0011 | Loss: 3188.8335\n",
      "Epoch: 265/3000 | Batch 0005/0011 | Loss: 3186.0078\n",
      "Epoch: 265/3000 | Batch 0006/0011 | Loss: 3131.6304\n",
      "Epoch: 265/3000 | Batch 0007/0011 | Loss: 3236.0247\n",
      "Epoch: 265/3000 | Batch 0008/0011 | Loss: 3182.5525\n",
      "Epoch: 265/3000 | Batch 0009/0011 | Loss: 3157.8711\n",
      "Epoch: 265/3000 | Batch 0010/0011 | Loss: 3128.9863\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 266/3000 | Batch 0000/0011 | Loss: 3098.5454\n",
      "Epoch: 266/3000 | Batch 0001/0011 | Loss: 3240.2300\n",
      "Epoch: 266/3000 | Batch 0002/0011 | Loss: 3139.2168\n",
      "Epoch: 266/3000 | Batch 0003/0011 | Loss: 3160.4434\n",
      "Epoch: 266/3000 | Batch 0004/0011 | Loss: 3099.6570\n",
      "Epoch: 266/3000 | Batch 0005/0011 | Loss: 3150.6433\n",
      "Epoch: 266/3000 | Batch 0006/0011 | Loss: 3175.2029\n",
      "Epoch: 266/3000 | Batch 0007/0011 | Loss: 3175.0181\n",
      "Epoch: 266/3000 | Batch 0008/0011 | Loss: 3160.5229\n",
      "Epoch: 266/3000 | Batch 0009/0011 | Loss: 3219.4907\n",
      "Epoch: 266/3000 | Batch 0010/0011 | Loss: 3004.0095\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 267/3000 | Batch 0000/0011 | Loss: 3126.9929\n",
      "Epoch: 267/3000 | Batch 0001/0011 | Loss: 3188.6787\n",
      "Epoch: 267/3000 | Batch 0002/0011 | Loss: 3123.9504\n",
      "Epoch: 267/3000 | Batch 0003/0011 | Loss: 3141.8750\n",
      "Epoch: 267/3000 | Batch 0004/0011 | Loss: 3222.5391\n",
      "Epoch: 267/3000 | Batch 0005/0011 | Loss: 3153.0300\n",
      "Epoch: 267/3000 | Batch 0006/0011 | Loss: 3108.6226\n",
      "Epoch: 267/3000 | Batch 0007/0011 | Loss: 3215.1418\n",
      "Epoch: 267/3000 | Batch 0008/0011 | Loss: 3135.8691\n",
      "Epoch: 267/3000 | Batch 0009/0011 | Loss: 3110.0481\n",
      "Epoch: 267/3000 | Batch 0010/0011 | Loss: 3211.2310\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 268/3000 | Batch 0000/0011 | Loss: 3169.7173\n",
      "Epoch: 268/3000 | Batch 0001/0011 | Loss: 3146.8096\n",
      "Epoch: 268/3000 | Batch 0002/0011 | Loss: 3102.8176\n",
      "Epoch: 268/3000 | Batch 0003/0011 | Loss: 3200.3689\n",
      "Epoch: 268/3000 | Batch 0004/0011 | Loss: 3084.9465\n",
      "Epoch: 268/3000 | Batch 0005/0011 | Loss: 3175.3286\n",
      "Epoch: 268/3000 | Batch 0006/0011 | Loss: 3154.3743\n",
      "Epoch: 268/3000 | Batch 0007/0011 | Loss: 3175.4336\n",
      "Epoch: 268/3000 | Batch 0008/0011 | Loss: 3180.3228\n",
      "Epoch: 268/3000 | Batch 0009/0011 | Loss: 3117.3098\n",
      "Epoch: 268/3000 | Batch 0010/0011 | Loss: 3254.4189\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 269/3000 | Batch 0000/0011 | Loss: 3186.2322\n",
      "Epoch: 269/3000 | Batch 0001/0011 | Loss: 3168.8528\n",
      "Epoch: 269/3000 | Batch 0002/0011 | Loss: 3115.4895\n",
      "Epoch: 269/3000 | Batch 0003/0011 | Loss: 3287.3811\n",
      "Epoch: 269/3000 | Batch 0004/0011 | Loss: 3086.6357\n",
      "Epoch: 269/3000 | Batch 0005/0011 | Loss: 3144.3572\n",
      "Epoch: 269/3000 | Batch 0006/0011 | Loss: 3135.9319\n",
      "Epoch: 269/3000 | Batch 0007/0011 | Loss: 3146.0381\n",
      "Epoch: 269/3000 | Batch 0008/0011 | Loss: 3185.5164\n",
      "Epoch: 269/3000 | Batch 0009/0011 | Loss: 3151.8511\n",
      "Epoch: 269/3000 | Batch 0010/0011 | Loss: 2970.9353\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 270/3000 | Batch 0000/0011 | Loss: 3253.6411\n",
      "Epoch: 270/3000 | Batch 0001/0011 | Loss: 3211.4302\n",
      "Epoch: 270/3000 | Batch 0002/0011 | Loss: 3088.6760\n",
      "Epoch: 270/3000 | Batch 0003/0011 | Loss: 3117.8015\n",
      "Epoch: 270/3000 | Batch 0004/0011 | Loss: 3097.5930\n",
      "Epoch: 270/3000 | Batch 0005/0011 | Loss: 3154.4846\n",
      "Epoch: 270/3000 | Batch 0006/0011 | Loss: 3248.9392\n",
      "Epoch: 270/3000 | Batch 0007/0011 | Loss: 3141.4094\n",
      "Epoch: 270/3000 | Batch 0008/0011 | Loss: 3134.1003\n",
      "Epoch: 270/3000 | Batch 0009/0011 | Loss: 3116.0999\n",
      "Epoch: 270/3000 | Batch 0010/0011 | Loss: 3153.7603\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 271/3000 | Batch 0000/0011 | Loss: 3151.6104\n",
      "Epoch: 271/3000 | Batch 0001/0011 | Loss: 3217.4377\n",
      "Epoch: 271/3000 | Batch 0002/0011 | Loss: 3100.9963\n",
      "Epoch: 271/3000 | Batch 0003/0011 | Loss: 3112.3418\n",
      "Epoch: 271/3000 | Batch 0004/0011 | Loss: 3141.0942\n",
      "Epoch: 271/3000 | Batch 0005/0011 | Loss: 3114.1440\n",
      "Epoch: 271/3000 | Batch 0006/0011 | Loss: 3201.1970\n",
      "Epoch: 271/3000 | Batch 0007/0011 | Loss: 3168.5771\n",
      "Epoch: 271/3000 | Batch 0008/0011 | Loss: 3122.4221\n",
      "Epoch: 271/3000 | Batch 0009/0011 | Loss: 3210.9023\n",
      "Epoch: 271/3000 | Batch 0010/0011 | Loss: 3091.8240\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 272/3000 | Batch 0000/0011 | Loss: 3228.4641\n",
      "Epoch: 272/3000 | Batch 0001/0011 | Loss: 3133.0339\n",
      "Epoch: 272/3000 | Batch 0002/0011 | Loss: 3198.1816\n",
      "Epoch: 272/3000 | Batch 0003/0011 | Loss: 3089.8789\n",
      "Epoch: 272/3000 | Batch 0004/0011 | Loss: 3054.7249\n",
      "Epoch: 272/3000 | Batch 0005/0011 | Loss: 3173.8159\n",
      "Epoch: 272/3000 | Batch 0006/0011 | Loss: 3189.9297\n",
      "Epoch: 272/3000 | Batch 0007/0011 | Loss: 3141.6506\n",
      "Epoch: 272/3000 | Batch 0008/0011 | Loss: 3221.4421\n",
      "Epoch: 272/3000 | Batch 0009/0011 | Loss: 3066.5193\n",
      "Epoch: 272/3000 | Batch 0010/0011 | Loss: 3170.4771\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 273/3000 | Batch 0000/0011 | Loss: 3146.7444\n",
      "Epoch: 273/3000 | Batch 0001/0011 | Loss: 3156.2563\n",
      "Epoch: 273/3000 | Batch 0002/0011 | Loss: 3081.3450\n",
      "Epoch: 273/3000 | Batch 0003/0011 | Loss: 3177.4060\n",
      "Epoch: 273/3000 | Batch 0004/0011 | Loss: 3089.8960\n",
      "Epoch: 273/3000 | Batch 0005/0011 | Loss: 3202.3020\n",
      "Epoch: 273/3000 | Batch 0006/0011 | Loss: 3154.4619\n",
      "Epoch: 273/3000 | Batch 0007/0011 | Loss: 3252.2410\n",
      "Epoch: 273/3000 | Batch 0008/0011 | Loss: 3148.4980\n",
      "Epoch: 273/3000 | Batch 0009/0011 | Loss: 3155.2031\n",
      "Epoch: 273/3000 | Batch 0010/0011 | Loss: 3221.7180\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 274/3000 | Batch 0000/0011 | Loss: 3135.0249\n",
      "Epoch: 274/3000 | Batch 0001/0011 | Loss: 3163.5537\n",
      "Epoch: 274/3000 | Batch 0002/0011 | Loss: 3113.4963\n",
      "Epoch: 274/3000 | Batch 0003/0011 | Loss: 3119.6890\n",
      "Epoch: 274/3000 | Batch 0004/0011 | Loss: 3153.1812\n",
      "Epoch: 274/3000 | Batch 0005/0011 | Loss: 3149.2080\n",
      "Epoch: 274/3000 | Batch 0006/0011 | Loss: 3137.0356\n",
      "Epoch: 274/3000 | Batch 0007/0011 | Loss: 3240.8757\n",
      "Epoch: 274/3000 | Batch 0008/0011 | Loss: 3232.6638\n",
      "Epoch: 274/3000 | Batch 0009/0011 | Loss: 3090.1409\n",
      "Epoch: 274/3000 | Batch 0010/0011 | Loss: 3161.2134\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 275/3000 | Batch 0000/0011 | Loss: 3123.8025\n",
      "Epoch: 275/3000 | Batch 0001/0011 | Loss: 3171.6729\n",
      "Epoch: 275/3000 | Batch 0002/0011 | Loss: 3216.2554\n",
      "Epoch: 275/3000 | Batch 0003/0011 | Loss: 3127.6833\n",
      "Epoch: 275/3000 | Batch 0004/0011 | Loss: 3149.5593\n",
      "Epoch: 275/3000 | Batch 0005/0011 | Loss: 3172.8931\n",
      "Epoch: 275/3000 | Batch 0006/0011 | Loss: 3146.6941\n",
      "Epoch: 275/3000 | Batch 0007/0011 | Loss: 3061.7520\n",
      "Epoch: 275/3000 | Batch 0008/0011 | Loss: 3191.3809\n",
      "Epoch: 275/3000 | Batch 0009/0011 | Loss: 3120.4929\n",
      "Epoch: 275/3000 | Batch 0010/0011 | Loss: 3497.0876\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 276/3000 | Batch 0000/0011 | Loss: 3158.4912\n",
      "Epoch: 276/3000 | Batch 0001/0011 | Loss: 3137.0088\n",
      "Epoch: 276/3000 | Batch 0002/0011 | Loss: 3162.6501\n",
      "Epoch: 276/3000 | Batch 0003/0011 | Loss: 3199.3967\n",
      "Epoch: 276/3000 | Batch 0004/0011 | Loss: 3106.0066\n",
      "Epoch: 276/3000 | Batch 0005/0011 | Loss: 3155.5898\n",
      "Epoch: 276/3000 | Batch 0006/0011 | Loss: 3198.6973\n",
      "Epoch: 276/3000 | Batch 0007/0011 | Loss: 3111.7751\n",
      "Epoch: 276/3000 | Batch 0008/0011 | Loss: 3129.2864\n",
      "Epoch: 276/3000 | Batch 0009/0011 | Loss: 3193.3367\n",
      "Epoch: 276/3000 | Batch 0010/0011 | Loss: 3108.0996\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 277/3000 | Batch 0000/0011 | Loss: 3116.2737\n",
      "Epoch: 277/3000 | Batch 0001/0011 | Loss: 3212.1392\n",
      "Epoch: 277/3000 | Batch 0002/0011 | Loss: 3160.7502\n",
      "Epoch: 277/3000 | Batch 0003/0011 | Loss: 3239.8088\n",
      "Epoch: 277/3000 | Batch 0004/0011 | Loss: 3192.0374\n",
      "Epoch: 277/3000 | Batch 0005/0011 | Loss: 3133.3472\n",
      "Epoch: 277/3000 | Batch 0006/0011 | Loss: 3146.4395\n",
      "Epoch: 277/3000 | Batch 0007/0011 | Loss: 3179.7971\n",
      "Epoch: 277/3000 | Batch 0008/0011 | Loss: 3089.3110\n",
      "Epoch: 277/3000 | Batch 0009/0011 | Loss: 3119.9768\n",
      "Epoch: 277/3000 | Batch 0010/0011 | Loss: 3149.1594\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 278/3000 | Batch 0000/0011 | Loss: 3149.1282\n",
      "Epoch: 278/3000 | Batch 0001/0011 | Loss: 3123.2119\n",
      "Epoch: 278/3000 | Batch 0002/0011 | Loss: 3242.1248\n",
      "Epoch: 278/3000 | Batch 0003/0011 | Loss: 3110.1003\n",
      "Epoch: 278/3000 | Batch 0004/0011 | Loss: 3182.0906\n",
      "Epoch: 278/3000 | Batch 0005/0011 | Loss: 3150.3594\n",
      "Epoch: 278/3000 | Batch 0006/0011 | Loss: 3155.6912\n",
      "Epoch: 278/3000 | Batch 0007/0011 | Loss: 3087.1333\n",
      "Epoch: 278/3000 | Batch 0008/0011 | Loss: 3162.1272\n",
      "Epoch: 278/3000 | Batch 0009/0011 | Loss: 3201.7148\n",
      "Epoch: 278/3000 | Batch 0010/0011 | Loss: 3093.6023\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 279/3000 | Batch 0000/0011 | Loss: 3121.1458\n",
      "Epoch: 279/3000 | Batch 0001/0011 | Loss: 3151.3650\n",
      "Epoch: 279/3000 | Batch 0002/0011 | Loss: 3118.9688\n",
      "Epoch: 279/3000 | Batch 0003/0011 | Loss: 3149.5684\n",
      "Epoch: 279/3000 | Batch 0004/0011 | Loss: 3176.9600\n",
      "Epoch: 279/3000 | Batch 0005/0011 | Loss: 3134.3459\n",
      "Epoch: 279/3000 | Batch 0006/0011 | Loss: 3161.0974\n",
      "Epoch: 279/3000 | Batch 0007/0011 | Loss: 3134.3442\n",
      "Epoch: 279/3000 | Batch 0008/0011 | Loss: 3206.4478\n",
      "Epoch: 279/3000 | Batch 0009/0011 | Loss: 3174.3713\n",
      "Epoch: 279/3000 | Batch 0010/0011 | Loss: 3026.5435\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 280/3000 | Batch 0000/0011 | Loss: 3191.8440\n",
      "Epoch: 280/3000 | Batch 0001/0011 | Loss: 3150.7256\n",
      "Epoch: 280/3000 | Batch 0002/0011 | Loss: 3084.9546\n",
      "Epoch: 280/3000 | Batch 0003/0011 | Loss: 3095.2473\n",
      "Epoch: 280/3000 | Batch 0004/0011 | Loss: 3129.8430\n",
      "Epoch: 280/3000 | Batch 0005/0011 | Loss: 3123.7925\n",
      "Epoch: 280/3000 | Batch 0006/0011 | Loss: 3175.0713\n",
      "Epoch: 280/3000 | Batch 0007/0011 | Loss: 3257.2332\n",
      "Epoch: 280/3000 | Batch 0008/0011 | Loss: 3172.9180\n",
      "Epoch: 280/3000 | Batch 0009/0011 | Loss: 3120.0781\n",
      "Epoch: 280/3000 | Batch 0010/0011 | Loss: 3253.2559\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 281/3000 | Batch 0000/0011 | Loss: 3058.3445\n",
      "Epoch: 281/3000 | Batch 0001/0011 | Loss: 3168.8669\n",
      "Epoch: 281/3000 | Batch 0002/0011 | Loss: 3142.8074\n",
      "Epoch: 281/3000 | Batch 0003/0011 | Loss: 3134.4951\n",
      "Epoch: 281/3000 | Batch 0004/0011 | Loss: 3222.6267\n",
      "Epoch: 281/3000 | Batch 0005/0011 | Loss: 3156.8643\n",
      "Epoch: 281/3000 | Batch 0006/0011 | Loss: 3153.8669\n",
      "Epoch: 281/3000 | Batch 0007/0011 | Loss: 3173.9255\n",
      "Epoch: 281/3000 | Batch 0008/0011 | Loss: 3161.3442\n",
      "Epoch: 281/3000 | Batch 0009/0011 | Loss: 3136.8157\n",
      "Epoch: 281/3000 | Batch 0010/0011 | Loss: 3115.3049\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 282/3000 | Batch 0000/0011 | Loss: 3062.7207\n",
      "Epoch: 282/3000 | Batch 0001/0011 | Loss: 3194.4382\n",
      "Epoch: 282/3000 | Batch 0002/0011 | Loss: 3145.7939\n",
      "Epoch: 282/3000 | Batch 0003/0011 | Loss: 3174.1106\n",
      "Epoch: 282/3000 | Batch 0004/0011 | Loss: 3151.3171\n",
      "Epoch: 282/3000 | Batch 0005/0011 | Loss: 3174.0664\n",
      "Epoch: 282/3000 | Batch 0006/0011 | Loss: 3182.1887\n",
      "Epoch: 282/3000 | Batch 0007/0011 | Loss: 3150.5508\n",
      "Epoch: 282/3000 | Batch 0008/0011 | Loss: 3133.5625\n",
      "Epoch: 282/3000 | Batch 0009/0011 | Loss: 3162.4773\n",
      "Epoch: 282/3000 | Batch 0010/0011 | Loss: 3235.5007\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 283/3000 | Batch 0000/0011 | Loss: 3132.3367\n",
      "Epoch: 283/3000 | Batch 0001/0011 | Loss: 3099.1863\n",
      "Epoch: 283/3000 | Batch 0002/0011 | Loss: 3150.7271\n",
      "Epoch: 283/3000 | Batch 0003/0011 | Loss: 3180.1743\n",
      "Epoch: 283/3000 | Batch 0004/0011 | Loss: 3189.8362\n",
      "Epoch: 283/3000 | Batch 0005/0011 | Loss: 3092.4238\n",
      "Epoch: 283/3000 | Batch 0006/0011 | Loss: 3205.1787\n",
      "Epoch: 283/3000 | Batch 0007/0011 | Loss: 3110.8179\n",
      "Epoch: 283/3000 | Batch 0008/0011 | Loss: 3202.2302\n",
      "Epoch: 283/3000 | Batch 0009/0011 | Loss: 3184.7954\n",
      "Epoch: 283/3000 | Batch 0010/0011 | Loss: 3180.0928\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 284/3000 | Batch 0000/0011 | Loss: 3129.9900\n",
      "Epoch: 284/3000 | Batch 0001/0011 | Loss: 3177.4280\n",
      "Epoch: 284/3000 | Batch 0002/0011 | Loss: 3121.0466\n",
      "Epoch: 284/3000 | Batch 0003/0011 | Loss: 3173.3108\n",
      "Epoch: 284/3000 | Batch 0004/0011 | Loss: 3129.7251\n",
      "Epoch: 284/3000 | Batch 0005/0011 | Loss: 3164.9766\n",
      "Epoch: 284/3000 | Batch 0006/0011 | Loss: 3099.9568\n",
      "Epoch: 284/3000 | Batch 0007/0011 | Loss: 3187.0591\n",
      "Epoch: 284/3000 | Batch 0008/0011 | Loss: 3247.1975\n",
      "Epoch: 284/3000 | Batch 0009/0011 | Loss: 3137.0857\n",
      "Epoch: 284/3000 | Batch 0010/0011 | Loss: 3129.8862\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 285/3000 | Batch 0000/0011 | Loss: 3111.5139\n",
      "Epoch: 285/3000 | Batch 0001/0011 | Loss: 3175.9746\n",
      "Epoch: 285/3000 | Batch 0002/0011 | Loss: 3106.3909\n",
      "Epoch: 285/3000 | Batch 0003/0011 | Loss: 3252.5176\n",
      "Epoch: 285/3000 | Batch 0004/0011 | Loss: 3157.2351\n",
      "Epoch: 285/3000 | Batch 0005/0011 | Loss: 3154.7888\n",
      "Epoch: 285/3000 | Batch 0006/0011 | Loss: 3106.2349\n",
      "Epoch: 285/3000 | Batch 0007/0011 | Loss: 3138.7505\n",
      "Epoch: 285/3000 | Batch 0008/0011 | Loss: 3138.6233\n",
      "Epoch: 285/3000 | Batch 0009/0011 | Loss: 3181.7251\n",
      "Epoch: 285/3000 | Batch 0010/0011 | Loss: 3046.9307\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 286/3000 | Batch 0000/0011 | Loss: 3133.7798\n",
      "Epoch: 286/3000 | Batch 0001/0011 | Loss: 3133.8237\n",
      "Epoch: 286/3000 | Batch 0002/0011 | Loss: 3105.4265\n",
      "Epoch: 286/3000 | Batch 0003/0011 | Loss: 3187.0374\n",
      "Epoch: 286/3000 | Batch 0004/0011 | Loss: 3104.7424\n",
      "Epoch: 286/3000 | Batch 0005/0011 | Loss: 3177.8364\n",
      "Epoch: 286/3000 | Batch 0006/0011 | Loss: 3133.9937\n",
      "Epoch: 286/3000 | Batch 0007/0011 | Loss: 3132.4089\n",
      "Epoch: 286/3000 | Batch 0008/0011 | Loss: 3154.3279\n",
      "Epoch: 286/3000 | Batch 0009/0011 | Loss: 3253.9260\n",
      "Epoch: 286/3000 | Batch 0010/0011 | Loss: 3125.0010\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 287/3000 | Batch 0000/0011 | Loss: 3180.0156\n",
      "Epoch: 287/3000 | Batch 0001/0011 | Loss: 3153.3411\n",
      "Epoch: 287/3000 | Batch 0002/0011 | Loss: 3183.6631\n",
      "Epoch: 287/3000 | Batch 0003/0011 | Loss: 3102.6094\n",
      "Epoch: 287/3000 | Batch 0004/0011 | Loss: 3170.8132\n",
      "Epoch: 287/3000 | Batch 0005/0011 | Loss: 3158.7139\n",
      "Epoch: 287/3000 | Batch 0006/0011 | Loss: 3166.7156\n",
      "Epoch: 287/3000 | Batch 0007/0011 | Loss: 3138.7886\n",
      "Epoch: 287/3000 | Batch 0008/0011 | Loss: 3123.9961\n",
      "Epoch: 287/3000 | Batch 0009/0011 | Loss: 3116.5991\n",
      "Epoch: 287/3000 | Batch 0010/0011 | Loss: 3246.1731\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 288/3000 | Batch 0000/0011 | Loss: 3157.1626\n",
      "Epoch: 288/3000 | Batch 0001/0011 | Loss: 3172.7302\n",
      "Epoch: 288/3000 | Batch 0002/0011 | Loss: 3129.3809\n",
      "Epoch: 288/3000 | Batch 0003/0011 | Loss: 3088.8459\n",
      "Epoch: 288/3000 | Batch 0004/0011 | Loss: 3185.2310\n",
      "Epoch: 288/3000 | Batch 0005/0011 | Loss: 3195.1584\n",
      "Epoch: 288/3000 | Batch 0006/0011 | Loss: 3138.2615\n",
      "Epoch: 288/3000 | Batch 0007/0011 | Loss: 3164.1296\n",
      "Epoch: 288/3000 | Batch 0008/0011 | Loss: 3168.0793\n",
      "Epoch: 288/3000 | Batch 0009/0011 | Loss: 3129.1621\n",
      "Epoch: 288/3000 | Batch 0010/0011 | Loss: 3039.7881\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 289/3000 | Batch 0000/0011 | Loss: 3108.3523\n",
      "Epoch: 289/3000 | Batch 0001/0011 | Loss: 3136.9304\n",
      "Epoch: 289/3000 | Batch 0002/0011 | Loss: 3214.0461\n",
      "Epoch: 289/3000 | Batch 0003/0011 | Loss: 3120.5542\n",
      "Epoch: 289/3000 | Batch 0004/0011 | Loss: 3144.5178\n",
      "Epoch: 289/3000 | Batch 0005/0011 | Loss: 3124.2222\n",
      "Epoch: 289/3000 | Batch 0006/0011 | Loss: 3205.0874\n",
      "Epoch: 289/3000 | Batch 0007/0011 | Loss: 3174.9766\n",
      "Epoch: 289/3000 | Batch 0008/0011 | Loss: 3209.3723\n",
      "Epoch: 289/3000 | Batch 0009/0011 | Loss: 3085.0212\n",
      "Epoch: 289/3000 | Batch 0010/0011 | Loss: 3022.5601\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 290/3000 | Batch 0000/0011 | Loss: 3197.3196\n",
      "Epoch: 290/3000 | Batch 0001/0011 | Loss: 3197.6755\n",
      "Epoch: 290/3000 | Batch 0002/0011 | Loss: 3107.6750\n",
      "Epoch: 290/3000 | Batch 0003/0011 | Loss: 3120.2969\n",
      "Epoch: 290/3000 | Batch 0004/0011 | Loss: 3122.8416\n",
      "Epoch: 290/3000 | Batch 0005/0011 | Loss: 3115.3340\n",
      "Epoch: 290/3000 | Batch 0006/0011 | Loss: 3167.8367\n",
      "Epoch: 290/3000 | Batch 0007/0011 | Loss: 3145.0271\n",
      "Epoch: 290/3000 | Batch 0008/0011 | Loss: 3170.8521\n",
      "Epoch: 290/3000 | Batch 0009/0011 | Loss: 3178.8037\n",
      "Epoch: 290/3000 | Batch 0010/0011 | Loss: 2982.5244\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 291/3000 | Batch 0000/0011 | Loss: 3158.2622\n",
      "Epoch: 291/3000 | Batch 0001/0011 | Loss: 3146.2637\n",
      "Epoch: 291/3000 | Batch 0002/0011 | Loss: 3129.1360\n",
      "Epoch: 291/3000 | Batch 0003/0011 | Loss: 3112.4883\n",
      "Epoch: 291/3000 | Batch 0004/0011 | Loss: 3136.7126\n",
      "Epoch: 291/3000 | Batch 0005/0011 | Loss: 3110.2122\n",
      "Epoch: 291/3000 | Batch 0006/0011 | Loss: 3127.2322\n",
      "Epoch: 291/3000 | Batch 0007/0011 | Loss: 3228.1580\n",
      "Epoch: 291/3000 | Batch 0008/0011 | Loss: 3190.3628\n",
      "Epoch: 291/3000 | Batch 0009/0011 | Loss: 3227.7380\n",
      "Epoch: 291/3000 | Batch 0010/0011 | Loss: 3177.3030\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 292/3000 | Batch 0000/0011 | Loss: 3208.0830\n",
      "Epoch: 292/3000 | Batch 0001/0011 | Loss: 3125.2825\n",
      "Epoch: 292/3000 | Batch 0002/0011 | Loss: 3182.0830\n",
      "Epoch: 292/3000 | Batch 0003/0011 | Loss: 3122.2505\n",
      "Epoch: 292/3000 | Batch 0004/0011 | Loss: 3201.7058\n",
      "Epoch: 292/3000 | Batch 0005/0011 | Loss: 3120.8633\n",
      "Epoch: 292/3000 | Batch 0006/0011 | Loss: 3183.2930\n",
      "Epoch: 292/3000 | Batch 0007/0011 | Loss: 3187.0911\n",
      "Epoch: 292/3000 | Batch 0008/0011 | Loss: 3131.8594\n",
      "Epoch: 292/3000 | Batch 0009/0011 | Loss: 3171.7458\n",
      "Epoch: 292/3000 | Batch 0010/0011 | Loss: 3108.3721\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 293/3000 | Batch 0000/0011 | Loss: 3179.2363\n",
      "Epoch: 293/3000 | Batch 0001/0011 | Loss: 3158.8262\n",
      "Epoch: 293/3000 | Batch 0002/0011 | Loss: 3166.5227\n",
      "Epoch: 293/3000 | Batch 0003/0011 | Loss: 3185.3948\n",
      "Epoch: 293/3000 | Batch 0004/0011 | Loss: 3108.3552\n",
      "Epoch: 293/3000 | Batch 0005/0011 | Loss: 3163.2393\n",
      "Epoch: 293/3000 | Batch 0006/0011 | Loss: 3111.7629\n",
      "Epoch: 293/3000 | Batch 0007/0011 | Loss: 3124.8481\n",
      "Epoch: 293/3000 | Batch 0008/0011 | Loss: 3159.7991\n",
      "Epoch: 293/3000 | Batch 0009/0011 | Loss: 3195.0259\n",
      "Epoch: 293/3000 | Batch 0010/0011 | Loss: 3056.1985\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 294/3000 | Batch 0000/0011 | Loss: 3112.7344\n",
      "Epoch: 294/3000 | Batch 0001/0011 | Loss: 3179.1814\n",
      "Epoch: 294/3000 | Batch 0002/0011 | Loss: 3133.6826\n",
      "Epoch: 294/3000 | Batch 0003/0011 | Loss: 3230.3003\n",
      "Epoch: 294/3000 | Batch 0004/0011 | Loss: 3088.2126\n",
      "Epoch: 294/3000 | Batch 0005/0011 | Loss: 3138.5989\n",
      "Epoch: 294/3000 | Batch 0006/0011 | Loss: 3135.3752\n",
      "Epoch: 294/3000 | Batch 0007/0011 | Loss: 3123.0259\n",
      "Epoch: 294/3000 | Batch 0008/0011 | Loss: 3196.0044\n",
      "Epoch: 294/3000 | Batch 0009/0011 | Loss: 3168.5654\n",
      "Epoch: 294/3000 | Batch 0010/0011 | Loss: 3300.2703\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 295/3000 | Batch 0000/0011 | Loss: 3163.4546\n",
      "Epoch: 295/3000 | Batch 0001/0011 | Loss: 3119.8230\n",
      "Epoch: 295/3000 | Batch 0002/0011 | Loss: 3119.7280\n",
      "Epoch: 295/3000 | Batch 0003/0011 | Loss: 3079.5352\n",
      "Epoch: 295/3000 | Batch 0004/0011 | Loss: 3233.0437\n",
      "Epoch: 295/3000 | Batch 0005/0011 | Loss: 3209.9207\n",
      "Epoch: 295/3000 | Batch 0006/0011 | Loss: 3309.2051\n",
      "Epoch: 295/3000 | Batch 0007/0011 | Loss: 3078.5310\n",
      "Epoch: 295/3000 | Batch 0008/0011 | Loss: 3110.2910\n",
      "Epoch: 295/3000 | Batch 0009/0011 | Loss: 3154.1187\n",
      "Epoch: 295/3000 | Batch 0010/0011 | Loss: 3039.6499\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 296/3000 | Batch 0000/0011 | Loss: 3196.6501\n",
      "Epoch: 296/3000 | Batch 0001/0011 | Loss: 3173.7974\n",
      "Epoch: 296/3000 | Batch 0002/0011 | Loss: 3196.1885\n",
      "Epoch: 296/3000 | Batch 0003/0011 | Loss: 3103.9692\n",
      "Epoch: 296/3000 | Batch 0004/0011 | Loss: 3028.6035\n",
      "Epoch: 296/3000 | Batch 0005/0011 | Loss: 3161.8743\n",
      "Epoch: 296/3000 | Batch 0006/0011 | Loss: 3114.2466\n",
      "Epoch: 296/3000 | Batch 0007/0011 | Loss: 3144.2957\n",
      "Epoch: 296/3000 | Batch 0008/0011 | Loss: 3240.3188\n",
      "Epoch: 296/3000 | Batch 0009/0011 | Loss: 3117.9111\n",
      "Epoch: 296/3000 | Batch 0010/0011 | Loss: 3186.9600\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 297/3000 | Batch 0000/0011 | Loss: 3122.2305\n",
      "Epoch: 297/3000 | Batch 0001/0011 | Loss: 3091.0693\n",
      "Epoch: 297/3000 | Batch 0002/0011 | Loss: 3217.9456\n",
      "Epoch: 297/3000 | Batch 0003/0011 | Loss: 3096.6963\n",
      "Epoch: 297/3000 | Batch 0004/0011 | Loss: 3158.6372\n",
      "Epoch: 297/3000 | Batch 0005/0011 | Loss: 3263.4294\n",
      "Epoch: 297/3000 | Batch 0006/0011 | Loss: 3097.0479\n",
      "Epoch: 297/3000 | Batch 0007/0011 | Loss: 3115.8657\n",
      "Epoch: 297/3000 | Batch 0008/0011 | Loss: 3233.6741\n",
      "Epoch: 297/3000 | Batch 0009/0011 | Loss: 3148.1255\n",
      "Epoch: 297/3000 | Batch 0010/0011 | Loss: 3220.2170\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 298/3000 | Batch 0000/0011 | Loss: 3156.2153\n",
      "Epoch: 298/3000 | Batch 0001/0011 | Loss: 3160.1084\n",
      "Epoch: 298/3000 | Batch 0002/0011 | Loss: 3157.1665\n",
      "Epoch: 298/3000 | Batch 0003/0011 | Loss: 3080.1411\n",
      "Epoch: 298/3000 | Batch 0004/0011 | Loss: 3135.0630\n",
      "Epoch: 298/3000 | Batch 0005/0011 | Loss: 3163.6506\n",
      "Epoch: 298/3000 | Batch 0006/0011 | Loss: 3172.3003\n",
      "Epoch: 298/3000 | Batch 0007/0011 | Loss: 3166.8271\n",
      "Epoch: 298/3000 | Batch 0008/0011 | Loss: 3148.6501\n",
      "Epoch: 298/3000 | Batch 0009/0011 | Loss: 3148.5085\n",
      "Epoch: 298/3000 | Batch 0010/0011 | Loss: 3201.4197\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 299/3000 | Batch 0000/0011 | Loss: 3239.8088\n",
      "Epoch: 299/3000 | Batch 0001/0011 | Loss: 3084.2390\n",
      "Epoch: 299/3000 | Batch 0002/0011 | Loss: 3162.1172\n",
      "Epoch: 299/3000 | Batch 0003/0011 | Loss: 3229.7339\n",
      "Epoch: 299/3000 | Batch 0004/0011 | Loss: 3115.8442\n",
      "Epoch: 299/3000 | Batch 0005/0011 | Loss: 3200.4204\n",
      "Epoch: 299/3000 | Batch 0006/0011 | Loss: 3122.5007\n",
      "Epoch: 299/3000 | Batch 0007/0011 | Loss: 3145.1414\n",
      "Epoch: 299/3000 | Batch 0008/0011 | Loss: 3117.1162\n",
      "Epoch: 299/3000 | Batch 0009/0011 | Loss: 3132.4448\n",
      "Epoch: 299/3000 | Batch 0010/0011 | Loss: 3054.0071\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 300/3000 | Batch 0000/0011 | Loss: 3120.0759\n",
      "Epoch: 300/3000 | Batch 0001/0011 | Loss: 3161.3347\n",
      "Epoch: 300/3000 | Batch 0002/0011 | Loss: 3209.9016\n",
      "Epoch: 300/3000 | Batch 0003/0011 | Loss: 3118.6909\n",
      "Epoch: 300/3000 | Batch 0004/0011 | Loss: 3093.0601\n",
      "Epoch: 300/3000 | Batch 0005/0011 | Loss: 3175.9951\n",
      "Epoch: 300/3000 | Batch 0006/0011 | Loss: 3152.1663\n",
      "Epoch: 300/3000 | Batch 0007/0011 | Loss: 3242.7915\n",
      "Epoch: 300/3000 | Batch 0008/0011 | Loss: 3149.9963\n",
      "Epoch: 300/3000 | Batch 0009/0011 | Loss: 3089.7874\n",
      "Epoch: 300/3000 | Batch 0010/0011 | Loss: 2973.5571\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 301/3000 | Batch 0000/0011 | Loss: 3083.3247\n",
      "Epoch: 301/3000 | Batch 0001/0011 | Loss: 3138.6274\n",
      "Epoch: 301/3000 | Batch 0002/0011 | Loss: 3148.0669\n",
      "Epoch: 301/3000 | Batch 0003/0011 | Loss: 3173.0361\n",
      "Epoch: 301/3000 | Batch 0004/0011 | Loss: 3264.2620\n",
      "Epoch: 301/3000 | Batch 0005/0011 | Loss: 3125.9087\n",
      "Epoch: 301/3000 | Batch 0006/0011 | Loss: 3074.5376\n",
      "Epoch: 301/3000 | Batch 0007/0011 | Loss: 3135.2239\n",
      "Epoch: 301/3000 | Batch 0008/0011 | Loss: 3163.2617\n",
      "Epoch: 301/3000 | Batch 0009/0011 | Loss: 3179.7715\n",
      "Epoch: 301/3000 | Batch 0010/0011 | Loss: 3327.5457\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 302/3000 | Batch 0000/0011 | Loss: 3137.4519\n",
      "Epoch: 302/3000 | Batch 0001/0011 | Loss: 3163.7571\n",
      "Epoch: 302/3000 | Batch 0002/0011 | Loss: 3123.0972\n",
      "Epoch: 302/3000 | Batch 0003/0011 | Loss: 3066.2148\n",
      "Epoch: 302/3000 | Batch 0004/0011 | Loss: 3127.3972\n",
      "Epoch: 302/3000 | Batch 0005/0011 | Loss: 3136.0835\n",
      "Epoch: 302/3000 | Batch 0006/0011 | Loss: 3172.1489\n",
      "Epoch: 302/3000 | Batch 0007/0011 | Loss: 3222.3958\n",
      "Epoch: 302/3000 | Batch 0008/0011 | Loss: 3178.5090\n",
      "Epoch: 302/3000 | Batch 0009/0011 | Loss: 3148.6731\n",
      "Epoch: 302/3000 | Batch 0010/0011 | Loss: 3126.9653\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 303/3000 | Batch 0000/0011 | Loss: 3162.9883\n",
      "Epoch: 303/3000 | Batch 0001/0011 | Loss: 3155.2112\n",
      "Epoch: 303/3000 | Batch 0002/0011 | Loss: 3109.5308\n",
      "Epoch: 303/3000 | Batch 0003/0011 | Loss: 3155.4104\n",
      "Epoch: 303/3000 | Batch 0004/0011 | Loss: 3157.9468\n",
      "Epoch: 303/3000 | Batch 0005/0011 | Loss: 3140.5591\n",
      "Epoch: 303/3000 | Batch 0006/0011 | Loss: 3213.8169\n",
      "Epoch: 303/3000 | Batch 0007/0011 | Loss: 3093.2441\n",
      "Epoch: 303/3000 | Batch 0008/0011 | Loss: 3081.9905\n",
      "Epoch: 303/3000 | Batch 0009/0011 | Loss: 3210.4446\n",
      "Epoch: 303/3000 | Batch 0010/0011 | Loss: 3418.8423\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 304/3000 | Batch 0000/0011 | Loss: 3122.6892\n",
      "Epoch: 304/3000 | Batch 0001/0011 | Loss: 3119.1614\n",
      "Epoch: 304/3000 | Batch 0002/0011 | Loss: 3145.3049\n",
      "Epoch: 304/3000 | Batch 0003/0011 | Loss: 3106.6562\n",
      "Epoch: 304/3000 | Batch 0004/0011 | Loss: 3124.6890\n",
      "Epoch: 304/3000 | Batch 0005/0011 | Loss: 3170.5837\n",
      "Epoch: 304/3000 | Batch 0006/0011 | Loss: 3167.9675\n",
      "Epoch: 304/3000 | Batch 0007/0011 | Loss: 3165.2554\n",
      "Epoch: 304/3000 | Batch 0008/0011 | Loss: 3131.5085\n",
      "Epoch: 304/3000 | Batch 0009/0011 | Loss: 3211.4189\n",
      "Epoch: 304/3000 | Batch 0010/0011 | Loss: 3102.3765\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 305/3000 | Batch 0000/0011 | Loss: 3149.0798\n",
      "Epoch: 305/3000 | Batch 0001/0011 | Loss: 3123.7354\n",
      "Epoch: 305/3000 | Batch 0002/0011 | Loss: 3132.4575\n",
      "Epoch: 305/3000 | Batch 0003/0011 | Loss: 3128.9263\n",
      "Epoch: 305/3000 | Batch 0004/0011 | Loss: 3056.0474\n",
      "Epoch: 305/3000 | Batch 0005/0011 | Loss: 3114.2583\n",
      "Epoch: 305/3000 | Batch 0006/0011 | Loss: 3196.0273\n",
      "Epoch: 305/3000 | Batch 0007/0011 | Loss: 3333.9370\n",
      "Epoch: 305/3000 | Batch 0008/0011 | Loss: 3171.8796\n",
      "Epoch: 305/3000 | Batch 0009/0011 | Loss: 3134.6523\n",
      "Epoch: 305/3000 | Batch 0010/0011 | Loss: 2969.9846\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 306/3000 | Batch 0000/0011 | Loss: 3188.6384\n",
      "Epoch: 306/3000 | Batch 0001/0011 | Loss: 3254.2583\n",
      "Epoch: 306/3000 | Batch 0002/0011 | Loss: 3106.6233\n",
      "Epoch: 306/3000 | Batch 0003/0011 | Loss: 3105.9651\n",
      "Epoch: 306/3000 | Batch 0004/0011 | Loss: 3130.8884\n",
      "Epoch: 306/3000 | Batch 0005/0011 | Loss: 3205.4731\n",
      "Epoch: 306/3000 | Batch 0006/0011 | Loss: 3046.9690\n",
      "Epoch: 306/3000 | Batch 0007/0011 | Loss: 3067.0271\n",
      "Epoch: 306/3000 | Batch 0008/0011 | Loss: 3191.0752\n",
      "Epoch: 306/3000 | Batch 0009/0011 | Loss: 3205.6333\n",
      "Epoch: 306/3000 | Batch 0010/0011 | Loss: 3104.9895\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 307/3000 | Batch 0000/0011 | Loss: 3163.4165\n",
      "Epoch: 307/3000 | Batch 0001/0011 | Loss: 3082.9207\n",
      "Epoch: 307/3000 | Batch 0002/0011 | Loss: 3200.6033\n",
      "Epoch: 307/3000 | Batch 0003/0011 | Loss: 3122.9160\n",
      "Epoch: 307/3000 | Batch 0004/0011 | Loss: 3102.0681\n",
      "Epoch: 307/3000 | Batch 0005/0011 | Loss: 3184.5732\n",
      "Epoch: 307/3000 | Batch 0006/0011 | Loss: 3167.4014\n",
      "Epoch: 307/3000 | Batch 0007/0011 | Loss: 3144.1196\n",
      "Epoch: 307/3000 | Batch 0008/0011 | Loss: 3131.4204\n",
      "Epoch: 307/3000 | Batch 0009/0011 | Loss: 3140.9893\n",
      "Epoch: 307/3000 | Batch 0010/0011 | Loss: 3251.1948\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 308/3000 | Batch 0000/0011 | Loss: 3122.3025\n",
      "Epoch: 308/3000 | Batch 0001/0011 | Loss: 3192.4321\n",
      "Epoch: 308/3000 | Batch 0002/0011 | Loss: 3128.0286\n",
      "Epoch: 308/3000 | Batch 0003/0011 | Loss: 3154.4016\n",
      "Epoch: 308/3000 | Batch 0004/0011 | Loss: 3109.7512\n",
      "Epoch: 308/3000 | Batch 0005/0011 | Loss: 3150.8445\n",
      "Epoch: 308/3000 | Batch 0006/0011 | Loss: 3150.8801\n",
      "Epoch: 308/3000 | Batch 0007/0011 | Loss: 3161.5527\n",
      "Epoch: 308/3000 | Batch 0008/0011 | Loss: 3124.5767\n",
      "Epoch: 308/3000 | Batch 0009/0011 | Loss: 3164.3098\n",
      "Epoch: 308/3000 | Batch 0010/0011 | Loss: 3147.6216\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 309/3000 | Batch 0000/0011 | Loss: 3154.3677\n",
      "Epoch: 309/3000 | Batch 0001/0011 | Loss: 3262.3965\n",
      "Epoch: 309/3000 | Batch 0002/0011 | Loss: 3175.4771\n",
      "Epoch: 309/3000 | Batch 0003/0011 | Loss: 3073.4475\n",
      "Epoch: 309/3000 | Batch 0004/0011 | Loss: 3074.7412\n",
      "Epoch: 309/3000 | Batch 0005/0011 | Loss: 3177.8975\n",
      "Epoch: 309/3000 | Batch 0006/0011 | Loss: 3158.8813\n",
      "Epoch: 309/3000 | Batch 0007/0011 | Loss: 3130.8655\n",
      "Epoch: 309/3000 | Batch 0008/0011 | Loss: 3122.9250\n",
      "Epoch: 309/3000 | Batch 0009/0011 | Loss: 3145.9011\n",
      "Epoch: 309/3000 | Batch 0010/0011 | Loss: 3087.9016\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 310/3000 | Batch 0000/0011 | Loss: 3178.6350\n",
      "Epoch: 310/3000 | Batch 0001/0011 | Loss: 3219.2678\n",
      "Epoch: 310/3000 | Batch 0002/0011 | Loss: 3164.3364\n",
      "Epoch: 310/3000 | Batch 0003/0011 | Loss: 3164.1741\n",
      "Epoch: 310/3000 | Batch 0004/0011 | Loss: 3093.0972\n",
      "Epoch: 310/3000 | Batch 0005/0011 | Loss: 3090.3943\n",
      "Epoch: 310/3000 | Batch 0006/0011 | Loss: 3192.9688\n",
      "Epoch: 310/3000 | Batch 0007/0011 | Loss: 3139.1943\n",
      "Epoch: 310/3000 | Batch 0008/0011 | Loss: 3131.3477\n",
      "Epoch: 310/3000 | Batch 0009/0011 | Loss: 3110.9297\n",
      "Epoch: 310/3000 | Batch 0010/0011 | Loss: 3127.2559\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 311/3000 | Batch 0000/0011 | Loss: 3110.4019\n",
      "Epoch: 311/3000 | Batch 0001/0011 | Loss: 3162.7209\n",
      "Epoch: 311/3000 | Batch 0002/0011 | Loss: 3121.9685\n",
      "Epoch: 311/3000 | Batch 0003/0011 | Loss: 3195.4424\n",
      "Epoch: 311/3000 | Batch 0004/0011 | Loss: 3121.8347\n",
      "Epoch: 311/3000 | Batch 0005/0011 | Loss: 3108.3132\n",
      "Epoch: 311/3000 | Batch 0006/0011 | Loss: 3225.3438\n",
      "Epoch: 311/3000 | Batch 0007/0011 | Loss: 3130.0620\n",
      "Epoch: 311/3000 | Batch 0008/0011 | Loss: 3181.0579\n",
      "Epoch: 311/3000 | Batch 0009/0011 | Loss: 3176.8320\n",
      "Epoch: 311/3000 | Batch 0010/0011 | Loss: 3144.7878\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 312/3000 | Batch 0000/0011 | Loss: 3172.3215\n",
      "Epoch: 312/3000 | Batch 0001/0011 | Loss: 3120.3313\n",
      "Epoch: 312/3000 | Batch 0002/0011 | Loss: 3116.3088\n",
      "Epoch: 312/3000 | Batch 0003/0011 | Loss: 3149.9272\n",
      "Epoch: 312/3000 | Batch 0004/0011 | Loss: 3177.6125\n",
      "Epoch: 312/3000 | Batch 0005/0011 | Loss: 3103.9263\n",
      "Epoch: 312/3000 | Batch 0006/0011 | Loss: 3081.9736\n",
      "Epoch: 312/3000 | Batch 0007/0011 | Loss: 3141.5349\n",
      "Epoch: 312/3000 | Batch 0008/0011 | Loss: 3211.5583\n",
      "Epoch: 312/3000 | Batch 0009/0011 | Loss: 3178.8855\n",
      "Epoch: 312/3000 | Batch 0010/0011 | Loss: 3181.1094\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 313/3000 | Batch 0000/0011 | Loss: 3104.6887\n",
      "Epoch: 313/3000 | Batch 0001/0011 | Loss: 3108.9805\n",
      "Epoch: 313/3000 | Batch 0002/0011 | Loss: 3091.6208\n",
      "Epoch: 313/3000 | Batch 0003/0011 | Loss: 3131.8555\n",
      "Epoch: 313/3000 | Batch 0004/0011 | Loss: 3223.3103\n",
      "Epoch: 313/3000 | Batch 0005/0011 | Loss: 3166.0684\n",
      "Epoch: 313/3000 | Batch 0006/0011 | Loss: 3145.1409\n",
      "Epoch: 313/3000 | Batch 0007/0011 | Loss: 3163.8635\n",
      "Epoch: 313/3000 | Batch 0008/0011 | Loss: 3124.0112\n",
      "Epoch: 313/3000 | Batch 0009/0011 | Loss: 3224.2085\n",
      "Epoch: 313/3000 | Batch 0010/0011 | Loss: 3064.2549\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 314/3000 | Batch 0000/0011 | Loss: 3140.5930\n",
      "Epoch: 314/3000 | Batch 0001/0011 | Loss: 3179.3999\n",
      "Epoch: 314/3000 | Batch 0002/0011 | Loss: 3136.3621\n",
      "Epoch: 314/3000 | Batch 0003/0011 | Loss: 3141.7737\n",
      "Epoch: 314/3000 | Batch 0004/0011 | Loss: 3165.4797\n",
      "Epoch: 314/3000 | Batch 0005/0011 | Loss: 3131.8594\n",
      "Epoch: 314/3000 | Batch 0006/0011 | Loss: 3187.2673\n",
      "Epoch: 314/3000 | Batch 0007/0011 | Loss: 3118.1602\n",
      "Epoch: 314/3000 | Batch 0008/0011 | Loss: 3100.3423\n",
      "Epoch: 314/3000 | Batch 0009/0011 | Loss: 3110.2893\n",
      "Epoch: 314/3000 | Batch 0010/0011 | Loss: 3211.2166\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 315/3000 | Batch 0000/0011 | Loss: 3132.8962\n",
      "Epoch: 315/3000 | Batch 0001/0011 | Loss: 3193.4180\n",
      "Epoch: 315/3000 | Batch 0002/0011 | Loss: 3188.9392\n",
      "Epoch: 315/3000 | Batch 0003/0011 | Loss: 3148.3975\n",
      "Epoch: 315/3000 | Batch 0004/0011 | Loss: 3109.3958\n",
      "Epoch: 315/3000 | Batch 0005/0011 | Loss: 3136.3269\n",
      "Epoch: 315/3000 | Batch 0006/0011 | Loss: 3147.9978\n",
      "Epoch: 315/3000 | Batch 0007/0011 | Loss: 3126.6306\n",
      "Epoch: 315/3000 | Batch 0008/0011 | Loss: 3173.2346\n",
      "Epoch: 315/3000 | Batch 0009/0011 | Loss: 3119.5706\n",
      "Epoch: 315/3000 | Batch 0010/0011 | Loss: 2969.2498\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 316/3000 | Batch 0000/0011 | Loss: 3168.1877\n",
      "Epoch: 316/3000 | Batch 0001/0011 | Loss: 3157.7258\n",
      "Epoch: 316/3000 | Batch 0002/0011 | Loss: 3117.0557\n",
      "Epoch: 316/3000 | Batch 0003/0011 | Loss: 3216.1606\n",
      "Epoch: 316/3000 | Batch 0004/0011 | Loss: 3126.2322\n",
      "Epoch: 316/3000 | Batch 0005/0011 | Loss: 3153.4561\n",
      "Epoch: 316/3000 | Batch 0006/0011 | Loss: 3095.1101\n",
      "Epoch: 316/3000 | Batch 0007/0011 | Loss: 3105.1277\n",
      "Epoch: 316/3000 | Batch 0008/0011 | Loss: 3133.5088\n",
      "Epoch: 316/3000 | Batch 0009/0011 | Loss: 3152.3491\n",
      "Epoch: 316/3000 | Batch 0010/0011 | Loss: 3072.2351\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 317/3000 | Batch 0000/0011 | Loss: 3101.4685\n",
      "Epoch: 317/3000 | Batch 0001/0011 | Loss: 3114.4072\n",
      "Epoch: 317/3000 | Batch 0002/0011 | Loss: 3167.3154\n",
      "Epoch: 317/3000 | Batch 0003/0011 | Loss: 3224.6538\n",
      "Epoch: 317/3000 | Batch 0004/0011 | Loss: 3099.8445\n",
      "Epoch: 317/3000 | Batch 0005/0011 | Loss: 3182.3909\n",
      "Epoch: 317/3000 | Batch 0006/0011 | Loss: 3101.3689\n",
      "Epoch: 317/3000 | Batch 0007/0011 | Loss: 3230.3835\n",
      "Epoch: 317/3000 | Batch 0008/0011 | Loss: 3132.5049\n",
      "Epoch: 317/3000 | Batch 0009/0011 | Loss: 3093.0566\n",
      "Epoch: 317/3000 | Batch 0010/0011 | Loss: 3155.0510\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 318/3000 | Batch 0000/0011 | Loss: 3193.0762\n",
      "Epoch: 318/3000 | Batch 0001/0011 | Loss: 3114.6482\n",
      "Epoch: 318/3000 | Batch 0002/0011 | Loss: 3221.6755\n",
      "Epoch: 318/3000 | Batch 0003/0011 | Loss: 3191.5647\n",
      "Epoch: 318/3000 | Batch 0004/0011 | Loss: 3123.2395\n",
      "Epoch: 318/3000 | Batch 0005/0011 | Loss: 3126.9282\n",
      "Epoch: 318/3000 | Batch 0006/0011 | Loss: 3128.3455\n",
      "Epoch: 318/3000 | Batch 0007/0011 | Loss: 3128.1616\n",
      "Epoch: 318/3000 | Batch 0008/0011 | Loss: 3077.5186\n",
      "Epoch: 318/3000 | Batch 0009/0011 | Loss: 3131.6289\n",
      "Epoch: 318/3000 | Batch 0010/0011 | Loss: 3141.1418\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 319/3000 | Batch 0000/0011 | Loss: 3165.1499\n",
      "Epoch: 319/3000 | Batch 0001/0011 | Loss: 3156.4087\n",
      "Epoch: 319/3000 | Batch 0002/0011 | Loss: 3184.4746\n",
      "Epoch: 319/3000 | Batch 0003/0011 | Loss: 3141.2358\n",
      "Epoch: 319/3000 | Batch 0004/0011 | Loss: 3156.6343\n",
      "Epoch: 319/3000 | Batch 0005/0011 | Loss: 3112.9570\n",
      "Epoch: 319/3000 | Batch 0006/0011 | Loss: 3193.9033\n",
      "Epoch: 319/3000 | Batch 0007/0011 | Loss: 3105.6091\n",
      "Epoch: 319/3000 | Batch 0008/0011 | Loss: 3169.9839\n",
      "Epoch: 319/3000 | Batch 0009/0011 | Loss: 3080.2048\n",
      "Epoch: 319/3000 | Batch 0010/0011 | Loss: 3051.6152\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 320/3000 | Batch 0000/0011 | Loss: 3117.1155\n",
      "Epoch: 320/3000 | Batch 0001/0011 | Loss: 3126.0957\n",
      "Epoch: 320/3000 | Batch 0002/0011 | Loss: 3143.5593\n",
      "Epoch: 320/3000 | Batch 0003/0011 | Loss: 3072.4192\n",
      "Epoch: 320/3000 | Batch 0004/0011 | Loss: 3189.8347\n",
      "Epoch: 320/3000 | Batch 0005/0011 | Loss: 3182.8115\n",
      "Epoch: 320/3000 | Batch 0006/0011 | Loss: 3105.1702\n",
      "Epoch: 320/3000 | Batch 0007/0011 | Loss: 3174.0605\n",
      "Epoch: 320/3000 | Batch 0008/0011 | Loss: 3145.9731\n",
      "Epoch: 320/3000 | Batch 0009/0011 | Loss: 3190.2546\n",
      "Epoch: 320/3000 | Batch 0010/0011 | Loss: 3113.0459\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 321/3000 | Batch 0000/0011 | Loss: 3079.2986\n",
      "Epoch: 321/3000 | Batch 0001/0011 | Loss: 3102.8894\n",
      "Epoch: 321/3000 | Batch 0002/0011 | Loss: 3186.3972\n",
      "Epoch: 321/3000 | Batch 0003/0011 | Loss: 3031.9001\n",
      "Epoch: 321/3000 | Batch 0004/0011 | Loss: 3185.4912\n",
      "Epoch: 321/3000 | Batch 0005/0011 | Loss: 3149.2878\n",
      "Epoch: 321/3000 | Batch 0006/0011 | Loss: 3148.7153\n",
      "Epoch: 321/3000 | Batch 0007/0011 | Loss: 3166.1902\n",
      "Epoch: 321/3000 | Batch 0008/0011 | Loss: 3143.2852\n",
      "Epoch: 321/3000 | Batch 0009/0011 | Loss: 3264.8748\n",
      "Epoch: 321/3000 | Batch 0010/0011 | Loss: 3237.5715\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 322/3000 | Batch 0000/0011 | Loss: 3128.4353\n",
      "Epoch: 322/3000 | Batch 0001/0011 | Loss: 3143.4724\n",
      "Epoch: 322/3000 | Batch 0002/0011 | Loss: 3166.5518\n",
      "Epoch: 322/3000 | Batch 0003/0011 | Loss: 3167.9658\n",
      "Epoch: 322/3000 | Batch 0004/0011 | Loss: 3104.3928\n",
      "Epoch: 322/3000 | Batch 0005/0011 | Loss: 3160.2971\n",
      "Epoch: 322/3000 | Batch 0006/0011 | Loss: 3202.9094\n",
      "Epoch: 322/3000 | Batch 0007/0011 | Loss: 3179.4707\n",
      "Epoch: 322/3000 | Batch 0008/0011 | Loss: 3089.1882\n",
      "Epoch: 322/3000 | Batch 0009/0011 | Loss: 3154.9397\n",
      "Epoch: 322/3000 | Batch 0010/0011 | Loss: 3011.6704\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 323/3000 | Batch 0000/0011 | Loss: 3075.7185\n",
      "Epoch: 323/3000 | Batch 0001/0011 | Loss: 3151.4397\n",
      "Epoch: 323/3000 | Batch 0002/0011 | Loss: 3114.4639\n",
      "Epoch: 323/3000 | Batch 0003/0011 | Loss: 3101.9014\n",
      "Epoch: 323/3000 | Batch 0004/0011 | Loss: 3145.6943\n",
      "Epoch: 323/3000 | Batch 0005/0011 | Loss: 3182.7771\n",
      "Epoch: 323/3000 | Batch 0006/0011 | Loss: 3128.3198\n",
      "Epoch: 323/3000 | Batch 0007/0011 | Loss: 3130.4231\n",
      "Epoch: 323/3000 | Batch 0008/0011 | Loss: 3173.0396\n",
      "Epoch: 323/3000 | Batch 0009/0011 | Loss: 3220.7786\n",
      "Epoch: 323/3000 | Batch 0010/0011 | Loss: 3238.5276\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 324/3000 | Batch 0000/0011 | Loss: 3075.0789\n",
      "Epoch: 324/3000 | Batch 0001/0011 | Loss: 3101.3933\n",
      "Epoch: 324/3000 | Batch 0002/0011 | Loss: 3131.4863\n",
      "Epoch: 324/3000 | Batch 0003/0011 | Loss: 3094.8362\n",
      "Epoch: 324/3000 | Batch 0004/0011 | Loss: 3153.4177\n",
      "Epoch: 324/3000 | Batch 0005/0011 | Loss: 3223.6584\n",
      "Epoch: 324/3000 | Batch 0006/0011 | Loss: 3246.2415\n",
      "Epoch: 324/3000 | Batch 0007/0011 | Loss: 3133.2405\n",
      "Epoch: 324/3000 | Batch 0008/0011 | Loss: 3134.1680\n",
      "Epoch: 324/3000 | Batch 0009/0011 | Loss: 3136.9001\n",
      "Epoch: 324/3000 | Batch 0010/0011 | Loss: 3063.6914\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 325/3000 | Batch 0000/0011 | Loss: 3128.0112\n",
      "Epoch: 325/3000 | Batch 0001/0011 | Loss: 3128.4202\n",
      "Epoch: 325/3000 | Batch 0002/0011 | Loss: 3111.1099\n",
      "Epoch: 325/3000 | Batch 0003/0011 | Loss: 3202.6833\n",
      "Epoch: 325/3000 | Batch 0004/0011 | Loss: 3188.4641\n",
      "Epoch: 325/3000 | Batch 0005/0011 | Loss: 3174.9966\n",
      "Epoch: 325/3000 | Batch 0006/0011 | Loss: 3061.7283\n",
      "Epoch: 325/3000 | Batch 0007/0011 | Loss: 3141.2888\n",
      "Epoch: 325/3000 | Batch 0008/0011 | Loss: 3159.3274\n",
      "Epoch: 325/3000 | Batch 0009/0011 | Loss: 3155.1558\n",
      "Epoch: 325/3000 | Batch 0010/0011 | Loss: 3094.4771\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 326/3000 | Batch 0000/0011 | Loss: 3117.5454\n",
      "Epoch: 326/3000 | Batch 0001/0011 | Loss: 3082.0938\n",
      "Epoch: 326/3000 | Batch 0002/0011 | Loss: 3192.7964\n",
      "Epoch: 326/3000 | Batch 0003/0011 | Loss: 3096.6094\n",
      "Epoch: 326/3000 | Batch 0004/0011 | Loss: 3198.3164\n",
      "Epoch: 326/3000 | Batch 0005/0011 | Loss: 3143.6926\n",
      "Epoch: 326/3000 | Batch 0006/0011 | Loss: 3190.6697\n",
      "Epoch: 326/3000 | Batch 0007/0011 | Loss: 3172.9724\n",
      "Epoch: 326/3000 | Batch 0008/0011 | Loss: 3102.1882\n",
      "Epoch: 326/3000 | Batch 0009/0011 | Loss: 3115.5579\n",
      "Epoch: 326/3000 | Batch 0010/0011 | Loss: 3194.7395\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 327/3000 | Batch 0000/0011 | Loss: 3241.8203\n",
      "Epoch: 327/3000 | Batch 0001/0011 | Loss: 3195.6938\n",
      "Epoch: 327/3000 | Batch 0002/0011 | Loss: 3127.3391\n",
      "Epoch: 327/3000 | Batch 0003/0011 | Loss: 3091.8848\n",
      "Epoch: 327/3000 | Batch 0004/0011 | Loss: 3073.8640\n",
      "Epoch: 327/3000 | Batch 0005/0011 | Loss: 3109.8423\n",
      "Epoch: 327/3000 | Batch 0006/0011 | Loss: 3210.4797\n",
      "Epoch: 327/3000 | Batch 0007/0011 | Loss: 3132.0981\n",
      "Epoch: 327/3000 | Batch 0008/0011 | Loss: 3130.5007\n",
      "Epoch: 327/3000 | Batch 0009/0011 | Loss: 3108.6174\n",
      "Epoch: 327/3000 | Batch 0010/0011 | Loss: 3027.0232\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 328/3000 | Batch 0000/0011 | Loss: 3125.7014\n",
      "Epoch: 328/3000 | Batch 0001/0011 | Loss: 3146.7268\n",
      "Epoch: 328/3000 | Batch 0002/0011 | Loss: 3117.6956\n",
      "Epoch: 328/3000 | Batch 0003/0011 | Loss: 3152.7009\n",
      "Epoch: 328/3000 | Batch 0004/0011 | Loss: 3159.9089\n",
      "Epoch: 328/3000 | Batch 0005/0011 | Loss: 3090.7405\n",
      "Epoch: 328/3000 | Batch 0006/0011 | Loss: 3168.3599\n",
      "Epoch: 328/3000 | Batch 0007/0011 | Loss: 3171.7339\n",
      "Epoch: 328/3000 | Batch 0008/0011 | Loss: 3118.7988\n",
      "Epoch: 328/3000 | Batch 0009/0011 | Loss: 3157.8306\n",
      "Epoch: 328/3000 | Batch 0010/0011 | Loss: 3087.3394\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 329/3000 | Batch 0000/0011 | Loss: 3206.6750\n",
      "Epoch: 329/3000 | Batch 0001/0011 | Loss: 3079.2249\n",
      "Epoch: 329/3000 | Batch 0002/0011 | Loss: 3204.1907\n",
      "Epoch: 329/3000 | Batch 0003/0011 | Loss: 3078.1709\n",
      "Epoch: 329/3000 | Batch 0004/0011 | Loss: 3103.3594\n",
      "Epoch: 329/3000 | Batch 0005/0011 | Loss: 3163.4849\n",
      "Epoch: 329/3000 | Batch 0006/0011 | Loss: 3158.9524\n",
      "Epoch: 329/3000 | Batch 0007/0011 | Loss: 3206.5581\n",
      "Epoch: 329/3000 | Batch 0008/0011 | Loss: 3170.9497\n",
      "Epoch: 329/3000 | Batch 0009/0011 | Loss: 3086.2339\n",
      "Epoch: 329/3000 | Batch 0010/0011 | Loss: 3098.1401\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 330/3000 | Batch 0000/0011 | Loss: 3111.7703\n",
      "Epoch: 330/3000 | Batch 0001/0011 | Loss: 3139.7175\n",
      "Epoch: 330/3000 | Batch 0002/0011 | Loss: 3146.1753\n",
      "Epoch: 330/3000 | Batch 0003/0011 | Loss: 3127.0259\n",
      "Epoch: 330/3000 | Batch 0004/0011 | Loss: 3182.6946\n",
      "Epoch: 330/3000 | Batch 0005/0011 | Loss: 3136.2815\n",
      "Epoch: 330/3000 | Batch 0006/0011 | Loss: 3126.2988\n",
      "Epoch: 330/3000 | Batch 0007/0011 | Loss: 3112.1443\n",
      "Epoch: 330/3000 | Batch 0008/0011 | Loss: 3158.1934\n",
      "Epoch: 330/3000 | Batch 0009/0011 | Loss: 3157.9663\n",
      "Epoch: 330/3000 | Batch 0010/0011 | Loss: 3103.3535\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 331/3000 | Batch 0000/0011 | Loss: 3147.6997\n",
      "Epoch: 331/3000 | Batch 0001/0011 | Loss: 3166.5242\n",
      "Epoch: 331/3000 | Batch 0002/0011 | Loss: 3107.6863\n",
      "Epoch: 331/3000 | Batch 0003/0011 | Loss: 3188.1101\n",
      "Epoch: 331/3000 | Batch 0004/0011 | Loss: 3161.8899\n",
      "Epoch: 331/3000 | Batch 0005/0011 | Loss: 3101.7214\n",
      "Epoch: 331/3000 | Batch 0006/0011 | Loss: 3071.5081\n",
      "Epoch: 331/3000 | Batch 0007/0011 | Loss: 3221.1392\n",
      "Epoch: 331/3000 | Batch 0008/0011 | Loss: 3106.0176\n",
      "Epoch: 331/3000 | Batch 0009/0011 | Loss: 3121.7244\n",
      "Epoch: 331/3000 | Batch 0010/0011 | Loss: 3141.0222\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 332/3000 | Batch 0000/0011 | Loss: 3164.6631\n",
      "Epoch: 332/3000 | Batch 0001/0011 | Loss: 3211.0012\n",
      "Epoch: 332/3000 | Batch 0002/0011 | Loss: 3157.6470\n",
      "Epoch: 332/3000 | Batch 0003/0011 | Loss: 3217.7534\n",
      "Epoch: 332/3000 | Batch 0004/0011 | Loss: 3063.7449\n",
      "Epoch: 332/3000 | Batch 0005/0011 | Loss: 3070.4954\n",
      "Epoch: 332/3000 | Batch 0006/0011 | Loss: 3123.1492\n",
      "Epoch: 332/3000 | Batch 0007/0011 | Loss: 3162.9404\n",
      "Epoch: 332/3000 | Batch 0008/0011 | Loss: 3165.8767\n",
      "Epoch: 332/3000 | Batch 0009/0011 | Loss: 3070.8738\n",
      "Epoch: 332/3000 | Batch 0010/0011 | Loss: 3292.8257\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 333/3000 | Batch 0000/0011 | Loss: 3148.3430\n",
      "Epoch: 333/3000 | Batch 0001/0011 | Loss: 3228.0701\n",
      "Epoch: 333/3000 | Batch 0002/0011 | Loss: 3142.8909\n",
      "Epoch: 333/3000 | Batch 0003/0011 | Loss: 3037.0129\n",
      "Epoch: 333/3000 | Batch 0004/0011 | Loss: 3106.9062\n",
      "Epoch: 333/3000 | Batch 0005/0011 | Loss: 3162.9993\n",
      "Epoch: 333/3000 | Batch 0006/0011 | Loss: 3115.4783\n",
      "Epoch: 333/3000 | Batch 0007/0011 | Loss: 3144.5525\n",
      "Epoch: 333/3000 | Batch 0008/0011 | Loss: 3147.8394\n",
      "Epoch: 333/3000 | Batch 0009/0011 | Loss: 3174.2947\n",
      "Epoch: 333/3000 | Batch 0010/0011 | Loss: 3083.4092\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 334/3000 | Batch 0000/0011 | Loss: 3156.4763\n",
      "Epoch: 334/3000 | Batch 0001/0011 | Loss: 3124.7783\n",
      "Epoch: 334/3000 | Batch 0002/0011 | Loss: 3179.0522\n",
      "Epoch: 334/3000 | Batch 0003/0011 | Loss: 3090.2490\n",
      "Epoch: 334/3000 | Batch 0004/0011 | Loss: 3148.9192\n",
      "Epoch: 334/3000 | Batch 0005/0011 | Loss: 3261.3550\n",
      "Epoch: 334/3000 | Batch 0006/0011 | Loss: 3093.9604\n",
      "Epoch: 334/3000 | Batch 0007/0011 | Loss: 3158.0942\n",
      "Epoch: 334/3000 | Batch 0008/0011 | Loss: 3095.8423\n",
      "Epoch: 334/3000 | Batch 0009/0011 | Loss: 3119.7114\n",
      "Epoch: 334/3000 | Batch 0010/0011 | Loss: 3173.6487\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 335/3000 | Batch 0000/0011 | Loss: 3088.0842\n",
      "Epoch: 335/3000 | Batch 0001/0011 | Loss: 3101.7327\n",
      "Epoch: 335/3000 | Batch 0002/0011 | Loss: 3138.1716\n",
      "Epoch: 335/3000 | Batch 0003/0011 | Loss: 3201.9309\n",
      "Epoch: 335/3000 | Batch 0004/0011 | Loss: 3161.5430\n",
      "Epoch: 335/3000 | Batch 0005/0011 | Loss: 3143.5288\n",
      "Epoch: 335/3000 | Batch 0006/0011 | Loss: 3114.0215\n",
      "Epoch: 335/3000 | Batch 0007/0011 | Loss: 3232.6099\n",
      "Epoch: 335/3000 | Batch 0008/0011 | Loss: 3077.5527\n",
      "Epoch: 335/3000 | Batch 0009/0011 | Loss: 3119.6365\n",
      "Epoch: 335/3000 | Batch 0010/0011 | Loss: 3050.6929\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 336/3000 | Batch 0000/0011 | Loss: 3084.8147\n",
      "Epoch: 336/3000 | Batch 0001/0011 | Loss: 3185.9031\n",
      "Epoch: 336/3000 | Batch 0002/0011 | Loss: 3124.6616\n",
      "Epoch: 336/3000 | Batch 0003/0011 | Loss: 3094.1787\n",
      "Epoch: 336/3000 | Batch 0004/0011 | Loss: 3188.9673\n",
      "Epoch: 336/3000 | Batch 0005/0011 | Loss: 3162.0798\n",
      "Epoch: 336/3000 | Batch 0006/0011 | Loss: 3157.4509\n",
      "Epoch: 336/3000 | Batch 0007/0011 | Loss: 3063.6201\n",
      "Epoch: 336/3000 | Batch 0008/0011 | Loss: 3173.5737\n",
      "Epoch: 336/3000 | Batch 0009/0011 | Loss: 3109.3538\n",
      "Epoch: 336/3000 | Batch 0010/0011 | Loss: 2928.8254\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 337/3000 | Batch 0000/0011 | Loss: 3103.8000\n",
      "Epoch: 337/3000 | Batch 0001/0011 | Loss: 3065.4585\n",
      "Epoch: 337/3000 | Batch 0002/0011 | Loss: 3219.4114\n",
      "Epoch: 337/3000 | Batch 0003/0011 | Loss: 3133.9729\n",
      "Epoch: 337/3000 | Batch 0004/0011 | Loss: 3079.3032\n",
      "Epoch: 337/3000 | Batch 0005/0011 | Loss: 3161.5920\n",
      "Epoch: 337/3000 | Batch 0006/0011 | Loss: 3227.1494\n",
      "Epoch: 337/3000 | Batch 0007/0011 | Loss: 3078.8552\n",
      "Epoch: 337/3000 | Batch 0008/0011 | Loss: 3139.5637\n",
      "Epoch: 337/3000 | Batch 0009/0011 | Loss: 3079.8110\n",
      "Epoch: 337/3000 | Batch 0010/0011 | Loss: 3322.3899\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 338/3000 | Batch 0000/0011 | Loss: 3088.1467\n",
      "Epoch: 338/3000 | Batch 0001/0011 | Loss: 3079.5293\n",
      "Epoch: 338/3000 | Batch 0002/0011 | Loss: 3208.1985\n",
      "Epoch: 338/3000 | Batch 0003/0011 | Loss: 3108.8955\n",
      "Epoch: 338/3000 | Batch 0004/0011 | Loss: 3149.3740\n",
      "Epoch: 338/3000 | Batch 0005/0011 | Loss: 3136.3057\n",
      "Epoch: 338/3000 | Batch 0006/0011 | Loss: 3217.5823\n",
      "Epoch: 338/3000 | Batch 0007/0011 | Loss: 3120.1941\n",
      "Epoch: 338/3000 | Batch 0008/0011 | Loss: 3152.5151\n",
      "Epoch: 338/3000 | Batch 0009/0011 | Loss: 3065.6355\n",
      "Epoch: 338/3000 | Batch 0010/0011 | Loss: 3065.4741\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 339/3000 | Batch 0000/0011 | Loss: 3124.4465\n",
      "Epoch: 339/3000 | Batch 0001/0011 | Loss: 3062.7820\n",
      "Epoch: 339/3000 | Batch 0002/0011 | Loss: 3198.6643\n",
      "Epoch: 339/3000 | Batch 0003/0011 | Loss: 3099.2966\n",
      "Epoch: 339/3000 | Batch 0004/0011 | Loss: 3158.6099\n",
      "Epoch: 339/3000 | Batch 0005/0011 | Loss: 3082.3777\n",
      "Epoch: 339/3000 | Batch 0006/0011 | Loss: 3235.6396\n",
      "Epoch: 339/3000 | Batch 0007/0011 | Loss: 3135.2605\n",
      "Epoch: 339/3000 | Batch 0008/0011 | Loss: 3093.9700\n",
      "Epoch: 339/3000 | Batch 0009/0011 | Loss: 3112.7837\n",
      "Epoch: 339/3000 | Batch 0010/0011 | Loss: 3023.9829\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 340/3000 | Batch 0000/0011 | Loss: 3133.1680\n",
      "Epoch: 340/3000 | Batch 0001/0011 | Loss: 3120.4741\n",
      "Epoch: 340/3000 | Batch 0002/0011 | Loss: 3155.2231\n",
      "Epoch: 340/3000 | Batch 0003/0011 | Loss: 3188.8936\n",
      "Epoch: 340/3000 | Batch 0004/0011 | Loss: 3153.5381\n",
      "Epoch: 340/3000 | Batch 0005/0011 | Loss: 3129.1157\n",
      "Epoch: 340/3000 | Batch 0006/0011 | Loss: 3133.6838\n",
      "Epoch: 340/3000 | Batch 0007/0011 | Loss: 3121.6394\n",
      "Epoch: 340/3000 | Batch 0008/0011 | Loss: 3133.8774\n",
      "Epoch: 340/3000 | Batch 0009/0011 | Loss: 3091.9048\n",
      "Epoch: 340/3000 | Batch 0010/0011 | Loss: 3096.1206\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 341/3000 | Batch 0000/0011 | Loss: 3124.2273\n",
      "Epoch: 341/3000 | Batch 0001/0011 | Loss: 3153.6953\n",
      "Epoch: 341/3000 | Batch 0002/0011 | Loss: 3078.4143\n",
      "Epoch: 341/3000 | Batch 0003/0011 | Loss: 3145.5647\n",
      "Epoch: 341/3000 | Batch 0004/0011 | Loss: 3142.0312\n",
      "Epoch: 341/3000 | Batch 0005/0011 | Loss: 3123.7590\n",
      "Epoch: 341/3000 | Batch 0006/0011 | Loss: 3152.1318\n",
      "Epoch: 341/3000 | Batch 0007/0011 | Loss: 3134.5073\n",
      "Epoch: 341/3000 | Batch 0008/0011 | Loss: 3172.8328\n",
      "Epoch: 341/3000 | Batch 0009/0011 | Loss: 3098.8696\n",
      "Epoch: 341/3000 | Batch 0010/0011 | Loss: 3101.8936\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 342/3000 | Batch 0000/0011 | Loss: 3107.5916\n",
      "Epoch: 342/3000 | Batch 0001/0011 | Loss: 3069.3345\n",
      "Epoch: 342/3000 | Batch 0002/0011 | Loss: 3111.6689\n",
      "Epoch: 342/3000 | Batch 0003/0011 | Loss: 3071.4802\n",
      "Epoch: 342/3000 | Batch 0004/0011 | Loss: 3083.7444\n",
      "Epoch: 342/3000 | Batch 0005/0011 | Loss: 3091.3213\n",
      "Epoch: 342/3000 | Batch 0006/0011 | Loss: 3342.0339\n",
      "Epoch: 342/3000 | Batch 0007/0011 | Loss: 3170.5569\n",
      "Epoch: 342/3000 | Batch 0008/0011 | Loss: 3124.9180\n",
      "Epoch: 342/3000 | Batch 0009/0011 | Loss: 3164.7952\n",
      "Epoch: 342/3000 | Batch 0010/0011 | Loss: 3000.7354\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 343/3000 | Batch 0000/0011 | Loss: 3117.1589\n",
      "Epoch: 343/3000 | Batch 0001/0011 | Loss: 3058.2080\n",
      "Epoch: 343/3000 | Batch 0002/0011 | Loss: 3122.4795\n",
      "Epoch: 343/3000 | Batch 0003/0011 | Loss: 3183.7256\n",
      "Epoch: 343/3000 | Batch 0004/0011 | Loss: 3115.1348\n",
      "Epoch: 343/3000 | Batch 0005/0011 | Loss: 3061.6951\n",
      "Epoch: 343/3000 | Batch 0006/0011 | Loss: 3126.1641\n",
      "Epoch: 343/3000 | Batch 0007/0011 | Loss: 3163.4521\n",
      "Epoch: 343/3000 | Batch 0008/0011 | Loss: 3222.5344\n",
      "Epoch: 343/3000 | Batch 0009/0011 | Loss: 3102.9050\n",
      "Epoch: 343/3000 | Batch 0010/0011 | Loss: 3216.9077\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 344/3000 | Batch 0000/0011 | Loss: 3100.0295\n",
      "Epoch: 344/3000 | Batch 0001/0011 | Loss: 3123.1506\n",
      "Epoch: 344/3000 | Batch 0002/0011 | Loss: 3154.3518\n",
      "Epoch: 344/3000 | Batch 0003/0011 | Loss: 3189.5527\n",
      "Epoch: 344/3000 | Batch 0004/0011 | Loss: 3115.6165\n",
      "Epoch: 344/3000 | Batch 0005/0011 | Loss: 3107.6987\n",
      "Epoch: 344/3000 | Batch 0006/0011 | Loss: 3112.1245\n",
      "Epoch: 344/3000 | Batch 0007/0011 | Loss: 3135.3083\n",
      "Epoch: 344/3000 | Batch 0008/0011 | Loss: 3106.3010\n",
      "Epoch: 344/3000 | Batch 0009/0011 | Loss: 3132.1401\n",
      "Epoch: 344/3000 | Batch 0010/0011 | Loss: 3076.2231\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 345/3000 | Batch 0000/0011 | Loss: 3129.7280\n",
      "Epoch: 345/3000 | Batch 0001/0011 | Loss: 3127.7415\n",
      "Epoch: 345/3000 | Batch 0002/0011 | Loss: 3165.0879\n",
      "Epoch: 345/3000 | Batch 0003/0011 | Loss: 3052.9678\n",
      "Epoch: 345/3000 | Batch 0004/0011 | Loss: 3154.0085\n",
      "Epoch: 345/3000 | Batch 0005/0011 | Loss: 3125.2405\n",
      "Epoch: 345/3000 | Batch 0006/0011 | Loss: 3190.3979\n",
      "Epoch: 345/3000 | Batch 0007/0011 | Loss: 3105.6912\n",
      "Epoch: 345/3000 | Batch 0008/0011 | Loss: 3113.6062\n",
      "Epoch: 345/3000 | Batch 0009/0011 | Loss: 3135.3198\n",
      "Epoch: 345/3000 | Batch 0010/0011 | Loss: 3139.8345\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 346/3000 | Batch 0000/0011 | Loss: 3058.9802\n",
      "Epoch: 346/3000 | Batch 0001/0011 | Loss: 3112.1150\n",
      "Epoch: 346/3000 | Batch 0002/0011 | Loss: 3237.0513\n",
      "Epoch: 346/3000 | Batch 0003/0011 | Loss: 3076.5747\n",
      "Epoch: 346/3000 | Batch 0004/0011 | Loss: 3076.3120\n",
      "Epoch: 346/3000 | Batch 0005/0011 | Loss: 3197.2913\n",
      "Epoch: 346/3000 | Batch 0006/0011 | Loss: 3244.2517\n",
      "Epoch: 346/3000 | Batch 0007/0011 | Loss: 3128.2993\n",
      "Epoch: 346/3000 | Batch 0008/0011 | Loss: 3070.3896\n",
      "Epoch: 346/3000 | Batch 0009/0011 | Loss: 3062.8364\n",
      "Epoch: 346/3000 | Batch 0010/0011 | Loss: 3320.6846\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 347/3000 | Batch 0000/0011 | Loss: 3095.3977\n",
      "Epoch: 347/3000 | Batch 0001/0011 | Loss: 3141.6003\n",
      "Epoch: 347/3000 | Batch 0002/0011 | Loss: 3142.7139\n",
      "Epoch: 347/3000 | Batch 0003/0011 | Loss: 3133.5718\n",
      "Epoch: 347/3000 | Batch 0004/0011 | Loss: 3161.0913\n",
      "Epoch: 347/3000 | Batch 0005/0011 | Loss: 3108.0815\n",
      "Epoch: 347/3000 | Batch 0006/0011 | Loss: 3128.5002\n",
      "Epoch: 347/3000 | Batch 0007/0011 | Loss: 3152.6609\n",
      "Epoch: 347/3000 | Batch 0008/0011 | Loss: 3082.6050\n",
      "Epoch: 347/3000 | Batch 0009/0011 | Loss: 3130.7163\n",
      "Epoch: 347/3000 | Batch 0010/0011 | Loss: 3161.7900\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 348/3000 | Batch 0000/0011 | Loss: 3194.3320\n",
      "Epoch: 348/3000 | Batch 0001/0011 | Loss: 3095.2600\n",
      "Epoch: 348/3000 | Batch 0002/0011 | Loss: 3131.7410\n",
      "Epoch: 348/3000 | Batch 0003/0011 | Loss: 3081.4822\n",
      "Epoch: 348/3000 | Batch 0004/0011 | Loss: 3160.6968\n",
      "Epoch: 348/3000 | Batch 0005/0011 | Loss: 3118.2888\n",
      "Epoch: 348/3000 | Batch 0006/0011 | Loss: 3108.0017\n",
      "Epoch: 348/3000 | Batch 0007/0011 | Loss: 3158.6243\n",
      "Epoch: 348/3000 | Batch 0008/0011 | Loss: 3119.8328\n",
      "Epoch: 348/3000 | Batch 0009/0011 | Loss: 3112.7292\n",
      "Epoch: 348/3000 | Batch 0010/0011 | Loss: 3093.3074\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 349/3000 | Batch 0000/0011 | Loss: 3155.9653\n",
      "Epoch: 349/3000 | Batch 0001/0011 | Loss: 3100.9558\n",
      "Epoch: 349/3000 | Batch 0002/0011 | Loss: 3194.2175\n",
      "Epoch: 349/3000 | Batch 0003/0011 | Loss: 3117.7866\n",
      "Epoch: 349/3000 | Batch 0004/0011 | Loss: 3118.2520\n",
      "Epoch: 349/3000 | Batch 0005/0011 | Loss: 3156.9697\n",
      "Epoch: 349/3000 | Batch 0006/0011 | Loss: 3097.2207\n",
      "Epoch: 349/3000 | Batch 0007/0011 | Loss: 3131.7205\n",
      "Epoch: 349/3000 | Batch 0008/0011 | Loss: 3022.3545\n",
      "Epoch: 349/3000 | Batch 0009/0011 | Loss: 3128.8262\n",
      "Epoch: 349/3000 | Batch 0010/0011 | Loss: 3066.4778\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 350/3000 | Batch 0000/0011 | Loss: 3074.6921\n",
      "Epoch: 350/3000 | Batch 0001/0011 | Loss: 3152.7214\n",
      "Epoch: 350/3000 | Batch 0002/0011 | Loss: 3156.6768\n",
      "Epoch: 350/3000 | Batch 0003/0011 | Loss: 3107.2788\n",
      "Epoch: 350/3000 | Batch 0004/0011 | Loss: 3129.5889\n",
      "Epoch: 350/3000 | Batch 0005/0011 | Loss: 3113.2683\n",
      "Epoch: 350/3000 | Batch 0006/0011 | Loss: 3101.0681\n",
      "Epoch: 350/3000 | Batch 0007/0011 | Loss: 3111.6262\n",
      "Epoch: 350/3000 | Batch 0008/0011 | Loss: 3205.7705\n",
      "Epoch: 350/3000 | Batch 0009/0011 | Loss: 3038.8691\n",
      "Epoch: 350/3000 | Batch 0010/0011 | Loss: 3179.6980\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 351/3000 | Batch 0000/0011 | Loss: 3136.1660\n",
      "Epoch: 351/3000 | Batch 0001/0011 | Loss: 3189.6003\n",
      "Epoch: 351/3000 | Batch 0002/0011 | Loss: 3057.5942\n",
      "Epoch: 351/3000 | Batch 0003/0011 | Loss: 3140.7388\n",
      "Epoch: 351/3000 | Batch 0004/0011 | Loss: 3113.1045\n",
      "Epoch: 351/3000 | Batch 0005/0011 | Loss: 3065.9131\n",
      "Epoch: 351/3000 | Batch 0006/0011 | Loss: 3152.9919\n",
      "Epoch: 351/3000 | Batch 0007/0011 | Loss: 3117.5466\n",
      "Epoch: 351/3000 | Batch 0008/0011 | Loss: 3043.4724\n",
      "Epoch: 351/3000 | Batch 0009/0011 | Loss: 3136.5835\n",
      "Epoch: 351/3000 | Batch 0010/0011 | Loss: 2971.1570\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 352/3000 | Batch 0000/0011 | Loss: 3107.5303\n",
      "Epoch: 352/3000 | Batch 0001/0011 | Loss: 3161.4553\n",
      "Epoch: 352/3000 | Batch 0002/0011 | Loss: 3095.1375\n",
      "Epoch: 352/3000 | Batch 0003/0011 | Loss: 3136.0466\n",
      "Epoch: 352/3000 | Batch 0004/0011 | Loss: 3010.8621\n",
      "Epoch: 352/3000 | Batch 0005/0011 | Loss: 3042.5662\n",
      "Epoch: 352/3000 | Batch 0006/0011 | Loss: 3076.3562\n",
      "Epoch: 352/3000 | Batch 0007/0011 | Loss: 3190.8245\n",
      "Epoch: 352/3000 | Batch 0008/0011 | Loss: 3060.1924\n",
      "Epoch: 352/3000 | Batch 0009/0011 | Loss: 3244.2009\n",
      "Epoch: 352/3000 | Batch 0010/0011 | Loss: 3145.3372\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 353/3000 | Batch 0000/0011 | Loss: 3106.3152\n",
      "Epoch: 353/3000 | Batch 0001/0011 | Loss: 3055.7957\n",
      "Epoch: 353/3000 | Batch 0002/0011 | Loss: 3162.3748\n",
      "Epoch: 353/3000 | Batch 0003/0011 | Loss: 3083.4583\n",
      "Epoch: 353/3000 | Batch 0004/0011 | Loss: 3128.7791\n",
      "Epoch: 353/3000 | Batch 0005/0011 | Loss: 3123.7109\n",
      "Epoch: 353/3000 | Batch 0006/0011 | Loss: 3056.1663\n",
      "Epoch: 353/3000 | Batch 0007/0011 | Loss: 3112.2361\n",
      "Epoch: 353/3000 | Batch 0008/0011 | Loss: 3218.8032\n",
      "Epoch: 353/3000 | Batch 0009/0011 | Loss: 3060.2598\n",
      "Epoch: 353/3000 | Batch 0010/0011 | Loss: 3062.7603\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 354/3000 | Batch 0000/0011 | Loss: 3078.9988\n",
      "Epoch: 354/3000 | Batch 0001/0011 | Loss: 3126.2234\n",
      "Epoch: 354/3000 | Batch 0002/0011 | Loss: 3043.5566\n",
      "Epoch: 354/3000 | Batch 0003/0011 | Loss: 3102.3450\n",
      "Epoch: 354/3000 | Batch 0004/0011 | Loss: 3095.8220\n",
      "Epoch: 354/3000 | Batch 0005/0011 | Loss: 3146.4873\n",
      "Epoch: 354/3000 | Batch 0006/0011 | Loss: 3117.3921\n",
      "Epoch: 354/3000 | Batch 0007/0011 | Loss: 3050.0742\n",
      "Epoch: 354/3000 | Batch 0008/0011 | Loss: 3157.5664\n",
      "Epoch: 354/3000 | Batch 0009/0011 | Loss: 3114.1650\n",
      "Epoch: 354/3000 | Batch 0010/0011 | Loss: 3127.2776\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 355/3000 | Batch 0000/0011 | Loss: 3072.8389\n",
      "Epoch: 355/3000 | Batch 0001/0011 | Loss: 3071.5706\n",
      "Epoch: 355/3000 | Batch 0002/0011 | Loss: 3101.4851\n",
      "Epoch: 355/3000 | Batch 0003/0011 | Loss: 3152.1252\n",
      "Epoch: 355/3000 | Batch 0004/0011 | Loss: 3049.1792\n",
      "Epoch: 355/3000 | Batch 0005/0011 | Loss: 3073.2441\n",
      "Epoch: 355/3000 | Batch 0006/0011 | Loss: 3076.7295\n",
      "Epoch: 355/3000 | Batch 0007/0011 | Loss: 3157.4900\n",
      "Epoch: 355/3000 | Batch 0008/0011 | Loss: 3121.5322\n",
      "Epoch: 355/3000 | Batch 0009/0011 | Loss: 3036.2744\n",
      "Epoch: 355/3000 | Batch 0010/0011 | Loss: 3500.8506\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 356/3000 | Batch 0000/0011 | Loss: 3056.9768\n",
      "Epoch: 356/3000 | Batch 0001/0011 | Loss: 3070.1392\n",
      "Epoch: 356/3000 | Batch 0002/0011 | Loss: 3002.6934\n",
      "Epoch: 356/3000 | Batch 0003/0011 | Loss: 3132.4382\n",
      "Epoch: 356/3000 | Batch 0004/0011 | Loss: 3113.1990\n",
      "Epoch: 356/3000 | Batch 0005/0011 | Loss: 3129.0330\n",
      "Epoch: 356/3000 | Batch 0006/0011 | Loss: 3121.8708\n",
      "Epoch: 356/3000 | Batch 0007/0011 | Loss: 3065.9438\n",
      "Epoch: 356/3000 | Batch 0008/0011 | Loss: 3211.0930\n",
      "Epoch: 356/3000 | Batch 0009/0011 | Loss: 3088.1301\n",
      "Epoch: 356/3000 | Batch 0010/0011 | Loss: 3094.4573\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 357/3000 | Batch 0000/0011 | Loss: 3075.2166\n",
      "Epoch: 357/3000 | Batch 0001/0011 | Loss: 3044.1870\n",
      "Epoch: 357/3000 | Batch 0002/0011 | Loss: 3151.6479\n",
      "Epoch: 357/3000 | Batch 0003/0011 | Loss: 3090.9026\n",
      "Epoch: 357/3000 | Batch 0004/0011 | Loss: 3092.6079\n",
      "Epoch: 357/3000 | Batch 0005/0011 | Loss: 3155.4929\n",
      "Epoch: 357/3000 | Batch 0006/0011 | Loss: 3117.4624\n",
      "Epoch: 357/3000 | Batch 0007/0011 | Loss: 3095.7498\n",
      "Epoch: 357/3000 | Batch 0008/0011 | Loss: 3055.8142\n",
      "Epoch: 357/3000 | Batch 0009/0011 | Loss: 3038.0164\n",
      "Epoch: 357/3000 | Batch 0010/0011 | Loss: 3155.1047\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 358/3000 | Batch 0000/0011 | Loss: 3034.3928\n",
      "Epoch: 358/3000 | Batch 0001/0011 | Loss: 3145.0840\n",
      "Epoch: 358/3000 | Batch 0002/0011 | Loss: 3094.7227\n",
      "Epoch: 358/3000 | Batch 0003/0011 | Loss: 3045.0498\n",
      "Epoch: 358/3000 | Batch 0004/0011 | Loss: 3128.0852\n",
      "Epoch: 358/3000 | Batch 0005/0011 | Loss: 3130.2534\n",
      "Epoch: 358/3000 | Batch 0006/0011 | Loss: 3060.3997\n",
      "Epoch: 358/3000 | Batch 0007/0011 | Loss: 3096.9829\n",
      "Epoch: 358/3000 | Batch 0008/0011 | Loss: 3071.5139\n",
      "Epoch: 358/3000 | Batch 0009/0011 | Loss: 3066.5854\n",
      "Epoch: 358/3000 | Batch 0010/0011 | Loss: 3152.1062\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 359/3000 | Batch 0000/0011 | Loss: 3097.5200\n",
      "Epoch: 359/3000 | Batch 0001/0011 | Loss: 3087.3945\n",
      "Epoch: 359/3000 | Batch 0002/0011 | Loss: 3066.3647\n",
      "Epoch: 359/3000 | Batch 0003/0011 | Loss: 3083.9304\n",
      "Epoch: 359/3000 | Batch 0004/0011 | Loss: 3076.9451\n",
      "Epoch: 359/3000 | Batch 0005/0011 | Loss: 3084.8823\n",
      "Epoch: 359/3000 | Batch 0006/0011 | Loss: 3101.5251\n",
      "Epoch: 359/3000 | Batch 0007/0011 | Loss: 3114.1833\n",
      "Epoch: 359/3000 | Batch 0008/0011 | Loss: 3063.9211\n",
      "Epoch: 359/3000 | Batch 0009/0011 | Loss: 3047.1509\n",
      "Epoch: 359/3000 | Batch 0010/0011 | Loss: 3424.4534\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 360/3000 | Batch 0000/0011 | Loss: 3035.4792\n",
      "Epoch: 360/3000 | Batch 0001/0011 | Loss: 3028.1882\n",
      "Epoch: 360/3000 | Batch 0002/0011 | Loss: 3059.5732\n",
      "Epoch: 360/3000 | Batch 0003/0011 | Loss: 3147.9285\n",
      "Epoch: 360/3000 | Batch 0004/0011 | Loss: 3134.8860\n",
      "Epoch: 360/3000 | Batch 0005/0011 | Loss: 3104.3193\n",
      "Epoch: 360/3000 | Batch 0006/0011 | Loss: 3050.3899\n",
      "Epoch: 360/3000 | Batch 0007/0011 | Loss: 3065.8564\n",
      "Epoch: 360/3000 | Batch 0008/0011 | Loss: 3117.1252\n",
      "Epoch: 360/3000 | Batch 0009/0011 | Loss: 3071.0400\n",
      "Epoch: 360/3000 | Batch 0010/0011 | Loss: 3008.5913\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 361/3000 | Batch 0000/0011 | Loss: 3052.1201\n",
      "Epoch: 361/3000 | Batch 0001/0011 | Loss: 3085.0020\n",
      "Epoch: 361/3000 | Batch 0002/0011 | Loss: 3072.0898\n",
      "Epoch: 361/3000 | Batch 0003/0011 | Loss: 3103.4399\n",
      "Epoch: 361/3000 | Batch 0004/0011 | Loss: 3143.9612\n",
      "Epoch: 361/3000 | Batch 0005/0011 | Loss: 3057.9343\n",
      "Epoch: 361/3000 | Batch 0006/0011 | Loss: 3064.8752\n",
      "Epoch: 361/3000 | Batch 0007/0011 | Loss: 3099.2585\n",
      "Epoch: 361/3000 | Batch 0008/0011 | Loss: 3043.7917\n",
      "Epoch: 361/3000 | Batch 0009/0011 | Loss: 3079.1479\n",
      "Epoch: 361/3000 | Batch 0010/0011 | Loss: 2957.6414\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 362/3000 | Batch 0000/0011 | Loss: 3072.5945\n",
      "Epoch: 362/3000 | Batch 0001/0011 | Loss: 3074.9124\n",
      "Epoch: 362/3000 | Batch 0002/0011 | Loss: 3056.4583\n",
      "Epoch: 362/3000 | Batch 0003/0011 | Loss: 3142.3403\n",
      "Epoch: 362/3000 | Batch 0004/0011 | Loss: 3112.2114\n",
      "Epoch: 362/3000 | Batch 0005/0011 | Loss: 3050.5261\n",
      "Epoch: 362/3000 | Batch 0006/0011 | Loss: 3055.2966\n",
      "Epoch: 362/3000 | Batch 0007/0011 | Loss: 3091.7161\n",
      "Epoch: 362/3000 | Batch 0008/0011 | Loss: 3006.8098\n",
      "Epoch: 362/3000 | Batch 0009/0011 | Loss: 3075.9094\n",
      "Epoch: 362/3000 | Batch 0010/0011 | Loss: 3098.7180\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 363/3000 | Batch 0000/0011 | Loss: 3105.3728\n",
      "Epoch: 363/3000 | Batch 0001/0011 | Loss: 3078.6262\n",
      "Epoch: 363/3000 | Batch 0002/0011 | Loss: 3040.7783\n",
      "Epoch: 363/3000 | Batch 0003/0011 | Loss: 3045.8347\n",
      "Epoch: 363/3000 | Batch 0004/0011 | Loss: 3048.9773\n",
      "Epoch: 363/3000 | Batch 0005/0011 | Loss: 3069.1536\n",
      "Epoch: 363/3000 | Batch 0006/0011 | Loss: 3122.5652\n",
      "Epoch: 363/3000 | Batch 0007/0011 | Loss: 3083.7432\n",
      "Epoch: 363/3000 | Batch 0008/0011 | Loss: 3061.7454\n",
      "Epoch: 363/3000 | Batch 0009/0011 | Loss: 3062.5706\n",
      "Epoch: 363/3000 | Batch 0010/0011 | Loss: 3144.5669\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 364/3000 | Batch 0000/0011 | Loss: 3056.4468\n",
      "Epoch: 364/3000 | Batch 0001/0011 | Loss: 3060.5579\n",
      "Epoch: 364/3000 | Batch 0002/0011 | Loss: 3070.3655\n",
      "Epoch: 364/3000 | Batch 0003/0011 | Loss: 3071.8782\n",
      "Epoch: 364/3000 | Batch 0004/0011 | Loss: 2989.3079\n",
      "Epoch: 364/3000 | Batch 0005/0011 | Loss: 3182.7007\n",
      "Epoch: 364/3000 | Batch 0006/0011 | Loss: 3085.8567\n",
      "Epoch: 364/3000 | Batch 0007/0011 | Loss: 3012.9995\n",
      "Epoch: 364/3000 | Batch 0008/0011 | Loss: 3101.8403\n",
      "Epoch: 364/3000 | Batch 0009/0011 | Loss: 3072.8821\n",
      "Epoch: 364/3000 | Batch 0010/0011 | Loss: 3151.5032\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 365/3000 | Batch 0000/0011 | Loss: 3044.9766\n",
      "Epoch: 365/3000 | Batch 0001/0011 | Loss: 3082.4951\n",
      "Epoch: 365/3000 | Batch 0002/0011 | Loss: 3159.4199\n",
      "Epoch: 365/3000 | Batch 0003/0011 | Loss: 3066.5723\n",
      "Epoch: 365/3000 | Batch 0004/0011 | Loss: 3052.4868\n",
      "Epoch: 365/3000 | Batch 0005/0011 | Loss: 3084.2612\n",
      "Epoch: 365/3000 | Batch 0006/0011 | Loss: 3052.8425\n",
      "Epoch: 365/3000 | Batch 0007/0011 | Loss: 3089.9895\n",
      "Epoch: 365/3000 | Batch 0008/0011 | Loss: 3041.5513\n",
      "Epoch: 365/3000 | Batch 0009/0011 | Loss: 3090.5544\n",
      "Epoch: 365/3000 | Batch 0010/0011 | Loss: 3049.1438\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 366/3000 | Batch 0000/0011 | Loss: 3085.9688\n",
      "Epoch: 366/3000 | Batch 0001/0011 | Loss: 3065.8083\n",
      "Epoch: 366/3000 | Batch 0002/0011 | Loss: 3042.1111\n",
      "Epoch: 366/3000 | Batch 0003/0011 | Loss: 3083.3042\n",
      "Epoch: 366/3000 | Batch 0004/0011 | Loss: 3108.9915\n",
      "Epoch: 366/3000 | Batch 0005/0011 | Loss: 3086.5476\n",
      "Epoch: 366/3000 | Batch 0006/0011 | Loss: 3055.0032\n",
      "Epoch: 366/3000 | Batch 0007/0011 | Loss: 3090.7312\n",
      "Epoch: 366/3000 | Batch 0008/0011 | Loss: 3048.7947\n",
      "Epoch: 366/3000 | Batch 0009/0011 | Loss: 3072.4951\n",
      "Epoch: 366/3000 | Batch 0010/0011 | Loss: 3015.6958\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 367/3000 | Batch 0000/0011 | Loss: 3128.0950\n",
      "Epoch: 367/3000 | Batch 0001/0011 | Loss: 3053.1311\n",
      "Epoch: 367/3000 | Batch 0002/0011 | Loss: 3074.0813\n",
      "Epoch: 367/3000 | Batch 0003/0011 | Loss: 3104.5298\n",
      "Epoch: 367/3000 | Batch 0004/0011 | Loss: 3003.3074\n",
      "Epoch: 367/3000 | Batch 0005/0011 | Loss: 3044.0059\n",
      "Epoch: 367/3000 | Batch 0006/0011 | Loss: 3079.5005\n",
      "Epoch: 367/3000 | Batch 0007/0011 | Loss: 3099.6714\n",
      "Epoch: 367/3000 | Batch 0008/0011 | Loss: 3066.7407\n",
      "Epoch: 367/3000 | Batch 0009/0011 | Loss: 3018.5903\n",
      "Epoch: 367/3000 | Batch 0010/0011 | Loss: 2961.5928\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 368/3000 | Batch 0000/0011 | Loss: 3048.1208\n",
      "Epoch: 368/3000 | Batch 0001/0011 | Loss: 3139.7273\n",
      "Epoch: 368/3000 | Batch 0002/0011 | Loss: 3062.9392\n",
      "Epoch: 368/3000 | Batch 0003/0011 | Loss: 3134.5149\n",
      "Epoch: 368/3000 | Batch 0004/0011 | Loss: 3036.5520\n",
      "Epoch: 368/3000 | Batch 0005/0011 | Loss: 3045.7742\n",
      "Epoch: 368/3000 | Batch 0006/0011 | Loss: 3057.1599\n",
      "Epoch: 368/3000 | Batch 0007/0011 | Loss: 3051.0105\n",
      "Epoch: 368/3000 | Batch 0008/0011 | Loss: 3049.6707\n",
      "Epoch: 368/3000 | Batch 0009/0011 | Loss: 3075.9431\n",
      "Epoch: 368/3000 | Batch 0010/0011 | Loss: 3107.8127\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 369/3000 | Batch 0000/0011 | Loss: 3054.3997\n",
      "Epoch: 369/3000 | Batch 0001/0011 | Loss: 3039.5078\n",
      "Epoch: 369/3000 | Batch 0002/0011 | Loss: 3062.2681\n",
      "Epoch: 369/3000 | Batch 0003/0011 | Loss: 3065.8823\n",
      "Epoch: 369/3000 | Batch 0004/0011 | Loss: 3078.2842\n",
      "Epoch: 369/3000 | Batch 0005/0011 | Loss: 3043.6008\n",
      "Epoch: 369/3000 | Batch 0006/0011 | Loss: 3018.8621\n",
      "Epoch: 369/3000 | Batch 0007/0011 | Loss: 3094.5813\n",
      "Epoch: 369/3000 | Batch 0008/0011 | Loss: 3101.9331\n",
      "Epoch: 369/3000 | Batch 0009/0011 | Loss: 3061.3533\n",
      "Epoch: 369/3000 | Batch 0010/0011 | Loss: 3358.8481\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 370/3000 | Batch 0000/0011 | Loss: 3020.1897\n",
      "Epoch: 370/3000 | Batch 0001/0011 | Loss: 3099.0559\n",
      "Epoch: 370/3000 | Batch 0002/0011 | Loss: 3067.9185\n",
      "Epoch: 370/3000 | Batch 0003/0011 | Loss: 3111.5510\n",
      "Epoch: 370/3000 | Batch 0004/0011 | Loss: 3020.2195\n",
      "Epoch: 370/3000 | Batch 0005/0011 | Loss: 3064.1726\n",
      "Epoch: 370/3000 | Batch 0006/0011 | Loss: 3048.8577\n",
      "Epoch: 370/3000 | Batch 0007/0011 | Loss: 3075.6089\n",
      "Epoch: 370/3000 | Batch 0008/0011 | Loss: 3137.1277\n",
      "Epoch: 370/3000 | Batch 0009/0011 | Loss: 3028.2400\n",
      "Epoch: 370/3000 | Batch 0010/0011 | Loss: 2954.1123\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 371/3000 | Batch 0000/0011 | Loss: 3071.0244\n",
      "Epoch: 371/3000 | Batch 0001/0011 | Loss: 3087.1357\n",
      "Epoch: 371/3000 | Batch 0002/0011 | Loss: 3088.5981\n",
      "Epoch: 371/3000 | Batch 0003/0011 | Loss: 3049.2356\n",
      "Epoch: 371/3000 | Batch 0004/0011 | Loss: 3003.0862\n",
      "Epoch: 371/3000 | Batch 0005/0011 | Loss: 3095.8389\n",
      "Epoch: 371/3000 | Batch 0006/0011 | Loss: 3118.0220\n",
      "Epoch: 371/3000 | Batch 0007/0011 | Loss: 2981.8254\n",
      "Epoch: 371/3000 | Batch 0008/0011 | Loss: 3066.5059\n",
      "Epoch: 371/3000 | Batch 0009/0011 | Loss: 3056.9285\n",
      "Epoch: 371/3000 | Batch 0010/0011 | Loss: 3029.9485\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 372/3000 | Batch 0000/0011 | Loss: 3057.1953\n",
      "Epoch: 372/3000 | Batch 0001/0011 | Loss: 3046.1729\n",
      "Epoch: 372/3000 | Batch 0002/0011 | Loss: 3012.9705\n",
      "Epoch: 372/3000 | Batch 0003/0011 | Loss: 3128.6238\n",
      "Epoch: 372/3000 | Batch 0004/0011 | Loss: 3086.8384\n",
      "Epoch: 372/3000 | Batch 0005/0011 | Loss: 3041.8047\n",
      "Epoch: 372/3000 | Batch 0006/0011 | Loss: 3074.7744\n",
      "Epoch: 372/3000 | Batch 0007/0011 | Loss: 3028.8203\n",
      "Epoch: 372/3000 | Batch 0008/0011 | Loss: 3039.3655\n",
      "Epoch: 372/3000 | Batch 0009/0011 | Loss: 3105.9473\n",
      "Epoch: 372/3000 | Batch 0010/0011 | Loss: 3037.5513\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 373/3000 | Batch 0000/0011 | Loss: 3071.5635\n",
      "Epoch: 373/3000 | Batch 0001/0011 | Loss: 3066.8933\n",
      "Epoch: 373/3000 | Batch 0002/0011 | Loss: 3086.5903\n",
      "Epoch: 373/3000 | Batch 0003/0011 | Loss: 3114.9785\n",
      "Epoch: 373/3000 | Batch 0004/0011 | Loss: 3031.6389\n",
      "Epoch: 373/3000 | Batch 0005/0011 | Loss: 3067.1016\n",
      "Epoch: 373/3000 | Batch 0006/0011 | Loss: 3042.7214\n",
      "Epoch: 373/3000 | Batch 0007/0011 | Loss: 3014.4729\n",
      "Epoch: 373/3000 | Batch 0008/0011 | Loss: 3071.7605\n",
      "Epoch: 373/3000 | Batch 0009/0011 | Loss: 3039.1833\n",
      "Epoch: 373/3000 | Batch 0010/0011 | Loss: 3065.5918\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 374/3000 | Batch 0000/0011 | Loss: 3090.4233\n",
      "Epoch: 374/3000 | Batch 0001/0011 | Loss: 3031.5874\n",
      "Epoch: 374/3000 | Batch 0002/0011 | Loss: 3025.9114\n",
      "Epoch: 374/3000 | Batch 0003/0011 | Loss: 2969.0273\n",
      "Epoch: 374/3000 | Batch 0004/0011 | Loss: 3065.3000\n",
      "Epoch: 374/3000 | Batch 0005/0011 | Loss: 3057.3076\n",
      "Epoch: 374/3000 | Batch 0006/0011 | Loss: 3092.5071\n",
      "Epoch: 374/3000 | Batch 0007/0011 | Loss: 3067.1338\n",
      "Epoch: 374/3000 | Batch 0008/0011 | Loss: 3136.0576\n",
      "Epoch: 374/3000 | Batch 0009/0011 | Loss: 3079.2004\n",
      "Epoch: 374/3000 | Batch 0010/0011 | Loss: 2980.4695\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 375/3000 | Batch 0000/0011 | Loss: 3073.7190\n",
      "Epoch: 375/3000 | Batch 0001/0011 | Loss: 3054.5828\n",
      "Epoch: 375/3000 | Batch 0002/0011 | Loss: 3066.6516\n",
      "Epoch: 375/3000 | Batch 0003/0011 | Loss: 3087.9890\n",
      "Epoch: 375/3000 | Batch 0004/0011 | Loss: 3055.2576\n",
      "Epoch: 375/3000 | Batch 0005/0011 | Loss: 3049.4458\n",
      "Epoch: 375/3000 | Batch 0006/0011 | Loss: 3025.7520\n",
      "Epoch: 375/3000 | Batch 0007/0011 | Loss: 3048.2595\n",
      "Epoch: 375/3000 | Batch 0008/0011 | Loss: 3135.8677\n",
      "Epoch: 375/3000 | Batch 0009/0011 | Loss: 3085.6602\n",
      "Epoch: 375/3000 | Batch 0010/0011 | Loss: 2914.7800\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 376/3000 | Batch 0000/0011 | Loss: 3096.7759\n",
      "Epoch: 376/3000 | Batch 0001/0011 | Loss: 3055.2468\n",
      "Epoch: 376/3000 | Batch 0002/0011 | Loss: 3072.9866\n",
      "Epoch: 376/3000 | Batch 0003/0011 | Loss: 3047.3879\n",
      "Epoch: 376/3000 | Batch 0004/0011 | Loss: 3010.4053\n",
      "Epoch: 376/3000 | Batch 0005/0011 | Loss: 3088.5815\n",
      "Epoch: 376/3000 | Batch 0006/0011 | Loss: 3079.7778\n",
      "Epoch: 376/3000 | Batch 0007/0011 | Loss: 3084.9641\n",
      "Epoch: 376/3000 | Batch 0008/0011 | Loss: 3051.2417\n",
      "Epoch: 376/3000 | Batch 0009/0011 | Loss: 3012.0198\n",
      "Epoch: 376/3000 | Batch 0010/0011 | Loss: 3193.0464\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 377/3000 | Batch 0000/0011 | Loss: 3068.7200\n",
      "Epoch: 377/3000 | Batch 0001/0011 | Loss: 3059.2563\n",
      "Epoch: 377/3000 | Batch 0002/0011 | Loss: 3037.7285\n",
      "Epoch: 377/3000 | Batch 0003/0011 | Loss: 3023.0964\n",
      "Epoch: 377/3000 | Batch 0004/0011 | Loss: 3113.9858\n",
      "Epoch: 377/3000 | Batch 0005/0011 | Loss: 3081.6646\n",
      "Epoch: 377/3000 | Batch 0006/0011 | Loss: 3074.4917\n",
      "Epoch: 377/3000 | Batch 0007/0011 | Loss: 3050.6223\n",
      "Epoch: 377/3000 | Batch 0008/0011 | Loss: 3051.9143\n",
      "Epoch: 377/3000 | Batch 0009/0011 | Loss: 3050.9492\n",
      "Epoch: 377/3000 | Batch 0010/0011 | Loss: 3134.8586\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 378/3000 | Batch 0000/0011 | Loss: 2997.6675\n",
      "Epoch: 378/3000 | Batch 0001/0011 | Loss: 3038.7046\n",
      "Epoch: 378/3000 | Batch 0002/0011 | Loss: 3077.1765\n",
      "Epoch: 378/3000 | Batch 0003/0011 | Loss: 3054.7800\n",
      "Epoch: 378/3000 | Batch 0004/0011 | Loss: 3015.2822\n",
      "Epoch: 378/3000 | Batch 0005/0011 | Loss: 3128.4004\n",
      "Epoch: 378/3000 | Batch 0006/0011 | Loss: 3137.1365\n",
      "Epoch: 378/3000 | Batch 0007/0011 | Loss: 3086.6514\n",
      "Epoch: 378/3000 | Batch 0008/0011 | Loss: 3031.3340\n",
      "Epoch: 378/3000 | Batch 0009/0011 | Loss: 3042.1980\n",
      "Epoch: 378/3000 | Batch 0010/0011 | Loss: 3017.8301\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 379/3000 | Batch 0000/0011 | Loss: 3081.9971\n",
      "Epoch: 379/3000 | Batch 0001/0011 | Loss: 3064.9934\n",
      "Epoch: 379/3000 | Batch 0002/0011 | Loss: 3028.6929\n",
      "Epoch: 379/3000 | Batch 0003/0011 | Loss: 3017.1514\n",
      "Epoch: 379/3000 | Batch 0004/0011 | Loss: 3054.1125\n",
      "Epoch: 379/3000 | Batch 0005/0011 | Loss: 3069.4846\n",
      "Epoch: 379/3000 | Batch 0006/0011 | Loss: 3091.8171\n",
      "Epoch: 379/3000 | Batch 0007/0011 | Loss: 2989.0898\n",
      "Epoch: 379/3000 | Batch 0008/0011 | Loss: 3096.9810\n",
      "Epoch: 379/3000 | Batch 0009/0011 | Loss: 3079.6343\n",
      "Epoch: 379/3000 | Batch 0010/0011 | Loss: 3350.7327\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 380/3000 | Batch 0000/0011 | Loss: 2983.9084\n",
      "Epoch: 380/3000 | Batch 0001/0011 | Loss: 3079.7676\n",
      "Epoch: 380/3000 | Batch 0002/0011 | Loss: 3043.4563\n",
      "Epoch: 380/3000 | Batch 0003/0011 | Loss: 3001.2698\n",
      "Epoch: 380/3000 | Batch 0004/0011 | Loss: 3062.0518\n",
      "Epoch: 380/3000 | Batch 0005/0011 | Loss: 3123.3687\n",
      "Epoch: 380/3000 | Batch 0006/0011 | Loss: 3074.0840\n",
      "Epoch: 380/3000 | Batch 0007/0011 | Loss: 3072.9619\n",
      "Epoch: 380/3000 | Batch 0008/0011 | Loss: 3039.8372\n",
      "Epoch: 380/3000 | Batch 0009/0011 | Loss: 3090.6367\n",
      "Epoch: 380/3000 | Batch 0010/0011 | Loss: 3022.5310\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 381/3000 | Batch 0000/0011 | Loss: 3125.3728\n",
      "Epoch: 381/3000 | Batch 0001/0011 | Loss: 3043.6016\n",
      "Epoch: 381/3000 | Batch 0002/0011 | Loss: 3034.7510\n",
      "Epoch: 381/3000 | Batch 0003/0011 | Loss: 3038.6785\n",
      "Epoch: 381/3000 | Batch 0004/0011 | Loss: 3059.9985\n",
      "Epoch: 381/3000 | Batch 0005/0011 | Loss: 3054.3740\n",
      "Epoch: 381/3000 | Batch 0006/0011 | Loss: 3062.0403\n",
      "Epoch: 381/3000 | Batch 0007/0011 | Loss: 3022.9607\n",
      "Epoch: 381/3000 | Batch 0008/0011 | Loss: 3040.9402\n",
      "Epoch: 381/3000 | Batch 0009/0011 | Loss: 3063.4727\n",
      "Epoch: 381/3000 | Batch 0010/0011 | Loss: 3159.2791\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 382/3000 | Batch 0000/0011 | Loss: 3044.1689\n",
      "Epoch: 382/3000 | Batch 0001/0011 | Loss: 3088.1277\n",
      "Epoch: 382/3000 | Batch 0002/0011 | Loss: 3066.6716\n",
      "Epoch: 382/3000 | Batch 0003/0011 | Loss: 3033.5046\n",
      "Epoch: 382/3000 | Batch 0004/0011 | Loss: 3045.6223\n",
      "Epoch: 382/3000 | Batch 0005/0011 | Loss: 3024.6206\n",
      "Epoch: 382/3000 | Batch 0006/0011 | Loss: 3067.0286\n",
      "Epoch: 382/3000 | Batch 0007/0011 | Loss: 3056.0430\n",
      "Epoch: 382/3000 | Batch 0008/0011 | Loss: 3067.6399\n",
      "Epoch: 382/3000 | Batch 0009/0011 | Loss: 3102.2996\n",
      "Epoch: 382/3000 | Batch 0010/0011 | Loss: 3059.0920\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 383/3000 | Batch 0000/0011 | Loss: 3080.0959\n",
      "Epoch: 383/3000 | Batch 0001/0011 | Loss: 3055.0278\n",
      "Epoch: 383/3000 | Batch 0002/0011 | Loss: 3030.0784\n",
      "Epoch: 383/3000 | Batch 0003/0011 | Loss: 2978.5469\n",
      "Epoch: 383/3000 | Batch 0004/0011 | Loss: 3074.5176\n",
      "Epoch: 383/3000 | Batch 0005/0011 | Loss: 3001.1226\n",
      "Epoch: 383/3000 | Batch 0006/0011 | Loss: 3075.9719\n",
      "Epoch: 383/3000 | Batch 0007/0011 | Loss: 3047.2800\n",
      "Epoch: 383/3000 | Batch 0008/0011 | Loss: 3119.3516\n",
      "Epoch: 383/3000 | Batch 0009/0011 | Loss: 3068.7981\n",
      "Epoch: 383/3000 | Batch 0010/0011 | Loss: 3123.3020\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 384/3000 | Batch 0000/0011 | Loss: 3023.7366\n",
      "Epoch: 384/3000 | Batch 0001/0011 | Loss: 3062.6101\n",
      "Epoch: 384/3000 | Batch 0002/0011 | Loss: 3033.0327\n",
      "Epoch: 384/3000 | Batch 0003/0011 | Loss: 3031.0439\n",
      "Epoch: 384/3000 | Batch 0004/0011 | Loss: 3077.8552\n",
      "Epoch: 384/3000 | Batch 0005/0011 | Loss: 3053.5952\n",
      "Epoch: 384/3000 | Batch 0006/0011 | Loss: 3036.6543\n",
      "Epoch: 384/3000 | Batch 0007/0011 | Loss: 3043.9768\n",
      "Epoch: 384/3000 | Batch 0008/0011 | Loss: 3097.3953\n",
      "Epoch: 384/3000 | Batch 0009/0011 | Loss: 3100.3911\n",
      "Epoch: 384/3000 | Batch 0010/0011 | Loss: 3081.0645\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 385/3000 | Batch 0000/0011 | Loss: 3042.4575\n",
      "Epoch: 385/3000 | Batch 0001/0011 | Loss: 3068.8914\n",
      "Epoch: 385/3000 | Batch 0002/0011 | Loss: 3083.7561\n",
      "Epoch: 385/3000 | Batch 0003/0011 | Loss: 2989.8027\n",
      "Epoch: 385/3000 | Batch 0004/0011 | Loss: 3021.8108\n",
      "Epoch: 385/3000 | Batch 0005/0011 | Loss: 3141.6765\n",
      "Epoch: 385/3000 | Batch 0006/0011 | Loss: 3109.3894\n",
      "Epoch: 385/3000 | Batch 0007/0011 | Loss: 2995.4116\n",
      "Epoch: 385/3000 | Batch 0008/0011 | Loss: 3049.7417\n",
      "Epoch: 385/3000 | Batch 0009/0011 | Loss: 3043.9541\n",
      "Epoch: 385/3000 | Batch 0010/0011 | Loss: 3031.4221\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 386/3000 | Batch 0000/0011 | Loss: 3029.8936\n",
      "Epoch: 386/3000 | Batch 0001/0011 | Loss: 3115.4109\n",
      "Epoch: 386/3000 | Batch 0002/0011 | Loss: 3006.1018\n",
      "Epoch: 386/3000 | Batch 0003/0011 | Loss: 3064.8850\n",
      "Epoch: 386/3000 | Batch 0004/0011 | Loss: 3030.7109\n",
      "Epoch: 386/3000 | Batch 0005/0011 | Loss: 3035.4580\n",
      "Epoch: 386/3000 | Batch 0006/0011 | Loss: 3029.8774\n",
      "Epoch: 386/3000 | Batch 0007/0011 | Loss: 3064.1643\n",
      "Epoch: 386/3000 | Batch 0008/0011 | Loss: 3072.1455\n",
      "Epoch: 386/3000 | Batch 0009/0011 | Loss: 3074.0562\n",
      "Epoch: 386/3000 | Batch 0010/0011 | Loss: 3050.4897\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 387/3000 | Batch 0000/0011 | Loss: 3024.0129\n",
      "Epoch: 387/3000 | Batch 0001/0011 | Loss: 3107.1511\n",
      "Epoch: 387/3000 | Batch 0002/0011 | Loss: 3079.6873\n",
      "Epoch: 387/3000 | Batch 0003/0011 | Loss: 3028.0669\n",
      "Epoch: 387/3000 | Batch 0004/0011 | Loss: 3072.2686\n",
      "Epoch: 387/3000 | Batch 0005/0011 | Loss: 3021.6758\n",
      "Epoch: 387/3000 | Batch 0006/0011 | Loss: 3065.6821\n",
      "Epoch: 387/3000 | Batch 0007/0011 | Loss: 3072.5161\n",
      "Epoch: 387/3000 | Batch 0008/0011 | Loss: 3063.7722\n",
      "Epoch: 387/3000 | Batch 0009/0011 | Loss: 3026.1040\n",
      "Epoch: 387/3000 | Batch 0010/0011 | Loss: 3065.4875\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 388/3000 | Batch 0000/0011 | Loss: 3081.6877\n",
      "Epoch: 388/3000 | Batch 0001/0011 | Loss: 3010.9829\n",
      "Epoch: 388/3000 | Batch 0002/0011 | Loss: 3078.3992\n",
      "Epoch: 388/3000 | Batch 0003/0011 | Loss: 3085.1824\n",
      "Epoch: 388/3000 | Batch 0004/0011 | Loss: 3053.8999\n",
      "Epoch: 388/3000 | Batch 0005/0011 | Loss: 3006.6538\n",
      "Epoch: 388/3000 | Batch 0006/0011 | Loss: 3044.9658\n",
      "Epoch: 388/3000 | Batch 0007/0011 | Loss: 2999.9604\n",
      "Epoch: 388/3000 | Batch 0008/0011 | Loss: 3116.5454\n",
      "Epoch: 388/3000 | Batch 0009/0011 | Loss: 3077.9546\n",
      "Epoch: 388/3000 | Batch 0010/0011 | Loss: 2962.2490\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 389/3000 | Batch 0000/0011 | Loss: 3050.9631\n",
      "Epoch: 389/3000 | Batch 0001/0011 | Loss: 3080.6453\n",
      "Epoch: 389/3000 | Batch 0002/0011 | Loss: 3045.9609\n",
      "Epoch: 389/3000 | Batch 0003/0011 | Loss: 3064.1992\n",
      "Epoch: 389/3000 | Batch 0004/0011 | Loss: 3011.4060\n",
      "Epoch: 389/3000 | Batch 0005/0011 | Loss: 3045.3799\n",
      "Epoch: 389/3000 | Batch 0006/0011 | Loss: 3020.3784\n",
      "Epoch: 389/3000 | Batch 0007/0011 | Loss: 3061.2549\n",
      "Epoch: 389/3000 | Batch 0008/0011 | Loss: 3068.9832\n",
      "Epoch: 389/3000 | Batch 0009/0011 | Loss: 3110.4968\n",
      "Epoch: 389/3000 | Batch 0010/0011 | Loss: 3049.3149\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 390/3000 | Batch 0000/0011 | Loss: 3018.1746\n",
      "Epoch: 390/3000 | Batch 0001/0011 | Loss: 3032.0715\n",
      "Epoch: 390/3000 | Batch 0002/0011 | Loss: 3042.9050\n",
      "Epoch: 390/3000 | Batch 0003/0011 | Loss: 3038.6047\n",
      "Epoch: 390/3000 | Batch 0004/0011 | Loss: 3047.8689\n",
      "Epoch: 390/3000 | Batch 0005/0011 | Loss: 3092.8506\n",
      "Epoch: 390/3000 | Batch 0006/0011 | Loss: 3132.3662\n",
      "Epoch: 390/3000 | Batch 0007/0011 | Loss: 3082.5596\n",
      "Epoch: 390/3000 | Batch 0008/0011 | Loss: 3038.0564\n",
      "Epoch: 390/3000 | Batch 0009/0011 | Loss: 2982.6372\n",
      "Epoch: 390/3000 | Batch 0010/0011 | Loss: 3019.9160\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 391/3000 | Batch 0000/0011 | Loss: 3051.1106\n",
      "Epoch: 391/3000 | Batch 0001/0011 | Loss: 2992.4954\n",
      "Epoch: 391/3000 | Batch 0002/0011 | Loss: 3063.8245\n",
      "Epoch: 391/3000 | Batch 0003/0011 | Loss: 3035.6279\n",
      "Epoch: 391/3000 | Batch 0004/0011 | Loss: 3040.6072\n",
      "Epoch: 391/3000 | Batch 0005/0011 | Loss: 3083.6797\n",
      "Epoch: 391/3000 | Batch 0006/0011 | Loss: 3050.6577\n",
      "Epoch: 391/3000 | Batch 0007/0011 | Loss: 3040.3091\n",
      "Epoch: 391/3000 | Batch 0008/0011 | Loss: 3024.7649\n",
      "Epoch: 391/3000 | Batch 0009/0011 | Loss: 3151.1909\n",
      "Epoch: 391/3000 | Batch 0010/0011 | Loss: 2956.4646\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 392/3000 | Batch 0000/0011 | Loss: 3040.3083\n",
      "Epoch: 392/3000 | Batch 0001/0011 | Loss: 3009.1482\n",
      "Epoch: 392/3000 | Batch 0002/0011 | Loss: 3145.4148\n",
      "Epoch: 392/3000 | Batch 0003/0011 | Loss: 3110.8254\n",
      "Epoch: 392/3000 | Batch 0004/0011 | Loss: 3023.9900\n",
      "Epoch: 392/3000 | Batch 0005/0011 | Loss: 3068.9731\n",
      "Epoch: 392/3000 | Batch 0006/0011 | Loss: 3016.6016\n",
      "Epoch: 392/3000 | Batch 0007/0011 | Loss: 3029.4050\n",
      "Epoch: 392/3000 | Batch 0008/0011 | Loss: 3031.2166\n",
      "Epoch: 392/3000 | Batch 0009/0011 | Loss: 3039.4038\n",
      "Epoch: 392/3000 | Batch 0010/0011 | Loss: 2996.9707\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 393/3000 | Batch 0000/0011 | Loss: 3015.8403\n",
      "Epoch: 393/3000 | Batch 0001/0011 | Loss: 3159.5796\n",
      "Epoch: 393/3000 | Batch 0002/0011 | Loss: 3047.9634\n",
      "Epoch: 393/3000 | Batch 0003/0011 | Loss: 3082.1423\n",
      "Epoch: 393/3000 | Batch 0004/0011 | Loss: 3036.0813\n",
      "Epoch: 393/3000 | Batch 0005/0011 | Loss: 3002.8157\n",
      "Epoch: 393/3000 | Batch 0006/0011 | Loss: 3075.1392\n",
      "Epoch: 393/3000 | Batch 0007/0011 | Loss: 3047.4299\n",
      "Epoch: 393/3000 | Batch 0008/0011 | Loss: 3034.5100\n",
      "Epoch: 393/3000 | Batch 0009/0011 | Loss: 3028.3533\n",
      "Epoch: 393/3000 | Batch 0010/0011 | Loss: 3058.9607\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 394/3000 | Batch 0000/0011 | Loss: 3002.6899\n",
      "Epoch: 394/3000 | Batch 0001/0011 | Loss: 2996.4590\n",
      "Epoch: 394/3000 | Batch 0002/0011 | Loss: 3047.4119\n",
      "Epoch: 394/3000 | Batch 0003/0011 | Loss: 3066.0005\n",
      "Epoch: 394/3000 | Batch 0004/0011 | Loss: 3059.2810\n",
      "Epoch: 394/3000 | Batch 0005/0011 | Loss: 3072.0737\n",
      "Epoch: 394/3000 | Batch 0006/0011 | Loss: 3063.5217\n",
      "Epoch: 394/3000 | Batch 0007/0011 | Loss: 3098.3340\n",
      "Epoch: 394/3000 | Batch 0008/0011 | Loss: 3090.9768\n",
      "Epoch: 394/3000 | Batch 0009/0011 | Loss: 2992.3215\n",
      "Epoch: 394/3000 | Batch 0010/0011 | Loss: 3176.5203\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 395/3000 | Batch 0000/0011 | Loss: 3073.5132\n",
      "Epoch: 395/3000 | Batch 0001/0011 | Loss: 3034.7666\n",
      "Epoch: 395/3000 | Batch 0002/0011 | Loss: 3082.0295\n",
      "Epoch: 395/3000 | Batch 0003/0011 | Loss: 3001.2769\n",
      "Epoch: 395/3000 | Batch 0004/0011 | Loss: 3082.3445\n",
      "Epoch: 395/3000 | Batch 0005/0011 | Loss: 3002.5596\n",
      "Epoch: 395/3000 | Batch 0006/0011 | Loss: 3008.2378\n",
      "Epoch: 395/3000 | Batch 0007/0011 | Loss: 3046.1123\n",
      "Epoch: 395/3000 | Batch 0008/0011 | Loss: 3105.1494\n",
      "Epoch: 395/3000 | Batch 0009/0011 | Loss: 3059.3433\n",
      "Epoch: 395/3000 | Batch 0010/0011 | Loss: 3263.7151\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 396/3000 | Batch 0000/0011 | Loss: 3045.8262\n",
      "Epoch: 396/3000 | Batch 0001/0011 | Loss: 3104.2122\n",
      "Epoch: 396/3000 | Batch 0002/0011 | Loss: 3051.3145\n",
      "Epoch: 396/3000 | Batch 0003/0011 | Loss: 3008.3120\n",
      "Epoch: 396/3000 | Batch 0004/0011 | Loss: 3045.7302\n",
      "Epoch: 396/3000 | Batch 0005/0011 | Loss: 3017.3704\n",
      "Epoch: 396/3000 | Batch 0006/0011 | Loss: 3073.3813\n",
      "Epoch: 396/3000 | Batch 0007/0011 | Loss: 3046.7732\n",
      "Epoch: 396/3000 | Batch 0008/0011 | Loss: 3065.3494\n",
      "Epoch: 396/3000 | Batch 0009/0011 | Loss: 3019.2083\n",
      "Epoch: 396/3000 | Batch 0010/0011 | Loss: 3112.8826\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 397/3000 | Batch 0000/0011 | Loss: 3062.1948\n",
      "Epoch: 397/3000 | Batch 0001/0011 | Loss: 3049.9648\n",
      "Epoch: 397/3000 | Batch 0002/0011 | Loss: 3036.4446\n",
      "Epoch: 397/3000 | Batch 0003/0011 | Loss: 3016.1021\n",
      "Epoch: 397/3000 | Batch 0004/0011 | Loss: 3078.7607\n",
      "Epoch: 397/3000 | Batch 0005/0011 | Loss: 3060.3008\n",
      "Epoch: 397/3000 | Batch 0006/0011 | Loss: 3103.2463\n",
      "Epoch: 397/3000 | Batch 0007/0011 | Loss: 3025.9187\n",
      "Epoch: 397/3000 | Batch 0008/0011 | Loss: 3029.2349\n",
      "Epoch: 397/3000 | Batch 0009/0011 | Loss: 3077.4919\n",
      "Epoch: 397/3000 | Batch 0010/0011 | Loss: 3007.9009\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 398/3000 | Batch 0000/0011 | Loss: 2994.0396\n",
      "Epoch: 398/3000 | Batch 0001/0011 | Loss: 3030.2598\n",
      "Epoch: 398/3000 | Batch 0002/0011 | Loss: 3041.7939\n",
      "Epoch: 398/3000 | Batch 0003/0011 | Loss: 3062.5989\n",
      "Epoch: 398/3000 | Batch 0004/0011 | Loss: 3055.1304\n",
      "Epoch: 398/3000 | Batch 0005/0011 | Loss: 3097.8809\n",
      "Epoch: 398/3000 | Batch 0006/0011 | Loss: 3049.5974\n",
      "Epoch: 398/3000 | Batch 0007/0011 | Loss: 3074.3691\n",
      "Epoch: 398/3000 | Batch 0008/0011 | Loss: 3076.1372\n",
      "Epoch: 398/3000 | Batch 0009/0011 | Loss: 3032.2188\n",
      "Epoch: 398/3000 | Batch 0010/0011 | Loss: 2997.8840\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 399/3000 | Batch 0000/0011 | Loss: 3039.2217\n",
      "Epoch: 399/3000 | Batch 0001/0011 | Loss: 3033.7236\n",
      "Epoch: 399/3000 | Batch 0002/0011 | Loss: 3056.4871\n",
      "Epoch: 399/3000 | Batch 0003/0011 | Loss: 3093.7454\n",
      "Epoch: 399/3000 | Batch 0004/0011 | Loss: 2987.3425\n",
      "Epoch: 399/3000 | Batch 0005/0011 | Loss: 3118.3132\n",
      "Epoch: 399/3000 | Batch 0006/0011 | Loss: 3073.2773\n",
      "Epoch: 399/3000 | Batch 0007/0011 | Loss: 3050.0352\n",
      "Epoch: 399/3000 | Batch 0008/0011 | Loss: 3035.9734\n",
      "Epoch: 399/3000 | Batch 0009/0011 | Loss: 3029.5059\n",
      "Epoch: 399/3000 | Batch 0010/0011 | Loss: 3043.7832\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 400/3000 | Batch 0000/0011 | Loss: 2980.9836\n",
      "Epoch: 400/3000 | Batch 0001/0011 | Loss: 3102.3457\n",
      "Epoch: 400/3000 | Batch 0002/0011 | Loss: 3031.3987\n",
      "Epoch: 400/3000 | Batch 0003/0011 | Loss: 3137.2954\n",
      "Epoch: 400/3000 | Batch 0004/0011 | Loss: 3011.6760\n",
      "Epoch: 400/3000 | Batch 0005/0011 | Loss: 3011.7957\n",
      "Epoch: 400/3000 | Batch 0006/0011 | Loss: 2988.8999\n",
      "Epoch: 400/3000 | Batch 0007/0011 | Loss: 3068.5964\n",
      "Epoch: 400/3000 | Batch 0008/0011 | Loss: 3133.0110\n",
      "Epoch: 400/3000 | Batch 0009/0011 | Loss: 3058.0510\n",
      "Epoch: 400/3000 | Batch 0010/0011 | Loss: 2884.0857\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 401/3000 | Batch 0000/0011 | Loss: 3002.3523\n",
      "Epoch: 401/3000 | Batch 0001/0011 | Loss: 3040.8948\n",
      "Epoch: 401/3000 | Batch 0002/0011 | Loss: 3089.7869\n",
      "Epoch: 401/3000 | Batch 0003/0011 | Loss: 3024.7158\n",
      "Epoch: 401/3000 | Batch 0004/0011 | Loss: 2999.4705\n",
      "Epoch: 401/3000 | Batch 0005/0011 | Loss: 3059.0623\n",
      "Epoch: 401/3000 | Batch 0006/0011 | Loss: 3070.9399\n",
      "Epoch: 401/3000 | Batch 0007/0011 | Loss: 3111.6855\n",
      "Epoch: 401/3000 | Batch 0008/0011 | Loss: 3062.4492\n",
      "Epoch: 401/3000 | Batch 0009/0011 | Loss: 3039.2256\n",
      "Epoch: 401/3000 | Batch 0010/0011 | Loss: 3005.7314\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 402/3000 | Batch 0000/0011 | Loss: 3006.5984\n",
      "Epoch: 402/3000 | Batch 0001/0011 | Loss: 3016.9446\n",
      "Epoch: 402/3000 | Batch 0002/0011 | Loss: 3124.8157\n",
      "Epoch: 402/3000 | Batch 0003/0011 | Loss: 3118.4060\n",
      "Epoch: 402/3000 | Batch 0004/0011 | Loss: 3024.4519\n",
      "Epoch: 402/3000 | Batch 0005/0011 | Loss: 3037.0986\n",
      "Epoch: 402/3000 | Batch 0006/0011 | Loss: 3037.7820\n",
      "Epoch: 402/3000 | Batch 0007/0011 | Loss: 2985.9277\n",
      "Epoch: 402/3000 | Batch 0008/0011 | Loss: 3039.8770\n",
      "Epoch: 402/3000 | Batch 0009/0011 | Loss: 3075.6057\n",
      "Epoch: 402/3000 | Batch 0010/0011 | Loss: 3044.0403\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 403/3000 | Batch 0000/0011 | Loss: 3074.5103\n",
      "Epoch: 403/3000 | Batch 0001/0011 | Loss: 3061.2468\n",
      "Epoch: 403/3000 | Batch 0002/0011 | Loss: 3018.4067\n",
      "Epoch: 403/3000 | Batch 0003/0011 | Loss: 2997.4536\n",
      "Epoch: 403/3000 | Batch 0004/0011 | Loss: 3060.7170\n",
      "Epoch: 403/3000 | Batch 0005/0011 | Loss: 3112.6516\n",
      "Epoch: 403/3000 | Batch 0006/0011 | Loss: 2979.3999\n",
      "Epoch: 403/3000 | Batch 0007/0011 | Loss: 3092.6831\n",
      "Epoch: 403/3000 | Batch 0008/0011 | Loss: 3008.7373\n",
      "Epoch: 403/3000 | Batch 0009/0011 | Loss: 3037.6870\n",
      "Epoch: 403/3000 | Batch 0010/0011 | Loss: 3185.1584\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 404/3000 | Batch 0000/0011 | Loss: 3060.8638\n",
      "Epoch: 404/3000 | Batch 0001/0011 | Loss: 3045.4214\n",
      "Epoch: 404/3000 | Batch 0002/0011 | Loss: 3055.3828\n",
      "Epoch: 404/3000 | Batch 0003/0011 | Loss: 3039.9243\n",
      "Epoch: 404/3000 | Batch 0004/0011 | Loss: 3021.4553\n",
      "Epoch: 404/3000 | Batch 0005/0011 | Loss: 3038.9158\n",
      "Epoch: 404/3000 | Batch 0006/0011 | Loss: 3037.7476\n",
      "Epoch: 404/3000 | Batch 0007/0011 | Loss: 3164.1626\n",
      "Epoch: 404/3000 | Batch 0008/0011 | Loss: 3016.4011\n",
      "Epoch: 404/3000 | Batch 0009/0011 | Loss: 3019.9810\n",
      "Epoch: 404/3000 | Batch 0010/0011 | Loss: 3140.5720\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 405/3000 | Batch 0000/0011 | Loss: 3046.7390\n",
      "Epoch: 405/3000 | Batch 0001/0011 | Loss: 3105.3003\n",
      "Epoch: 405/3000 | Batch 0002/0011 | Loss: 3109.0967\n",
      "Epoch: 405/3000 | Batch 0003/0011 | Loss: 3045.7454\n",
      "Epoch: 405/3000 | Batch 0004/0011 | Loss: 2971.2354\n",
      "Epoch: 405/3000 | Batch 0005/0011 | Loss: 3050.1975\n",
      "Epoch: 405/3000 | Batch 0006/0011 | Loss: 3017.5408\n",
      "Epoch: 405/3000 | Batch 0007/0011 | Loss: 3080.3723\n",
      "Epoch: 405/3000 | Batch 0008/0011 | Loss: 3088.6658\n",
      "Epoch: 405/3000 | Batch 0009/0011 | Loss: 2997.1455\n",
      "Epoch: 405/3000 | Batch 0010/0011 | Loss: 2903.4937\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 406/3000 | Batch 0000/0011 | Loss: 3066.5752\n",
      "Epoch: 406/3000 | Batch 0001/0011 | Loss: 3029.9224\n",
      "Epoch: 406/3000 | Batch 0002/0011 | Loss: 2989.7632\n",
      "Epoch: 406/3000 | Batch 0003/0011 | Loss: 3114.7993\n",
      "Epoch: 406/3000 | Batch 0004/0011 | Loss: 3114.9136\n",
      "Epoch: 406/3000 | Batch 0005/0011 | Loss: 3018.7068\n",
      "Epoch: 406/3000 | Batch 0006/0011 | Loss: 3044.0938\n",
      "Epoch: 406/3000 | Batch 0007/0011 | Loss: 2967.5837\n",
      "Epoch: 406/3000 | Batch 0008/0011 | Loss: 3053.5610\n",
      "Epoch: 406/3000 | Batch 0009/0011 | Loss: 3076.8381\n",
      "Epoch: 406/3000 | Batch 0010/0011 | Loss: 3059.7927\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 407/3000 | Batch 0000/0011 | Loss: 3055.8635\n",
      "Epoch: 407/3000 | Batch 0001/0011 | Loss: 3041.5894\n",
      "Epoch: 407/3000 | Batch 0002/0011 | Loss: 3022.5144\n",
      "Epoch: 407/3000 | Batch 0003/0011 | Loss: 3099.8491\n",
      "Epoch: 407/3000 | Batch 0004/0011 | Loss: 3033.8091\n",
      "Epoch: 407/3000 | Batch 0005/0011 | Loss: 3064.7175\n",
      "Epoch: 407/3000 | Batch 0006/0011 | Loss: 3093.1370\n",
      "Epoch: 407/3000 | Batch 0007/0011 | Loss: 3045.1038\n",
      "Epoch: 407/3000 | Batch 0008/0011 | Loss: 3048.5222\n",
      "Epoch: 407/3000 | Batch 0009/0011 | Loss: 3018.3992\n",
      "Epoch: 407/3000 | Batch 0010/0011 | Loss: 2967.5833\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 408/3000 | Batch 0000/0011 | Loss: 3062.0186\n",
      "Epoch: 408/3000 | Batch 0001/0011 | Loss: 3083.2866\n",
      "Epoch: 408/3000 | Batch 0002/0011 | Loss: 3024.7969\n",
      "Epoch: 408/3000 | Batch 0003/0011 | Loss: 3046.7266\n",
      "Epoch: 408/3000 | Batch 0004/0011 | Loss: 3000.2678\n",
      "Epoch: 408/3000 | Batch 0005/0011 | Loss: 3047.4221\n",
      "Epoch: 408/3000 | Batch 0006/0011 | Loss: 3072.4382\n",
      "Epoch: 408/3000 | Batch 0007/0011 | Loss: 3034.4287\n",
      "Epoch: 408/3000 | Batch 0008/0011 | Loss: 3082.7866\n",
      "Epoch: 408/3000 | Batch 0009/0011 | Loss: 3046.1721\n",
      "Epoch: 408/3000 | Batch 0010/0011 | Loss: 2948.2786\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 409/3000 | Batch 0000/0011 | Loss: 3010.5967\n",
      "Epoch: 409/3000 | Batch 0001/0011 | Loss: 3055.1558\n",
      "Epoch: 409/3000 | Batch 0002/0011 | Loss: 3022.8899\n",
      "Epoch: 409/3000 | Batch 0003/0011 | Loss: 3021.4280\n",
      "Epoch: 409/3000 | Batch 0004/0011 | Loss: 3056.2029\n",
      "Epoch: 409/3000 | Batch 0005/0011 | Loss: 3041.7405\n",
      "Epoch: 409/3000 | Batch 0006/0011 | Loss: 3085.0205\n",
      "Epoch: 409/3000 | Batch 0007/0011 | Loss: 3008.4790\n",
      "Epoch: 409/3000 | Batch 0008/0011 | Loss: 3095.4539\n",
      "Epoch: 409/3000 | Batch 0009/0011 | Loss: 3072.0115\n",
      "Epoch: 409/3000 | Batch 0010/0011 | Loss: 3033.0586\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 410/3000 | Batch 0000/0011 | Loss: 3138.6230\n",
      "Epoch: 410/3000 | Batch 0001/0011 | Loss: 3064.6653\n",
      "Epoch: 410/3000 | Batch 0002/0011 | Loss: 3023.4971\n",
      "Epoch: 410/3000 | Batch 0003/0011 | Loss: 3017.6570\n",
      "Epoch: 410/3000 | Batch 0004/0011 | Loss: 3030.7861\n",
      "Epoch: 410/3000 | Batch 0005/0011 | Loss: 3029.8120\n",
      "Epoch: 410/3000 | Batch 0006/0011 | Loss: 3052.5332\n",
      "Epoch: 410/3000 | Batch 0007/0011 | Loss: 3041.4678\n",
      "Epoch: 410/3000 | Batch 0008/0011 | Loss: 3135.8660\n",
      "Epoch: 410/3000 | Batch 0009/0011 | Loss: 2977.7693\n",
      "Epoch: 410/3000 | Batch 0010/0011 | Loss: 2974.3616\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 411/3000 | Batch 0000/0011 | Loss: 3061.4338\n",
      "Epoch: 411/3000 | Batch 0001/0011 | Loss: 3089.5696\n",
      "Epoch: 411/3000 | Batch 0002/0011 | Loss: 3078.7639\n",
      "Epoch: 411/3000 | Batch 0003/0011 | Loss: 3001.9590\n",
      "Epoch: 411/3000 | Batch 0004/0011 | Loss: 2997.5647\n",
      "Epoch: 411/3000 | Batch 0005/0011 | Loss: 3026.4600\n",
      "Epoch: 411/3000 | Batch 0006/0011 | Loss: 3056.2627\n",
      "Epoch: 411/3000 | Batch 0007/0011 | Loss: 3059.8064\n",
      "Epoch: 411/3000 | Batch 0008/0011 | Loss: 3106.6414\n",
      "Epoch: 411/3000 | Batch 0009/0011 | Loss: 3071.4998\n",
      "Epoch: 411/3000 | Batch 0010/0011 | Loss: 2881.4873\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 412/3000 | Batch 0000/0011 | Loss: 3092.9563\n",
      "Epoch: 412/3000 | Batch 0001/0011 | Loss: 3023.3704\n",
      "Epoch: 412/3000 | Batch 0002/0011 | Loss: 3020.0249\n",
      "Epoch: 412/3000 | Batch 0003/0011 | Loss: 3064.0110\n",
      "Epoch: 412/3000 | Batch 0004/0011 | Loss: 3042.8162\n",
      "Epoch: 412/3000 | Batch 0005/0011 | Loss: 3037.2168\n",
      "Epoch: 412/3000 | Batch 0006/0011 | Loss: 3050.0225\n",
      "Epoch: 412/3000 | Batch 0007/0011 | Loss: 3077.5024\n",
      "Epoch: 412/3000 | Batch 0008/0011 | Loss: 3040.4290\n",
      "Epoch: 412/3000 | Batch 0009/0011 | Loss: 3021.6270\n",
      "Epoch: 412/3000 | Batch 0010/0011 | Loss: 3015.4517\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 413/3000 | Batch 0000/0011 | Loss: 3078.0977\n",
      "Epoch: 413/3000 | Batch 0001/0011 | Loss: 3046.6235\n",
      "Epoch: 413/3000 | Batch 0002/0011 | Loss: 2983.0779\n",
      "Epoch: 413/3000 | Batch 0003/0011 | Loss: 3025.8103\n",
      "Epoch: 413/3000 | Batch 0004/0011 | Loss: 3115.8672\n",
      "Epoch: 413/3000 | Batch 0005/0011 | Loss: 2988.8296\n",
      "Epoch: 413/3000 | Batch 0006/0011 | Loss: 3022.2703\n",
      "Epoch: 413/3000 | Batch 0007/0011 | Loss: 3023.0754\n",
      "Epoch: 413/3000 | Batch 0008/0011 | Loss: 3065.7573\n",
      "Epoch: 413/3000 | Batch 0009/0011 | Loss: 3076.7244\n",
      "Epoch: 413/3000 | Batch 0010/0011 | Loss: 3134.8635\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 414/3000 | Batch 0000/0011 | Loss: 3019.0854\n",
      "Epoch: 414/3000 | Batch 0001/0011 | Loss: 2986.1157\n",
      "Epoch: 414/3000 | Batch 0002/0011 | Loss: 3073.2737\n",
      "Epoch: 414/3000 | Batch 0003/0011 | Loss: 3023.4006\n",
      "Epoch: 414/3000 | Batch 0004/0011 | Loss: 3019.8884\n",
      "Epoch: 414/3000 | Batch 0005/0011 | Loss: 3041.2324\n",
      "Epoch: 414/3000 | Batch 0006/0011 | Loss: 3070.7007\n",
      "Epoch: 414/3000 | Batch 0007/0011 | Loss: 3073.1780\n",
      "Epoch: 414/3000 | Batch 0008/0011 | Loss: 3026.2031\n",
      "Epoch: 414/3000 | Batch 0009/0011 | Loss: 3031.3406\n",
      "Epoch: 414/3000 | Batch 0010/0011 | Loss: 3466.7957\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 415/3000 | Batch 0000/0011 | Loss: 3041.1003\n",
      "Epoch: 415/3000 | Batch 0001/0011 | Loss: 3074.8958\n",
      "Epoch: 415/3000 | Batch 0002/0011 | Loss: 3047.2776\n",
      "Epoch: 415/3000 | Batch 0003/0011 | Loss: 3059.0369\n",
      "Epoch: 415/3000 | Batch 0004/0011 | Loss: 3037.2896\n",
      "Epoch: 415/3000 | Batch 0005/0011 | Loss: 3067.9707\n",
      "Epoch: 415/3000 | Batch 0006/0011 | Loss: 3049.0276\n",
      "Epoch: 415/3000 | Batch 0007/0011 | Loss: 3042.2793\n",
      "Epoch: 415/3000 | Batch 0008/0011 | Loss: 3079.3647\n",
      "Epoch: 415/3000 | Batch 0009/0011 | Loss: 3061.9348\n",
      "Epoch: 415/3000 | Batch 0010/0011 | Loss: 3129.1052\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 416/3000 | Batch 0000/0011 | Loss: 3082.4512\n",
      "Epoch: 416/3000 | Batch 0001/0011 | Loss: 3095.9094\n",
      "Epoch: 416/3000 | Batch 0002/0011 | Loss: 3080.2180\n",
      "Epoch: 416/3000 | Batch 0003/0011 | Loss: 3008.2461\n",
      "Epoch: 416/3000 | Batch 0004/0011 | Loss: 3041.7190\n",
      "Epoch: 416/3000 | Batch 0005/0011 | Loss: 3061.5339\n",
      "Epoch: 416/3000 | Batch 0006/0011 | Loss: 3045.0518\n",
      "Epoch: 416/3000 | Batch 0007/0011 | Loss: 3070.8074\n",
      "Epoch: 416/3000 | Batch 0008/0011 | Loss: 3057.2930\n",
      "Epoch: 416/3000 | Batch 0009/0011 | Loss: 3052.1895\n",
      "Epoch: 416/3000 | Batch 0010/0011 | Loss: 3035.4329\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 417/3000 | Batch 0000/0011 | Loss: 3020.4592\n",
      "Epoch: 417/3000 | Batch 0001/0011 | Loss: 3026.8215\n",
      "Epoch: 417/3000 | Batch 0002/0011 | Loss: 3051.9968\n",
      "Epoch: 417/3000 | Batch 0003/0011 | Loss: 3065.0320\n",
      "Epoch: 417/3000 | Batch 0004/0011 | Loss: 3067.6895\n",
      "Epoch: 417/3000 | Batch 0005/0011 | Loss: 2996.5803\n",
      "Epoch: 417/3000 | Batch 0006/0011 | Loss: 3101.7000\n",
      "Epoch: 417/3000 | Batch 0007/0011 | Loss: 3078.8699\n",
      "Epoch: 417/3000 | Batch 0008/0011 | Loss: 3086.6707\n",
      "Epoch: 417/3000 | Batch 0009/0011 | Loss: 3017.7815\n",
      "Epoch: 417/3000 | Batch 0010/0011 | Loss: 2913.4414\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 418/3000 | Batch 0000/0011 | Loss: 3066.0305\n",
      "Epoch: 418/3000 | Batch 0001/0011 | Loss: 3045.9685\n",
      "Epoch: 418/3000 | Batch 0002/0011 | Loss: 3057.1589\n",
      "Epoch: 418/3000 | Batch 0003/0011 | Loss: 3041.2915\n",
      "Epoch: 418/3000 | Batch 0004/0011 | Loss: 3019.6506\n",
      "Epoch: 418/3000 | Batch 0005/0011 | Loss: 3076.2139\n",
      "Epoch: 418/3000 | Batch 0006/0011 | Loss: 3065.5176\n",
      "Epoch: 418/3000 | Batch 0007/0011 | Loss: 3059.5635\n",
      "Epoch: 418/3000 | Batch 0008/0011 | Loss: 2984.1079\n",
      "Epoch: 418/3000 | Batch 0009/0011 | Loss: 3039.0886\n",
      "Epoch: 418/3000 | Batch 0010/0011 | Loss: 2999.5693\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 419/3000 | Batch 0000/0011 | Loss: 3061.9341\n",
      "Epoch: 419/3000 | Batch 0001/0011 | Loss: 3053.8679\n",
      "Epoch: 419/3000 | Batch 0002/0011 | Loss: 3050.4868\n",
      "Epoch: 419/3000 | Batch 0003/0011 | Loss: 3015.4854\n",
      "Epoch: 419/3000 | Batch 0004/0011 | Loss: 3044.5559\n",
      "Epoch: 419/3000 | Batch 0005/0011 | Loss: 3070.7415\n",
      "Epoch: 419/3000 | Batch 0006/0011 | Loss: 3000.6465\n",
      "Epoch: 419/3000 | Batch 0007/0011 | Loss: 3033.9324\n",
      "Epoch: 419/3000 | Batch 0008/0011 | Loss: 3043.6565\n",
      "Epoch: 419/3000 | Batch 0009/0011 | Loss: 3043.7646\n",
      "Epoch: 419/3000 | Batch 0010/0011 | Loss: 3100.0503\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 420/3000 | Batch 0000/0011 | Loss: 2960.3059\n",
      "Epoch: 420/3000 | Batch 0001/0011 | Loss: 2975.3708\n",
      "Epoch: 420/3000 | Batch 0002/0011 | Loss: 3054.8252\n",
      "Epoch: 420/3000 | Batch 0003/0011 | Loss: 3050.6479\n",
      "Epoch: 420/3000 | Batch 0004/0011 | Loss: 3059.7314\n",
      "Epoch: 420/3000 | Batch 0005/0011 | Loss: 3066.6707\n",
      "Epoch: 420/3000 | Batch 0006/0011 | Loss: 3110.5110\n",
      "Epoch: 420/3000 | Batch 0007/0011 | Loss: 3080.4866\n",
      "Epoch: 420/3000 | Batch 0008/0011 | Loss: 3032.4058\n",
      "Epoch: 420/3000 | Batch 0009/0011 | Loss: 3030.1489\n",
      "Epoch: 420/3000 | Batch 0010/0011 | Loss: 3029.6799\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 421/3000 | Batch 0000/0011 | Loss: 3025.0679\n",
      "Epoch: 421/3000 | Batch 0001/0011 | Loss: 3067.2722\n",
      "Epoch: 421/3000 | Batch 0002/0011 | Loss: 3051.7908\n",
      "Epoch: 421/3000 | Batch 0003/0011 | Loss: 3088.3035\n",
      "Epoch: 421/3000 | Batch 0004/0011 | Loss: 3013.5332\n",
      "Epoch: 421/3000 | Batch 0005/0011 | Loss: 3009.6487\n",
      "Epoch: 421/3000 | Batch 0006/0011 | Loss: 3050.0427\n",
      "Epoch: 421/3000 | Batch 0007/0011 | Loss: 3080.8699\n",
      "Epoch: 421/3000 | Batch 0008/0011 | Loss: 3080.9749\n",
      "Epoch: 421/3000 | Batch 0009/0011 | Loss: 3007.4749\n",
      "Epoch: 421/3000 | Batch 0010/0011 | Loss: 2888.0901\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 422/3000 | Batch 0000/0011 | Loss: 2993.2859\n",
      "Epoch: 422/3000 | Batch 0001/0011 | Loss: 3056.5906\n",
      "Epoch: 422/3000 | Batch 0002/0011 | Loss: 3031.1418\n",
      "Epoch: 422/3000 | Batch 0003/0011 | Loss: 3063.6257\n",
      "Epoch: 422/3000 | Batch 0004/0011 | Loss: 3040.2266\n",
      "Epoch: 422/3000 | Batch 0005/0011 | Loss: 3030.7175\n",
      "Epoch: 422/3000 | Batch 0006/0011 | Loss: 3124.0173\n",
      "Epoch: 422/3000 | Batch 0007/0011 | Loss: 3047.1367\n",
      "Epoch: 422/3000 | Batch 0008/0011 | Loss: 3078.0676\n",
      "Epoch: 422/3000 | Batch 0009/0011 | Loss: 2997.5437\n",
      "Epoch: 422/3000 | Batch 0010/0011 | Loss: 3084.0273\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 423/3000 | Batch 0000/0011 | Loss: 3045.6941\n",
      "Epoch: 423/3000 | Batch 0001/0011 | Loss: 3081.1443\n",
      "Epoch: 423/3000 | Batch 0002/0011 | Loss: 3019.1685\n",
      "Epoch: 423/3000 | Batch 0003/0011 | Loss: 3082.8369\n",
      "Epoch: 423/3000 | Batch 0004/0011 | Loss: 3099.4678\n",
      "Epoch: 423/3000 | Batch 0005/0011 | Loss: 3006.6716\n",
      "Epoch: 423/3000 | Batch 0006/0011 | Loss: 3000.7688\n",
      "Epoch: 423/3000 | Batch 0007/0011 | Loss: 3026.6187\n",
      "Epoch: 423/3000 | Batch 0008/0011 | Loss: 3005.9382\n",
      "Epoch: 423/3000 | Batch 0009/0011 | Loss: 3067.2661\n",
      "Epoch: 423/3000 | Batch 0010/0011 | Loss: 3038.6484\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 424/3000 | Batch 0000/0011 | Loss: 3023.1599\n",
      "Epoch: 424/3000 | Batch 0001/0011 | Loss: 3050.2991\n",
      "Epoch: 424/3000 | Batch 0002/0011 | Loss: 3005.5283\n",
      "Epoch: 424/3000 | Batch 0003/0011 | Loss: 3049.1567\n",
      "Epoch: 424/3000 | Batch 0004/0011 | Loss: 3061.9590\n",
      "Epoch: 424/3000 | Batch 0005/0011 | Loss: 3049.8496\n",
      "Epoch: 424/3000 | Batch 0006/0011 | Loss: 3070.3770\n",
      "Epoch: 424/3000 | Batch 0007/0011 | Loss: 3010.5676\n",
      "Epoch: 424/3000 | Batch 0008/0011 | Loss: 3016.8608\n",
      "Epoch: 424/3000 | Batch 0009/0011 | Loss: 3048.1670\n",
      "Epoch: 424/3000 | Batch 0010/0011 | Loss: 3355.1726\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 425/3000 | Batch 0000/0011 | Loss: 3014.7471\n",
      "Epoch: 425/3000 | Batch 0001/0011 | Loss: 3023.6873\n",
      "Epoch: 425/3000 | Batch 0002/0011 | Loss: 3019.4451\n",
      "Epoch: 425/3000 | Batch 0003/0011 | Loss: 2993.2546\n",
      "Epoch: 425/3000 | Batch 0004/0011 | Loss: 3111.9900\n",
      "Epoch: 425/3000 | Batch 0005/0011 | Loss: 3027.3188\n",
      "Epoch: 425/3000 | Batch 0006/0011 | Loss: 3030.4138\n",
      "Epoch: 425/3000 | Batch 0007/0011 | Loss: 3132.7180\n",
      "Epoch: 425/3000 | Batch 0008/0011 | Loss: 3046.2957\n",
      "Epoch: 425/3000 | Batch 0009/0011 | Loss: 3021.1367\n",
      "Epoch: 425/3000 | Batch 0010/0011 | Loss: 3159.6038\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 426/3000 | Batch 0000/0011 | Loss: 3079.8711\n",
      "Epoch: 426/3000 | Batch 0001/0011 | Loss: 3072.0261\n",
      "Epoch: 426/3000 | Batch 0002/0011 | Loss: 3043.4143\n",
      "Epoch: 426/3000 | Batch 0003/0011 | Loss: 2993.8367\n",
      "Epoch: 426/3000 | Batch 0004/0011 | Loss: 3076.6396\n",
      "Epoch: 426/3000 | Batch 0005/0011 | Loss: 3023.6240\n",
      "Epoch: 426/3000 | Batch 0006/0011 | Loss: 2997.9773\n",
      "Epoch: 426/3000 | Batch 0007/0011 | Loss: 3086.7732\n",
      "Epoch: 426/3000 | Batch 0008/0011 | Loss: 3043.1296\n",
      "Epoch: 426/3000 | Batch 0009/0011 | Loss: 3008.6797\n",
      "Epoch: 426/3000 | Batch 0010/0011 | Loss: 3032.1924\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 427/3000 | Batch 0000/0011 | Loss: 3040.4146\n",
      "Epoch: 427/3000 | Batch 0001/0011 | Loss: 3006.4580\n",
      "Epoch: 427/3000 | Batch 0002/0011 | Loss: 3124.5237\n",
      "Epoch: 427/3000 | Batch 0003/0011 | Loss: 3030.9487\n",
      "Epoch: 427/3000 | Batch 0004/0011 | Loss: 3007.7039\n",
      "Epoch: 427/3000 | Batch 0005/0011 | Loss: 2982.5669\n",
      "Epoch: 427/3000 | Batch 0006/0011 | Loss: 3033.8259\n",
      "Epoch: 427/3000 | Batch 0007/0011 | Loss: 3100.8457\n",
      "Epoch: 427/3000 | Batch 0008/0011 | Loss: 3031.3528\n",
      "Epoch: 427/3000 | Batch 0009/0011 | Loss: 3068.9983\n",
      "Epoch: 427/3000 | Batch 0010/0011 | Loss: 3004.5779\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 428/3000 | Batch 0000/0011 | Loss: 3034.1890\n",
      "Epoch: 428/3000 | Batch 0001/0011 | Loss: 3076.1589\n",
      "Epoch: 428/3000 | Batch 0002/0011 | Loss: 3048.8989\n",
      "Epoch: 428/3000 | Batch 0003/0011 | Loss: 3034.1555\n",
      "Epoch: 428/3000 | Batch 0004/0011 | Loss: 2960.3079\n",
      "Epoch: 428/3000 | Batch 0005/0011 | Loss: 3037.6060\n",
      "Epoch: 428/3000 | Batch 0006/0011 | Loss: 3052.5435\n",
      "Epoch: 428/3000 | Batch 0007/0011 | Loss: 3026.0457\n",
      "Epoch: 428/3000 | Batch 0008/0011 | Loss: 3092.9753\n",
      "Epoch: 428/3000 | Batch 0009/0011 | Loss: 3022.3176\n",
      "Epoch: 428/3000 | Batch 0010/0011 | Loss: 3332.1689\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 429/3000 | Batch 0000/0011 | Loss: 3085.6101\n",
      "Epoch: 429/3000 | Batch 0001/0011 | Loss: 3042.2451\n",
      "Epoch: 429/3000 | Batch 0002/0011 | Loss: 3024.1680\n",
      "Epoch: 429/3000 | Batch 0003/0011 | Loss: 2988.4229\n",
      "Epoch: 429/3000 | Batch 0004/0011 | Loss: 2991.7227\n",
      "Epoch: 429/3000 | Batch 0005/0011 | Loss: 3083.3110\n",
      "Epoch: 429/3000 | Batch 0006/0011 | Loss: 3027.6309\n",
      "Epoch: 429/3000 | Batch 0007/0011 | Loss: 3046.3955\n",
      "Epoch: 429/3000 | Batch 0008/0011 | Loss: 3066.5398\n",
      "Epoch: 429/3000 | Batch 0009/0011 | Loss: 3077.5417\n",
      "Epoch: 429/3000 | Batch 0010/0011 | Loss: 2906.4119\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 430/3000 | Batch 0000/0011 | Loss: 3069.7241\n",
      "Epoch: 430/3000 | Batch 0001/0011 | Loss: 3045.3840\n",
      "Epoch: 430/3000 | Batch 0002/0011 | Loss: 3043.1672\n",
      "Epoch: 430/3000 | Batch 0003/0011 | Loss: 3058.9031\n",
      "Epoch: 430/3000 | Batch 0004/0011 | Loss: 2999.0181\n",
      "Epoch: 430/3000 | Batch 0005/0011 | Loss: 3053.2600\n",
      "Epoch: 430/3000 | Batch 0006/0011 | Loss: 3047.4080\n",
      "Epoch: 430/3000 | Batch 0007/0011 | Loss: 3024.8057\n",
      "Epoch: 430/3000 | Batch 0008/0011 | Loss: 3059.5928\n",
      "Epoch: 430/3000 | Batch 0009/0011 | Loss: 3007.7964\n",
      "Epoch: 430/3000 | Batch 0010/0011 | Loss: 2989.8337\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 431/3000 | Batch 0000/0011 | Loss: 2996.5452\n",
      "Epoch: 431/3000 | Batch 0001/0011 | Loss: 3017.6526\n",
      "Epoch: 431/3000 | Batch 0002/0011 | Loss: 3082.4050\n",
      "Epoch: 431/3000 | Batch 0003/0011 | Loss: 2998.1792\n",
      "Epoch: 431/3000 | Batch 0004/0011 | Loss: 3082.0339\n",
      "Epoch: 431/3000 | Batch 0005/0011 | Loss: 3046.6230\n",
      "Epoch: 431/3000 | Batch 0006/0011 | Loss: 3037.3945\n",
      "Epoch: 431/3000 | Batch 0007/0011 | Loss: 3035.9688\n",
      "Epoch: 431/3000 | Batch 0008/0011 | Loss: 3027.5659\n",
      "Epoch: 431/3000 | Batch 0009/0011 | Loss: 3056.5383\n",
      "Epoch: 431/3000 | Batch 0010/0011 | Loss: 3149.3315\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 432/3000 | Batch 0000/0011 | Loss: 3010.6882\n",
      "Epoch: 432/3000 | Batch 0001/0011 | Loss: 3059.6626\n",
      "Epoch: 432/3000 | Batch 0002/0011 | Loss: 2986.3250\n",
      "Epoch: 432/3000 | Batch 0003/0011 | Loss: 3014.2842\n",
      "Epoch: 432/3000 | Batch 0004/0011 | Loss: 3043.7432\n",
      "Epoch: 432/3000 | Batch 0005/0011 | Loss: 3043.5444\n",
      "Epoch: 432/3000 | Batch 0006/0011 | Loss: 3025.8232\n",
      "Epoch: 432/3000 | Batch 0007/0011 | Loss: 3080.2288\n",
      "Epoch: 432/3000 | Batch 0008/0011 | Loss: 3072.8777\n",
      "Epoch: 432/3000 | Batch 0009/0011 | Loss: 3039.0769\n",
      "Epoch: 432/3000 | Batch 0010/0011 | Loss: 3120.1516\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 433/3000 | Batch 0000/0011 | Loss: 3072.0625\n",
      "Epoch: 433/3000 | Batch 0001/0011 | Loss: 3019.3821\n",
      "Epoch: 433/3000 | Batch 0002/0011 | Loss: 3041.1365\n",
      "Epoch: 433/3000 | Batch 0003/0011 | Loss: 3069.1028\n",
      "Epoch: 433/3000 | Batch 0004/0011 | Loss: 2979.7578\n",
      "Epoch: 433/3000 | Batch 0005/0011 | Loss: 3034.2812\n",
      "Epoch: 433/3000 | Batch 0006/0011 | Loss: 3029.2502\n",
      "Epoch: 433/3000 | Batch 0007/0011 | Loss: 3033.5166\n",
      "Epoch: 433/3000 | Batch 0008/0011 | Loss: 3080.3953\n",
      "Epoch: 433/3000 | Batch 0009/0011 | Loss: 3020.9138\n",
      "Epoch: 433/3000 | Batch 0010/0011 | Loss: 3156.6226\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 434/3000 | Batch 0000/0011 | Loss: 3057.0249\n",
      "Epoch: 434/3000 | Batch 0001/0011 | Loss: 2969.1436\n",
      "Epoch: 434/3000 | Batch 0002/0011 | Loss: 3012.2776\n",
      "Epoch: 434/3000 | Batch 0003/0011 | Loss: 2986.0764\n",
      "Epoch: 434/3000 | Batch 0004/0011 | Loss: 3068.1997\n",
      "Epoch: 434/3000 | Batch 0005/0011 | Loss: 3060.7869\n",
      "Epoch: 434/3000 | Batch 0006/0011 | Loss: 2977.7153\n",
      "Epoch: 434/3000 | Batch 0007/0011 | Loss: 3061.2068\n",
      "Epoch: 434/3000 | Batch 0008/0011 | Loss: 3035.2339\n",
      "Epoch: 434/3000 | Batch 0009/0011 | Loss: 3056.5911\n",
      "Epoch: 434/3000 | Batch 0010/0011 | Loss: 2988.6267\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 435/3000 | Batch 0000/0011 | Loss: 3047.5134\n",
      "Epoch: 435/3000 | Batch 0001/0011 | Loss: 2992.4868\n",
      "Epoch: 435/3000 | Batch 0002/0011 | Loss: 2992.6914\n",
      "Epoch: 435/3000 | Batch 0003/0011 | Loss: 2979.2803\n",
      "Epoch: 435/3000 | Batch 0004/0011 | Loss: 3023.3379\n",
      "Epoch: 435/3000 | Batch 0005/0011 | Loss: 3069.6526\n",
      "Epoch: 435/3000 | Batch 0006/0011 | Loss: 3040.4917\n",
      "Epoch: 435/3000 | Batch 0007/0011 | Loss: 3052.9031\n",
      "Epoch: 435/3000 | Batch 0008/0011 | Loss: 3010.8350\n",
      "Epoch: 435/3000 | Batch 0009/0011 | Loss: 3037.2004\n",
      "Epoch: 435/3000 | Batch 0010/0011 | Loss: 2917.9229\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 436/3000 | Batch 0000/0011 | Loss: 3051.1658\n",
      "Epoch: 436/3000 | Batch 0001/0011 | Loss: 3140.4241\n",
      "Epoch: 436/3000 | Batch 0002/0011 | Loss: 3047.5713\n",
      "Epoch: 436/3000 | Batch 0003/0011 | Loss: 2978.5845\n",
      "Epoch: 436/3000 | Batch 0004/0011 | Loss: 3059.9451\n",
      "Epoch: 436/3000 | Batch 0005/0011 | Loss: 2995.0149\n",
      "Epoch: 436/3000 | Batch 0006/0011 | Loss: 2990.2446\n",
      "Epoch: 436/3000 | Batch 0007/0011 | Loss: 2971.8845\n",
      "Epoch: 436/3000 | Batch 0008/0011 | Loss: 3008.1658\n",
      "Epoch: 436/3000 | Batch 0009/0011 | Loss: 2995.9402\n",
      "Epoch: 436/3000 | Batch 0010/0011 | Loss: 3119.6050\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 437/3000 | Batch 0000/0011 | Loss: 3010.3176\n",
      "Epoch: 437/3000 | Batch 0001/0011 | Loss: 3010.4812\n",
      "Epoch: 437/3000 | Batch 0002/0011 | Loss: 3001.8984\n",
      "Epoch: 437/3000 | Batch 0003/0011 | Loss: 3033.6455\n",
      "Epoch: 437/3000 | Batch 0004/0011 | Loss: 3001.8120\n",
      "Epoch: 437/3000 | Batch 0005/0011 | Loss: 2986.8098\n",
      "Epoch: 437/3000 | Batch 0006/0011 | Loss: 3044.2361\n",
      "Epoch: 437/3000 | Batch 0007/0011 | Loss: 3067.2588\n",
      "Epoch: 437/3000 | Batch 0008/0011 | Loss: 3032.3457\n",
      "Epoch: 437/3000 | Batch 0009/0011 | Loss: 3040.3914\n",
      "Epoch: 437/3000 | Batch 0010/0011 | Loss: 3061.2490\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 438/3000 | Batch 0000/0011 | Loss: 3066.1172\n",
      "Epoch: 438/3000 | Batch 0001/0011 | Loss: 3063.7346\n",
      "Epoch: 438/3000 | Batch 0002/0011 | Loss: 2990.9558\n",
      "Epoch: 438/3000 | Batch 0003/0011 | Loss: 3068.9087\n",
      "Epoch: 438/3000 | Batch 0004/0011 | Loss: 2948.4133\n",
      "Epoch: 438/3000 | Batch 0005/0011 | Loss: 3005.9507\n",
      "Epoch: 438/3000 | Batch 0006/0011 | Loss: 3047.0859\n",
      "Epoch: 438/3000 | Batch 0007/0011 | Loss: 3035.7000\n",
      "Epoch: 438/3000 | Batch 0008/0011 | Loss: 3021.1619\n",
      "Epoch: 438/3000 | Batch 0009/0011 | Loss: 3003.4395\n",
      "Epoch: 438/3000 | Batch 0010/0011 | Loss: 2865.0935\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 439/3000 | Batch 0000/0011 | Loss: 3040.9661\n",
      "Epoch: 439/3000 | Batch 0001/0011 | Loss: 2954.9351\n",
      "Epoch: 439/3000 | Batch 0002/0011 | Loss: 3052.3801\n",
      "Epoch: 439/3000 | Batch 0003/0011 | Loss: 3011.1377\n",
      "Epoch: 439/3000 | Batch 0004/0011 | Loss: 3044.1101\n",
      "Epoch: 439/3000 | Batch 0005/0011 | Loss: 2981.8542\n",
      "Epoch: 439/3000 | Batch 0006/0011 | Loss: 3018.2700\n",
      "Epoch: 439/3000 | Batch 0007/0011 | Loss: 3052.3601\n",
      "Epoch: 439/3000 | Batch 0008/0011 | Loss: 3027.9705\n",
      "Epoch: 439/3000 | Batch 0009/0011 | Loss: 3010.9248\n",
      "Epoch: 439/3000 | Batch 0010/0011 | Loss: 2992.9531\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 440/3000 | Batch 0000/0011 | Loss: 3048.2622\n",
      "Epoch: 440/3000 | Batch 0001/0011 | Loss: 3028.5874\n",
      "Epoch: 440/3000 | Batch 0002/0011 | Loss: 2989.7153\n",
      "Epoch: 440/3000 | Batch 0003/0011 | Loss: 2996.3586\n",
      "Epoch: 440/3000 | Batch 0004/0011 | Loss: 3006.5330\n",
      "Epoch: 440/3000 | Batch 0005/0011 | Loss: 2989.1875\n",
      "Epoch: 440/3000 | Batch 0006/0011 | Loss: 3023.7593\n",
      "Epoch: 440/3000 | Batch 0007/0011 | Loss: 3029.3384\n",
      "Epoch: 440/3000 | Batch 0008/0011 | Loss: 3029.5969\n",
      "Epoch: 440/3000 | Batch 0009/0011 | Loss: 3061.9702\n",
      "Epoch: 440/3000 | Batch 0010/0011 | Loss: 3052.4980\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 441/3000 | Batch 0000/0011 | Loss: 3072.8123\n",
      "Epoch: 441/3000 | Batch 0001/0011 | Loss: 2961.8281\n",
      "Epoch: 441/3000 | Batch 0002/0011 | Loss: 2951.2773\n",
      "Epoch: 441/3000 | Batch 0003/0011 | Loss: 3029.3574\n",
      "Epoch: 441/3000 | Batch 0004/0011 | Loss: 3000.6086\n",
      "Epoch: 441/3000 | Batch 0005/0011 | Loss: 3028.4292\n",
      "Epoch: 441/3000 | Batch 0006/0011 | Loss: 3061.9363\n",
      "Epoch: 441/3000 | Batch 0007/0011 | Loss: 2980.5210\n",
      "Epoch: 441/3000 | Batch 0008/0011 | Loss: 3030.1450\n",
      "Epoch: 441/3000 | Batch 0009/0011 | Loss: 3043.7593\n",
      "Epoch: 441/3000 | Batch 0010/0011 | Loss: 3001.0972\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 442/3000 | Batch 0000/0011 | Loss: 2976.2417\n",
      "Epoch: 442/3000 | Batch 0001/0011 | Loss: 3020.4893\n",
      "Epoch: 442/3000 | Batch 0002/0011 | Loss: 3043.1216\n",
      "Epoch: 442/3000 | Batch 0003/0011 | Loss: 2990.1240\n",
      "Epoch: 442/3000 | Batch 0004/0011 | Loss: 3055.3833\n",
      "Epoch: 442/3000 | Batch 0005/0011 | Loss: 3048.4351\n",
      "Epoch: 442/3000 | Batch 0006/0011 | Loss: 3005.4717\n",
      "Epoch: 442/3000 | Batch 0007/0011 | Loss: 3032.0266\n",
      "Epoch: 442/3000 | Batch 0008/0011 | Loss: 2988.6421\n",
      "Epoch: 442/3000 | Batch 0009/0011 | Loss: 3016.5759\n",
      "Epoch: 442/3000 | Batch 0010/0011 | Loss: 2956.5466\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 443/3000 | Batch 0000/0011 | Loss: 3044.2607\n",
      "Epoch: 443/3000 | Batch 0001/0011 | Loss: 3035.1091\n",
      "Epoch: 443/3000 | Batch 0002/0011 | Loss: 2989.7361\n",
      "Epoch: 443/3000 | Batch 0003/0011 | Loss: 3035.1172\n",
      "Epoch: 443/3000 | Batch 0004/0011 | Loss: 3068.2041\n",
      "Epoch: 443/3000 | Batch 0005/0011 | Loss: 3072.8860\n",
      "Epoch: 443/3000 | Batch 0006/0011 | Loss: 2985.0325\n",
      "Epoch: 443/3000 | Batch 0007/0011 | Loss: 2967.4905\n",
      "Epoch: 443/3000 | Batch 0008/0011 | Loss: 2975.2312\n",
      "Epoch: 443/3000 | Batch 0009/0011 | Loss: 3011.9092\n",
      "Epoch: 443/3000 | Batch 0010/0011 | Loss: 3010.8474\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 444/3000 | Batch 0000/0011 | Loss: 3063.5630\n",
      "Epoch: 444/3000 | Batch 0001/0011 | Loss: 3011.5986\n",
      "Epoch: 444/3000 | Batch 0002/0011 | Loss: 2986.1086\n",
      "Epoch: 444/3000 | Batch 0003/0011 | Loss: 3002.7446\n",
      "Epoch: 444/3000 | Batch 0004/0011 | Loss: 2995.9263\n",
      "Epoch: 444/3000 | Batch 0005/0011 | Loss: 2955.9209\n",
      "Epoch: 444/3000 | Batch 0006/0011 | Loss: 3011.9929\n",
      "Epoch: 444/3000 | Batch 0007/0011 | Loss: 3029.8577\n",
      "Epoch: 444/3000 | Batch 0008/0011 | Loss: 3047.1213\n",
      "Epoch: 444/3000 | Batch 0009/0011 | Loss: 3094.0652\n",
      "Epoch: 444/3000 | Batch 0010/0011 | Loss: 2818.5193\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 445/3000 | Batch 0000/0011 | Loss: 3004.8418\n",
      "Epoch: 445/3000 | Batch 0001/0011 | Loss: 3004.2061\n",
      "Epoch: 445/3000 | Batch 0002/0011 | Loss: 2984.7297\n",
      "Epoch: 445/3000 | Batch 0003/0011 | Loss: 2950.1865\n",
      "Epoch: 445/3000 | Batch 0004/0011 | Loss: 3034.6230\n",
      "Epoch: 445/3000 | Batch 0005/0011 | Loss: 3056.4705\n",
      "Epoch: 445/3000 | Batch 0006/0011 | Loss: 3030.3716\n",
      "Epoch: 445/3000 | Batch 0007/0011 | Loss: 3017.5208\n",
      "Epoch: 445/3000 | Batch 0008/0011 | Loss: 3038.6025\n",
      "Epoch: 445/3000 | Batch 0009/0011 | Loss: 3006.7097\n",
      "Epoch: 445/3000 | Batch 0010/0011 | Loss: 3156.7341\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 446/3000 | Batch 0000/0011 | Loss: 3053.8579\n",
      "Epoch: 446/3000 | Batch 0001/0011 | Loss: 3028.2544\n",
      "Epoch: 446/3000 | Batch 0002/0011 | Loss: 2985.9490\n",
      "Epoch: 446/3000 | Batch 0003/0011 | Loss: 2969.2502\n",
      "Epoch: 446/3000 | Batch 0004/0011 | Loss: 3004.8037\n",
      "Epoch: 446/3000 | Batch 0005/0011 | Loss: 2971.3748\n",
      "Epoch: 446/3000 | Batch 0006/0011 | Loss: 3029.8054\n",
      "Epoch: 446/3000 | Batch 0007/0011 | Loss: 3005.7126\n",
      "Epoch: 446/3000 | Batch 0008/0011 | Loss: 3132.6580\n",
      "Epoch: 446/3000 | Batch 0009/0011 | Loss: 3005.8232\n",
      "Epoch: 446/3000 | Batch 0010/0011 | Loss: 2936.4517\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 447/3000 | Batch 0000/0011 | Loss: 2982.1533\n",
      "Epoch: 447/3000 | Batch 0001/0011 | Loss: 2993.9365\n",
      "Epoch: 447/3000 | Batch 0002/0011 | Loss: 3004.2605\n",
      "Epoch: 447/3000 | Batch 0003/0011 | Loss: 3032.4373\n",
      "Epoch: 447/3000 | Batch 0004/0011 | Loss: 2995.8586\n",
      "Epoch: 447/3000 | Batch 0005/0011 | Loss: 2989.8230\n",
      "Epoch: 447/3000 | Batch 0006/0011 | Loss: 3026.7788\n",
      "Epoch: 447/3000 | Batch 0007/0011 | Loss: 3104.7422\n",
      "Epoch: 447/3000 | Batch 0008/0011 | Loss: 3091.7002\n",
      "Epoch: 447/3000 | Batch 0009/0011 | Loss: 2954.0183\n",
      "Epoch: 447/3000 | Batch 0010/0011 | Loss: 3102.8828\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 448/3000 | Batch 0000/0011 | Loss: 3017.0833\n",
      "Epoch: 448/3000 | Batch 0001/0011 | Loss: 3029.1079\n",
      "Epoch: 448/3000 | Batch 0002/0011 | Loss: 3005.3899\n",
      "Epoch: 448/3000 | Batch 0003/0011 | Loss: 3023.1399\n",
      "Epoch: 448/3000 | Batch 0004/0011 | Loss: 3021.0146\n",
      "Epoch: 448/3000 | Batch 0005/0011 | Loss: 3018.0188\n",
      "Epoch: 448/3000 | Batch 0006/0011 | Loss: 3073.4548\n",
      "Epoch: 448/3000 | Batch 0007/0011 | Loss: 3013.5520\n",
      "Epoch: 448/3000 | Batch 0008/0011 | Loss: 2980.4270\n",
      "Epoch: 448/3000 | Batch 0009/0011 | Loss: 3023.9275\n",
      "Epoch: 448/3000 | Batch 0010/0011 | Loss: 2980.4910\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 449/3000 | Batch 0000/0011 | Loss: 2944.8767\n",
      "Epoch: 449/3000 | Batch 0001/0011 | Loss: 3014.8945\n",
      "Epoch: 449/3000 | Batch 0002/0011 | Loss: 3023.1108\n",
      "Epoch: 449/3000 | Batch 0003/0011 | Loss: 3038.7202\n",
      "Epoch: 449/3000 | Batch 0004/0011 | Loss: 3091.2991\n",
      "Epoch: 449/3000 | Batch 0005/0011 | Loss: 2987.5151\n",
      "Epoch: 449/3000 | Batch 0006/0011 | Loss: 3028.2380\n",
      "Epoch: 449/3000 | Batch 0007/0011 | Loss: 2995.2581\n",
      "Epoch: 449/3000 | Batch 0008/0011 | Loss: 3004.4407\n",
      "Epoch: 449/3000 | Batch 0009/0011 | Loss: 3019.0383\n",
      "Epoch: 449/3000 | Batch 0010/0011 | Loss: 3033.9644\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 450/3000 | Batch 0000/0011 | Loss: 2976.7959\n",
      "Epoch: 450/3000 | Batch 0001/0011 | Loss: 2961.0254\n",
      "Epoch: 450/3000 | Batch 0002/0011 | Loss: 2918.8445\n",
      "Epoch: 450/3000 | Batch 0003/0011 | Loss: 2994.6802\n",
      "Epoch: 450/3000 | Batch 0004/0011 | Loss: 2946.8535\n",
      "Epoch: 450/3000 | Batch 0005/0011 | Loss: 2988.4038\n",
      "Epoch: 450/3000 | Batch 0006/0011 | Loss: 2990.8899\n",
      "Epoch: 450/3000 | Batch 0007/0011 | Loss: 2927.5361\n",
      "Epoch: 450/3000 | Batch 0008/0011 | Loss: 2926.5369\n",
      "Epoch: 450/3000 | Batch 0009/0011 | Loss: 2907.5632\n",
      "Epoch: 450/3000 | Batch 0010/0011 | Loss: 2921.7041\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 451/3000 | Batch 0000/0011 | Loss: 2952.9951\n",
      "Epoch: 451/3000 | Batch 0001/0011 | Loss: 2972.5793\n",
      "Epoch: 451/3000 | Batch 0002/0011 | Loss: 2911.3416\n",
      "Epoch: 451/3000 | Batch 0003/0011 | Loss: 2959.8708\n",
      "Epoch: 451/3000 | Batch 0004/0011 | Loss: 2930.5076\n",
      "Epoch: 451/3000 | Batch 0005/0011 | Loss: 2990.5056\n",
      "Epoch: 451/3000 | Batch 0006/0011 | Loss: 2946.6802\n",
      "Epoch: 451/3000 | Batch 0007/0011 | Loss: 2961.7891\n",
      "Epoch: 451/3000 | Batch 0008/0011 | Loss: 2923.7939\n",
      "Epoch: 451/3000 | Batch 0009/0011 | Loss: 2921.6775\n",
      "Epoch: 451/3000 | Batch 0010/0011 | Loss: 2948.4875\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 452/3000 | Batch 0000/0011 | Loss: 2905.0569\n",
      "Epoch: 452/3000 | Batch 0001/0011 | Loss: 2987.9868\n",
      "Epoch: 452/3000 | Batch 0002/0011 | Loss: 2959.9922\n",
      "Epoch: 452/3000 | Batch 0003/0011 | Loss: 2929.3374\n",
      "Epoch: 452/3000 | Batch 0004/0011 | Loss: 2901.6594\n",
      "Epoch: 452/3000 | Batch 0005/0011 | Loss: 2908.2021\n",
      "Epoch: 452/3000 | Batch 0006/0011 | Loss: 2984.6199\n",
      "Epoch: 452/3000 | Batch 0007/0011 | Loss: 2974.3289\n",
      "Epoch: 452/3000 | Batch 0008/0011 | Loss: 2899.6912\n",
      "Epoch: 452/3000 | Batch 0009/0011 | Loss: 2953.8113\n",
      "Epoch: 452/3000 | Batch 0010/0011 | Loss: 2940.8394\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 453/3000 | Batch 0000/0011 | Loss: 2929.2449\n",
      "Epoch: 453/3000 | Batch 0001/0011 | Loss: 2918.1392\n",
      "Epoch: 453/3000 | Batch 0002/0011 | Loss: 2901.8862\n",
      "Epoch: 453/3000 | Batch 0003/0011 | Loss: 2938.3601\n",
      "Epoch: 453/3000 | Batch 0004/0011 | Loss: 2998.7681\n",
      "Epoch: 453/3000 | Batch 0005/0011 | Loss: 2960.6565\n",
      "Epoch: 453/3000 | Batch 0006/0011 | Loss: 3004.5020\n",
      "Epoch: 453/3000 | Batch 0007/0011 | Loss: 2896.4541\n",
      "Epoch: 453/3000 | Batch 0008/0011 | Loss: 2904.7808\n",
      "Epoch: 453/3000 | Batch 0009/0011 | Loss: 2927.9177\n",
      "Epoch: 453/3000 | Batch 0010/0011 | Loss: 2988.3674\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 454/3000 | Batch 0000/0011 | Loss: 2913.0906\n",
      "Epoch: 454/3000 | Batch 0001/0011 | Loss: 2920.8474\n",
      "Epoch: 454/3000 | Batch 0002/0011 | Loss: 2921.4495\n",
      "Epoch: 454/3000 | Batch 0003/0011 | Loss: 2949.0918\n",
      "Epoch: 454/3000 | Batch 0004/0011 | Loss: 2982.1541\n",
      "Epoch: 454/3000 | Batch 0005/0011 | Loss: 2934.0369\n",
      "Epoch: 454/3000 | Batch 0006/0011 | Loss: 2969.1470\n",
      "Epoch: 454/3000 | Batch 0007/0011 | Loss: 2913.7739\n",
      "Epoch: 454/3000 | Batch 0008/0011 | Loss: 2964.7773\n",
      "Epoch: 454/3000 | Batch 0009/0011 | Loss: 2885.7024\n",
      "Epoch: 454/3000 | Batch 0010/0011 | Loss: 3017.5627\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 455/3000 | Batch 0000/0011 | Loss: 2947.8403\n",
      "Epoch: 455/3000 | Batch 0001/0011 | Loss: 2967.8433\n",
      "Epoch: 455/3000 | Batch 0002/0011 | Loss: 2955.5120\n",
      "Epoch: 455/3000 | Batch 0003/0011 | Loss: 2943.5303\n",
      "Epoch: 455/3000 | Batch 0004/0011 | Loss: 2907.9155\n",
      "Epoch: 455/3000 | Batch 0005/0011 | Loss: 2960.9219\n",
      "Epoch: 455/3000 | Batch 0006/0011 | Loss: 2933.1924\n",
      "Epoch: 455/3000 | Batch 0007/0011 | Loss: 2907.7119\n",
      "Epoch: 455/3000 | Batch 0008/0011 | Loss: 2866.0530\n",
      "Epoch: 455/3000 | Batch 0009/0011 | Loss: 2963.9592\n",
      "Epoch: 455/3000 | Batch 0010/0011 | Loss: 2901.0898\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 456/3000 | Batch 0000/0011 | Loss: 2949.8594\n",
      "Epoch: 456/3000 | Batch 0001/0011 | Loss: 2975.0215\n",
      "Epoch: 456/3000 | Batch 0002/0011 | Loss: 2905.1589\n",
      "Epoch: 456/3000 | Batch 0003/0011 | Loss: 2896.4128\n",
      "Epoch: 456/3000 | Batch 0004/0011 | Loss: 2948.7361\n",
      "Epoch: 456/3000 | Batch 0005/0011 | Loss: 2939.5610\n",
      "Epoch: 456/3000 | Batch 0006/0011 | Loss: 2944.9934\n",
      "Epoch: 456/3000 | Batch 0007/0011 | Loss: 2945.9768\n",
      "Epoch: 456/3000 | Batch 0008/0011 | Loss: 2973.3691\n",
      "Epoch: 456/3000 | Batch 0009/0011 | Loss: 2931.7988\n",
      "Epoch: 456/3000 | Batch 0010/0011 | Loss: 2935.4788\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 457/3000 | Batch 0000/0011 | Loss: 2958.0999\n",
      "Epoch: 457/3000 | Batch 0001/0011 | Loss: 3015.1101\n",
      "Epoch: 457/3000 | Batch 0002/0011 | Loss: 2911.4839\n",
      "Epoch: 457/3000 | Batch 0003/0011 | Loss: 2942.5894\n",
      "Epoch: 457/3000 | Batch 0004/0011 | Loss: 2955.0659\n",
      "Epoch: 457/3000 | Batch 0005/0011 | Loss: 2989.2039\n",
      "Epoch: 457/3000 | Batch 0006/0011 | Loss: 2927.9695\n",
      "Epoch: 457/3000 | Batch 0007/0011 | Loss: 2873.1194\n",
      "Epoch: 457/3000 | Batch 0008/0011 | Loss: 2900.5842\n",
      "Epoch: 457/3000 | Batch 0009/0011 | Loss: 2918.3743\n",
      "Epoch: 457/3000 | Batch 0010/0011 | Loss: 3206.0396\n",
      "Time elapsed: 0.13 min\n",
      "Epoch: 458/3000 | Batch 0000/0011 | Loss: 2944.9980\n",
      "Epoch: 458/3000 | Batch 0001/0011 | Loss: 2931.7515\n",
      "Epoch: 458/3000 | Batch 0002/0011 | Loss: 2918.4365\n",
      "Epoch: 458/3000 | Batch 0003/0011 | Loss: 2987.7034\n",
      "Epoch: 458/3000 | Batch 0004/0011 | Loss: 2945.8025\n",
      "Epoch: 458/3000 | Batch 0005/0011 | Loss: 2942.3572\n",
      "Epoch: 458/3000 | Batch 0006/0011 | Loss: 2977.8113\n",
      "Epoch: 458/3000 | Batch 0007/0011 | Loss: 2965.4861\n",
      "Epoch: 458/3000 | Batch 0008/0011 | Loss: 2881.9910\n",
      "Epoch: 458/3000 | Batch 0009/0011 | Loss: 2881.3674\n",
      "Epoch: 458/3000 | Batch 0010/0011 | Loss: 2954.6819\n",
      "Time elapsed: 0.13 min\n",
      "Epoch: 459/3000 | Batch 0000/0011 | Loss: 2962.2329\n",
      "Epoch: 459/3000 | Batch 0001/0011 | Loss: 2886.7930\n",
      "Epoch: 459/3000 | Batch 0002/0011 | Loss: 2934.8696\n",
      "Epoch: 459/3000 | Batch 0003/0011 | Loss: 2962.6445\n",
      "Epoch: 459/3000 | Batch 0004/0011 | Loss: 2942.2419\n",
      "Epoch: 459/3000 | Batch 0005/0011 | Loss: 2958.5527\n",
      "Epoch: 459/3000 | Batch 0006/0011 | Loss: 2928.1848\n",
      "Epoch: 459/3000 | Batch 0007/0011 | Loss: 2929.2065\n",
      "Epoch: 459/3000 | Batch 0008/0011 | Loss: 2924.2307\n",
      "Epoch: 459/3000 | Batch 0009/0011 | Loss: 2933.3391\n",
      "Epoch: 459/3000 | Batch 0010/0011 | Loss: 2886.9268\n",
      "Time elapsed: 0.13 min\n",
      "Epoch: 460/3000 | Batch 0000/0011 | Loss: 2897.5925\n",
      "Epoch: 460/3000 | Batch 0001/0011 | Loss: 2942.7317\n",
      "Epoch: 460/3000 | Batch 0002/0011 | Loss: 2914.8435\n",
      "Epoch: 460/3000 | Batch 0003/0011 | Loss: 2923.0808\n",
      "Epoch: 460/3000 | Batch 0004/0011 | Loss: 2951.6445\n",
      "Epoch: 460/3000 | Batch 0005/0011 | Loss: 2912.4153\n",
      "Epoch: 460/3000 | Batch 0006/0011 | Loss: 2928.1877\n",
      "Epoch: 460/3000 | Batch 0007/0011 | Loss: 2932.4294\n",
      "Epoch: 460/3000 | Batch 0008/0011 | Loss: 3001.5457\n",
      "Epoch: 460/3000 | Batch 0009/0011 | Loss: 2940.5793\n",
      "Epoch: 460/3000 | Batch 0010/0011 | Loss: 2960.2410\n",
      "Time elapsed: 0.13 min\n",
      "Epoch: 461/3000 | Batch 0000/0011 | Loss: 2947.5728\n",
      "Epoch: 461/3000 | Batch 0001/0011 | Loss: 2923.5312\n",
      "Epoch: 461/3000 | Batch 0002/0011 | Loss: 3011.0574\n",
      "Epoch: 461/3000 | Batch 0003/0011 | Loss: 2916.7825\n",
      "Epoch: 461/3000 | Batch 0004/0011 | Loss: 2896.6687\n",
      "Epoch: 461/3000 | Batch 0005/0011 | Loss: 2906.1101\n",
      "Epoch: 461/3000 | Batch 0006/0011 | Loss: 2927.5366\n",
      "Epoch: 461/3000 | Batch 0007/0011 | Loss: 2928.1118\n",
      "Epoch: 461/3000 | Batch 0008/0011 | Loss: 2933.3105\n",
      "Epoch: 461/3000 | Batch 0009/0011 | Loss: 2928.7383\n",
      "Epoch: 461/3000 | Batch 0010/0011 | Loss: 3107.2229\n",
      "Time elapsed: 0.13 min\n",
      "Epoch: 462/3000 | Batch 0000/0011 | Loss: 2945.5347\n",
      "Epoch: 462/3000 | Batch 0001/0011 | Loss: 2934.5339\n",
      "Epoch: 462/3000 | Batch 0002/0011 | Loss: 2909.2185\n",
      "Epoch: 462/3000 | Batch 0003/0011 | Loss: 2909.1858\n",
      "Epoch: 462/3000 | Batch 0004/0011 | Loss: 2948.8862\n",
      "Epoch: 462/3000 | Batch 0005/0011 | Loss: 2981.1123\n",
      "Epoch: 462/3000 | Batch 0006/0011 | Loss: 2971.4365\n",
      "Epoch: 462/3000 | Batch 0007/0011 | Loss: 2935.8679\n",
      "Epoch: 462/3000 | Batch 0008/0011 | Loss: 2866.9814\n",
      "Epoch: 462/3000 | Batch 0009/0011 | Loss: 2950.8665\n",
      "Epoch: 462/3000 | Batch 0010/0011 | Loss: 3040.2615\n",
      "Time elapsed: 0.13 min\n",
      "Epoch: 463/3000 | Batch 0000/0011 | Loss: 2913.5320\n",
      "Epoch: 463/3000 | Batch 0001/0011 | Loss: 2979.6538\n",
      "Epoch: 463/3000 | Batch 0002/0011 | Loss: 2904.0508\n",
      "Epoch: 463/3000 | Batch 0003/0011 | Loss: 2886.6206\n",
      "Epoch: 463/3000 | Batch 0004/0011 | Loss: 2948.4885\n",
      "Epoch: 463/3000 | Batch 0005/0011 | Loss: 2965.1528\n",
      "Epoch: 463/3000 | Batch 0006/0011 | Loss: 2965.0034\n",
      "Epoch: 463/3000 | Batch 0007/0011 | Loss: 2948.6909\n",
      "Epoch: 463/3000 | Batch 0008/0011 | Loss: 2879.5503\n",
      "Epoch: 463/3000 | Batch 0009/0011 | Loss: 2934.1672\n",
      "Epoch: 463/3000 | Batch 0010/0011 | Loss: 2985.2261\n",
      "Time elapsed: 0.13 min\n",
      "Epoch: 464/3000 | Batch 0000/0011 | Loss: 2977.3347\n",
      "Epoch: 464/3000 | Batch 0001/0011 | Loss: 2996.8418\n",
      "Epoch: 464/3000 | Batch 0002/0011 | Loss: 2905.1445\n",
      "Epoch: 464/3000 | Batch 0003/0011 | Loss: 2946.3726\n",
      "Epoch: 464/3000 | Batch 0004/0011 | Loss: 2932.7427\n",
      "Epoch: 464/3000 | Batch 0005/0011 | Loss: 2952.2236\n",
      "Epoch: 464/3000 | Batch 0006/0011 | Loss: 2924.3809\n",
      "Epoch: 464/3000 | Batch 0007/0011 | Loss: 2898.4263\n",
      "Epoch: 464/3000 | Batch 0008/0011 | Loss: 2912.6577\n",
      "Epoch: 464/3000 | Batch 0009/0011 | Loss: 2920.5679\n",
      "Epoch: 464/3000 | Batch 0010/0011 | Loss: 2954.7361\n",
      "Time elapsed: 0.13 min\n",
      "Epoch: 465/3000 | Batch 0000/0011 | Loss: 2930.6787\n",
      "Epoch: 465/3000 | Batch 0001/0011 | Loss: 2945.9280\n",
      "Epoch: 465/3000 | Batch 0002/0011 | Loss: 2903.4536\n",
      "Epoch: 465/3000 | Batch 0003/0011 | Loss: 2923.4233\n",
      "Epoch: 465/3000 | Batch 0004/0011 | Loss: 2940.0330\n",
      "Epoch: 465/3000 | Batch 0005/0011 | Loss: 2931.4287\n",
      "Epoch: 465/3000 | Batch 0006/0011 | Loss: 2941.0164\n",
      "Epoch: 465/3000 | Batch 0007/0011 | Loss: 2961.4189\n",
      "Epoch: 465/3000 | Batch 0008/0011 | Loss: 2972.1357\n",
      "Epoch: 465/3000 | Batch 0009/0011 | Loss: 2895.2354\n",
      "Epoch: 465/3000 | Batch 0010/0011 | Loss: 2998.5205\n",
      "Time elapsed: 0.13 min\n",
      "Epoch: 466/3000 | Batch 0000/0011 | Loss: 2970.8167\n",
      "Epoch: 466/3000 | Batch 0001/0011 | Loss: 2921.9941\n",
      "Epoch: 466/3000 | Batch 0002/0011 | Loss: 2925.4482\n",
      "Epoch: 466/3000 | Batch 0003/0011 | Loss: 2955.5688\n",
      "Epoch: 466/3000 | Batch 0004/0011 | Loss: 2957.3813\n",
      "Epoch: 466/3000 | Batch 0005/0011 | Loss: 2949.7100\n",
      "Epoch: 466/3000 | Batch 0006/0011 | Loss: 2904.2017\n",
      "Epoch: 466/3000 | Batch 0007/0011 | Loss: 2904.8979\n",
      "Epoch: 466/3000 | Batch 0008/0011 | Loss: 2905.3049\n",
      "Epoch: 466/3000 | Batch 0009/0011 | Loss: 2929.4575\n",
      "Epoch: 466/3000 | Batch 0010/0011 | Loss: 2975.6873\n",
      "Time elapsed: 0.13 min\n",
      "Epoch: 467/3000 | Batch 0000/0011 | Loss: 2904.1697\n",
      "Epoch: 467/3000 | Batch 0001/0011 | Loss: 2949.7847\n",
      "Epoch: 467/3000 | Batch 0002/0011 | Loss: 2901.9583\n",
      "Epoch: 467/3000 | Batch 0003/0011 | Loss: 2933.9116\n",
      "Epoch: 467/3000 | Batch 0004/0011 | Loss: 2997.7046\n",
      "Epoch: 467/3000 | Batch 0005/0011 | Loss: 2930.6777\n",
      "Epoch: 467/3000 | Batch 0006/0011 | Loss: 2894.0503\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 19\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#%% running application on case study BRCASubtype\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# This function trains VAE or CVAE, or GAN, WGAN, WGANGP, MAF, GLOW, RealNVP \u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#      given data, model, batch_size, learning_rate, epoch, off_aug and pre_model\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# pre_model:         transfer learning input model. If pre_model == None, no transfer learning\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# save_model:        if the trained model should be saved, specify the path and name of the saved model\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m ApplyExperiment(path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../Case/LIHCSubtype/\u001b[39m\u001b[38;5;124m\"\u001b[39m, dataname \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLIHCSubtypeFamInd_train294_DESeq\u001b[39m\u001b[38;5;124m\"\u001b[39m, apply_log \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[1;32m     20\u001b[0m                 new_size \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1000\u001b[39m], model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCVAE1-10\u001b[39m\u001b[38;5;124m\"\u001b[39m , batch_frac \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m, \n\u001b[1;32m     21\u001b[0m                 learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0005\u001b[39m, epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3000\u001b[39m, early_stop_num \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, \n\u001b[1;32m     22\u001b[0m                 off_aug \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, AE_head_num \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m, Gaussian_head_num \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m9\u001b[39m, \n\u001b[1;32m     23\u001b[0m                 pre_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, save_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/syng_bts/python/Experiments_new.py:709\u001b[0m, in \u001b[0;36mApplyExperiment\u001b[0;34m(path, dataname, apply_log, new_size, model, batch_frac, learning_rate, epoch, early_stop_num, off_aug, AE_head_num, Gaussian_head_num, pre_model, save_model)\u001b[0m\n\u001b[1;32m    706\u001b[0m     log_pd\u001b[38;5;241m.\u001b[39mto_csv(Path(losspath), index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    708\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAE\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m modelname:\n\u001b[0;32m--> 709\u001b[0m     log_dict \u001b[38;5;241m=\u001b[39m training_AEs(\n\u001b[1;32m    710\u001b[0m         savepath\u001b[38;5;241m=\u001b[39msavepath,  \u001b[38;5;66;03m# path to save reconstructed samples\u001b[39;00m\n\u001b[1;32m    711\u001b[0m         savepathnew\u001b[38;5;241m=\u001b[39msavepathnew,  \u001b[38;5;66;03m# path to save newly generated samples\u001b[39;00m\n\u001b[1;32m    712\u001b[0m         rawdata\u001b[38;5;241m=\u001b[39mrawdata,  \u001b[38;5;66;03m# raw data tensor with samples in row, features in column\u001b[39;00m\n\u001b[1;32m    713\u001b[0m         rawlabels\u001b[38;5;241m=\u001b[39mrawlabels,  \u001b[38;5;66;03m# abels for each sample, n_samples * 1, will not be used in AE or VAE\u001b[39;00m\n\u001b[1;32m    714\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mround\u001b[39m(rawdata\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m batch_frac),  \u001b[38;5;66;03m# batch size\u001b[39;00m\n\u001b[1;32m    715\u001b[0m         random_seed\u001b[38;5;241m=\u001b[39mrandom_seed,\n\u001b[1;32m    716\u001b[0m         modelname\u001b[38;5;241m=\u001b[39mmodelname,  \u001b[38;5;66;03m# choose from \"VAE\", \"AE\"\u001b[39;00m\n\u001b[1;32m    717\u001b[0m         num_epochs\u001b[38;5;241m=\u001b[39mnum_epochs,  \u001b[38;5;66;03m# maximum number of epochs if early stop is not triggered\u001b[39;00m\n\u001b[1;32m    718\u001b[0m         learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate,\n\u001b[1;32m    719\u001b[0m         kl_weight\u001b[38;5;241m=\u001b[39mkl_weight,  \u001b[38;5;66;03m# only take effect if model name is VAE, default value is\u001b[39;00m\n\u001b[1;32m    720\u001b[0m         early_stop\u001b[38;5;241m=\u001b[39mearly_stop,  \u001b[38;5;66;03m# whether use early stopping rule\u001b[39;00m\n\u001b[1;32m    721\u001b[0m         early_stop_num\u001b[38;5;241m=\u001b[39mearly_stop_num,  \u001b[38;5;66;03m# stop training if loss does not improve for early_stop_num epochs\u001b[39;00m\n\u001b[1;32m    722\u001b[0m         pre_model\u001b[38;5;241m=\u001b[39mpre_model,  \u001b[38;5;66;03m# load pre-trained model from transfer learning\u001b[39;00m\n\u001b[1;32m    723\u001b[0m         save_model\u001b[38;5;241m=\u001b[39msave_model,  \u001b[38;5;66;03m# save model for transfer learning, specify the path if want to save model\u001b[39;00m\n\u001b[1;32m    724\u001b[0m         loss_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMSE\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# only choose WMSE if you know the weights, ow. choose MSE by default\u001b[39;00m\n\u001b[1;32m    725\u001b[0m         save_recons\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,  \u001b[38;5;66;03m# whether save reconstructed data, if True, savepath must be provided\u001b[39;00m\n\u001b[1;32m    726\u001b[0m         new_size\u001b[38;5;241m=\u001b[39mnew_size,  \u001b[38;5;66;03m# how many new samples you want to generate\u001b[39;00m\n\u001b[1;32m    727\u001b[0m         save_new\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# whether save new samples, if True, savepathnew must be provided\u001b[39;00m\n\u001b[1;32m    728\u001b[0m         plot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    729\u001b[0m     )  \u001b[38;5;66;03m# whether plot reconstructed samples' heatmap\u001b[39;00m\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVAEs model training finished.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    732\u001b[0m     log_pd \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\n\u001b[1;32m    733\u001b[0m         {\n\u001b[1;32m    734\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkl\u001b[39m\u001b[38;5;124m\"\u001b[39m: log_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_kl_loss_per_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    735\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecons\u001b[39m\u001b[38;5;124m\"\u001b[39m: log_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_reconstruction_loss_per_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    736\u001b[0m         }\n\u001b[1;32m    737\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/syng_bts/python/helper_training_new.py:78\u001b[0m, in \u001b[0;36mtraining_AEs\u001b[0;34m(savepath, savepathnew, rawdata, rawlabels, batch_size, random_seed, modelname, num_epochs, learning_rate, pre_model, save_model, kl_weight, early_stop, early_stop_num, loss_fn, save_recons, new_size, save_new, plot)\u001b[0m\n\u001b[1;32m     75\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m rawdata\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m modelname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCVAE\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 78\u001b[0m     log_dict, best_model \u001b[38;5;241m=\u001b[39m ht\u001b[38;5;241m.\u001b[39mtrain_CVAE(num_epochs \u001b[38;5;241m=\u001b[39m num_epochs,\n\u001b[1;32m     79\u001b[0m                                       model \u001b[38;5;241m=\u001b[39m model,\n\u001b[1;32m     80\u001b[0m                                       loss_fn \u001b[38;5;241m=\u001b[39m loss_fn,\n\u001b[1;32m     81\u001b[0m                                       optimizer \u001b[38;5;241m=\u001b[39m optimizer, \n\u001b[1;32m     82\u001b[0m                                       train_loader \u001b[38;5;241m=\u001b[39m train_loader,\n\u001b[1;32m     83\u001b[0m                                       early_stop \u001b[38;5;241m=\u001b[39m early_stop,\n\u001b[1;32m     84\u001b[0m                                       early_stop_num \u001b[38;5;241m=\u001b[39m early_stop_num,\n\u001b[1;32m     85\u001b[0m                                       skip_epoch_stats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     86\u001b[0m                                       reconstruction_term_weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     87\u001b[0m                                       kl_weight \u001b[38;5;241m=\u001b[39m kl_weight,\n\u001b[1;32m     88\u001b[0m                                       logging_interval \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m,\n\u001b[1;32m     89\u001b[0m                                       save_model \u001b[38;5;241m=\u001b[39m save_model)\n\u001b[1;32m     90\u001b[0m     plot_training_loss(log_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_reconstruction_loss_per_batch\u001b[39m\u001b[38;5;124m'\u001b[39m], num_epochs, custom_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (reconstruction)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     91\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/syng_bts/python/helper_train_new.py:287\u001b[0m, in \u001b[0;36mtrain_CVAE\u001b[0;34m(num_epochs, model, optimizer, train_loader, early_stop, early_stop_num, loss_fn, logging_interval, skip_epoch_stats, reconstruction_term_weight, kl_weight, save_model)\u001b[0m\n\u001b[1;32m    283\u001b[0m epoch_loss\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m    285\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 287\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    289\u001b[0m \u001b[38;5;66;03m# UPDATE MODEL PARAMETERS\u001b[39;00m\n\u001b[1;32m    290\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/torch/_tensor.py:647\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    639\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    640\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    645\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    646\u001b[0m     )\n\u001b[0;32m--> 647\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    648\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    649\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/torch/autograd/__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m _engine_run_backward(\n\u001b[1;32m    355\u001b[0m     tensors,\n\u001b[1;32m    356\u001b[0m     grad_tensors_,\n\u001b[1;32m    357\u001b[0m     retain_graph,\n\u001b[1;32m    358\u001b[0m     create_graph,\n\u001b[1;32m    359\u001b[0m     inputs_tuple,\n\u001b[1;32m    360\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    361\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    362\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/torch/autograd/graph.py:829\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    830\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    831\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#%% running application on case study BRCASubtype\n",
    "# This function trains VAE or CVAE, or GAN, WGAN, WGANGP, MAF, GLOW, RealNVP \n",
    "#      given data, model, batch_size, learning_rate, epoch, off_aug and pre_model\n",
    "#      and generate new samples with size specified by the users.\n",
    "# path:              path for reading real data and saving new data\n",
    "# dataname :         pure data name without .csv. Eg: SKCMPositive_3\n",
    "# apply_log:         logical whether apply log transformation before training\n",
    "# model:             name of the model to be trained\n",
    "# batch_frac:        batch fraction\n",
    "# learning_rate:     learning rate \n",
    "# epoch:             choose from None (early_stop), or any interger, if choose None, early_stop_num will take effect\n",
    "# early_stop_num:    if loss does not improve for early_stop_num epochs, the training will stop. Default value is 30. Only take effect when epoch == \"early_stop\"\n",
    "# off_aug:           choose from AE_head, Gaussian_head, None. if choose AE_head, AE_head_num will take effect. If choose Gaussian_head, Gaussian_head_num will take effect. If choose None, no offline augmentation\n",
    "# AE_head_num:       how many folds of AEhead augmentation needed. Default value is 2, Only take effect when off_aug == \"AE_head\"\n",
    "# Gaussian_head_num: how many folds of Gaussianhead augmentation needed. Default value is 9, Only take effect when off_aug == \"Gaussian_head\"\n",
    "# pre_model:         transfer learning input model. If pre_model == None, no transfer learning\n",
    "# save_model:        if the trained model should be saved, specify the path and name of the saved model\n",
    "        \n",
    "ApplyExperiment(path = \"../Case/LIHCSubtype/\", dataname = \"LIHCSubtypeFamInd_train294_DESeq\", apply_log = True, \n",
    "                new_size = [1000], model = \"CVAE1-10\" , batch_frac = 0.1, \n",
    "                learning_rate = 0.0005, epoch = 3000, early_stop_num = 20, \n",
    "                off_aug = None, AE_head_num = 2, Gaussian_head_num = 9, \n",
    "                pre_model = None, save_model = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Read data, path is ../Transfer/PRAD.csv\n",
      "2. Determine the model is maf with kl-weight = 1\n",
      "3. Determine the training parameters are epoch = 10 off_aug = No learing rate = 0.0005 batch_frac = 0.1\n",
      "3. Training starts ......\n",
      "\n",
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train, Log likelihood in nats: 6631.751554: 100%|██████████| 551/551 [00:00<00:00, 658.50it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 19\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#%% Running transfer learning\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# This function run transfer learning using VAE or CVAE, or GAN, WGAN, WGANGP, MAF, GLOW, RealNVP \u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#      given fromdata, todata, model, batch_size, learning_rate, epoch, off_aug and pre_model\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# epoch:             choose from None (early_stop), or any interger, if choose None, early_stop_num will take effect\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# off_aug:           choose from AE_head, Gaussian_head, None. if choose AE_head, AE_head_num will take effect. If choose Gaussian_head, Gaussian_head_num will take effect. If choose None, no offline augmentation\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m TransferExperiment(pilot_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, fromname \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPRAD\u001b[39m\u001b[38;5;124m\"\u001b[39m, toname \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBRCA\u001b[39m\u001b[38;5;124m\"\u001b[39m, fromsize \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m551\u001b[39m, \n\u001b[1;32m     20\u001b[0m          new_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m, apply_log \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaf\u001b[39m\u001b[38;5;124m\"\u001b[39m, epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m     21\u001b[0m          batch_frac \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m, learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0005\u001b[39m, off_aug \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/syng_bts/python/Experiments_new.py:864\u001b[0m, in \u001b[0;36mTransferExperiment\u001b[0;34m(pilot_size, fromname, toname, fromsize, model, new_size, apply_log, epoch, batch_frac, learning_rate, off_aug)\u001b[0m\n\u001b[1;32m    862\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../Transfer/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    863\u001b[0m save_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../Transfer/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m toname \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m fromname \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m model \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 864\u001b[0m ApplyExperiment(\n\u001b[1;32m    865\u001b[0m     path\u001b[38;5;241m=\u001b[39mpath,\n\u001b[1;32m    866\u001b[0m     dataname\u001b[38;5;241m=\u001b[39mfromname,\n\u001b[1;32m    867\u001b[0m     apply_log\u001b[38;5;241m=\u001b[39mapply_log,\n\u001b[1;32m    868\u001b[0m     new_size\u001b[38;5;241m=\u001b[39m[fromsize],\n\u001b[1;32m    869\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    870\u001b[0m     batch_frac\u001b[38;5;241m=\u001b[39mbatch_frac,\n\u001b[1;32m    871\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate,\n\u001b[1;32m    872\u001b[0m     epoch\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m    873\u001b[0m     early_stop_num\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m,\n\u001b[1;32m    874\u001b[0m     off_aug\u001b[38;5;241m=\u001b[39moff_aug,\n\u001b[1;32m    875\u001b[0m     AE_head_num\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    876\u001b[0m     Gaussian_head_num\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m9\u001b[39m,\n\u001b[1;32m    877\u001b[0m     pre_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    878\u001b[0m     save_model\u001b[38;5;241m=\u001b[39msave_model,\n\u001b[1;32m    879\u001b[0m )\n\u001b[1;32m    881\u001b[0m \u001b[38;5;66;03m# training toname using pre-model\u001b[39;00m\n\u001b[1;32m    882\u001b[0m pre_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../Transfer/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m toname \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m fromname \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m model \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/syng_bts/python/Experiments_new.py:750\u001b[0m, in \u001b[0;36mApplyExperiment\u001b[0;34m(path, dataname, apply_log, new_size, model, batch_frac, learning_rate, epoch, early_stop_num, off_aug, AE_head_num, Gaussian_head_num, pre_model, save_model)\u001b[0m\n\u001b[1;32m    748\u001b[0m     log_pd\u001b[38;5;241m.\u001b[39mto_csv(Path(losspath), index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaf\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m modelname:\n\u001b[0;32m--> 750\u001b[0m     training_flows(\n\u001b[1;32m    751\u001b[0m         savepathnew\u001b[38;5;241m=\u001b[39msavepathnew,\n\u001b[1;32m    752\u001b[0m         rawdata\u001b[38;5;241m=\u001b[39mrawdata,\n\u001b[1;32m    753\u001b[0m         batch_frac\u001b[38;5;241m=\u001b[39mbatch_frac,\n\u001b[1;32m    754\u001b[0m         valid_batch_frac\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m,\n\u001b[1;32m    755\u001b[0m         random_seed\u001b[38;5;241m=\u001b[39mrandom_seed,\n\u001b[1;32m    756\u001b[0m         modelname\u001b[38;5;241m=\u001b[39mmodelname,\n\u001b[1;32m    757\u001b[0m         num_blocks\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m    758\u001b[0m         num_epoches\u001b[38;5;241m=\u001b[39mnum_epochs,\n\u001b[1;32m    759\u001b[0m         learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate,\n\u001b[1;32m    760\u001b[0m         new_size\u001b[38;5;241m=\u001b[39mnew_size,\n\u001b[1;32m    761\u001b[0m         num_hidden\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m226\u001b[39m,\n\u001b[1;32m    762\u001b[0m         early_stop\u001b[38;5;241m=\u001b[39mearly_stop,  \u001b[38;5;66;03m# whether use early stopping rule\u001b[39;00m\n\u001b[1;32m    763\u001b[0m         early_stop_num\u001b[38;5;241m=\u001b[39mearly_stop_num,\n\u001b[1;32m    764\u001b[0m         \u001b[38;5;66;03m# stop training if loss does not improve for early_stop_num epochs\u001b[39;00m\n\u001b[1;32m    765\u001b[0m         pre_model\u001b[38;5;241m=\u001b[39mpre_model,  \u001b[38;5;66;03m# load pre-trained model from transfer learning\u001b[39;00m\n\u001b[1;32m    766\u001b[0m         save_model\u001b[38;5;241m=\u001b[39msave_model,\n\u001b[1;32m    767\u001b[0m         plot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    768\u001b[0m     )\n\u001b[1;32m    769\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrealnvp\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m modelname:\n\u001b[1;32m    770\u001b[0m     training_flows(\n\u001b[1;32m    771\u001b[0m         savepathnew\u001b[38;5;241m=\u001b[39msavepathnew,\n\u001b[1;32m    772\u001b[0m         rawdata\u001b[38;5;241m=\u001b[39mrawdata,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    787\u001b[0m         plot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    788\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/syng_bts/python/helper_training_new.py:607\u001b[0m, in \u001b[0;36mtraining_flows\u001b[0;34m(savepathnew, rawdata, batch_frac, valid_batch_frac, random_seed, modelname, num_blocks, num_epoches, learning_rate, new_size, num_hidden, early_stop, early_stop_num, pre_model, save_model, plot)\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epoches):\n\u001b[1;32m    605\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch))\n\u001b[0;32m--> 607\u001b[0m     global_step, train_loss \u001b[38;5;241m=\u001b[39m train(epoch, global_step, writer)\n\u001b[1;32m    611\u001b[0m     \u001b[38;5;66;03m# # With validation version\u001b[39;00m\n\u001b[1;32m    612\u001b[0m     \u001b[38;5;66;03m# validation_loss = validate(epoch, model, valid_loader, global_step, writer)\u001b[39;00m\n\u001b[1;32m    613\u001b[0m     \u001b[38;5;66;03m# if epoch - best_validation_epoch >= 30:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m#     plot_new_samples(model=model, savepathnew=savepathnew.replace('.csv', f'_epoch_{epoch}.csv'), latent_size=num_hidden, modelname=modelname,\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;66;03m#                      num_images=new_size, plot=plot)\u001b[39;00m\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m early_stop:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/syng_bts/python/helper_training_new.py:556\u001b[0m, in \u001b[0;36mtraining_flows.<locals>.train\u001b[0;34m(epoch, global_step, writer)\u001b[0m\n\u001b[1;32m    553\u001b[0m         module\u001b[38;5;241m.\u001b[39mmomentum \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 556\u001b[0m         model(train_loader\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mtensors[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(data\u001b[38;5;241m.\u001b[39mdevice))\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mmodules():\n\u001b[1;32m    559\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(module, ht\u001b[38;5;241m.\u001b[39mBatchNormFlow):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/syng_bts/python/helper_train_new.py:1226\u001b[0m, in \u001b[0;36mFlowSequential.forward\u001b[0;34m(self, inputs, cond_inputs, mode, logdets)\u001b[0m\n\u001b[1;32m   1224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdirect\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m   1225\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m-> 1226\u001b[0m         inputs, logdet \u001b[38;5;241m=\u001b[39m module(inputs, cond_inputs, mode)\n\u001b[1;32m   1227\u001b[0m         logdets \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m logdet\n\u001b[1;32m   1228\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/syng_bts/python/helper_train_new.py:962\u001b[0m, in \u001b[0;36mBatchNormFlow.forward\u001b[0;34m(self, inputs, cond_inputs, mode)\u001b[0m\n\u001b[1;32m    959\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m    960\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_mean \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    961\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_var \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 962\u001b[0m         inputs \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_mean)\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps\n\u001b[1;32m    964\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean\u001b[38;5;241m.\u001b[39mmul_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmomentum)\n\u001b[1;32m    965\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var\u001b[38;5;241m.\u001b[39mmul_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmomentum)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#%% Running transfer learning\n",
    "# This function run transfer learning using VAE or CVAE, or GAN, WGAN, WGANGP, MAF, GLOW, RealNVP \n",
    "#      given fromdata, todata, model, batch_size, learning_rate, epoch, off_aug and pre_model\n",
    "#      and generate new samples with size specified by the users.\n",
    "#      The fine tuning model training can be pilot experiments or apply experiment\n",
    "# Make sure data files for pre_model training and fine tuning model training are in Transfer/\n",
    "# pilot_size:        if None, the fine tuning model will be apply experiment and new_size will take effect\n",
    "#                    otherwise, the fine tuning model will be trained using pilot experiments\n",
    "# fromname:          the dataname for pre_model training \n",
    "# toname:            the dataname for fine tuning model training\n",
    "# fromsize:          the sample size of the fromdata\n",
    "# new_size:          if apply experiment, this will be the sample size of generated samples\n",
    "# apply_log:         logical whether apply log transformation before training\n",
    "# model:             name of the model to be trained\n",
    "# batch_frac:        batch fraction\n",
    "# learning_rate:     learning rate \n",
    "# epoch:             choose from None (early_stop), or any interger, if choose None, early_stop_num will take effect\n",
    "# off_aug:           choose from AE_head, Gaussian_head, None. if choose AE_head, AE_head_num will take effect. If choose Gaussian_head, Gaussian_head_num will take effect. If choose None, no offline augmentation\n",
    "TransferExperiment(pilot_size = None, fromname = \"PRAD\", toname = \"BRCA\", fromsize = 551, \n",
    "         new_size = 500, apply_log = True, model = \"maf\", epoch = 10,\n",
    "         batch_frac = 0.1, learning_rate = 0.0005, off_aug = None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
